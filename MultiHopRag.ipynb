{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "iterative_multihop_rag.py\n",
    "Iterative Multi-Hop RAG - Performs multiple sequential retrievals, each building on previous results.\n",
    "\n",
    "The system:\n",
    "1. Initial retrieval based on query\n",
    "2. Analyzes results and generates follow-up questions\n",
    "3. Performs subsequent retrievals (hops) based on information gaps\n",
    "4. Chains information across hops\n",
    "5. Synthesizes final answer from all hops\n",
    "\n",
    "Environment variables needed:\n",
    "    PINECONE_API_KEY, GROQ_API_KEY\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import pdfplumber\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "INDEX_NAME = \"pinecone-multihop\"\n",
    "DIM = 384\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "\n",
    "if PINECONE_API_KEY is None or GROQ_API_KEY is None:\n",
    "    print(\"ERROR: Set PINECONE_API_KEY and GROQ_API_KEY environment variables\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s\"\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    \n",
    "    def add(self, component, ns):\n",
    "        self.store[component].append(ns)\n",
    "    \n",
    "    def pretty_print(self):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, vals in sorted(self.store.items()):\n",
    "            total = sum(vals)\n",
    "            avg = total // len(vals) if vals else 0\n",
    "            print(f\"\\n{comp}:\")\n",
    "            print(f\"  Count: {len(vals)}, Total: {format_time_ns(total)}, Avg: {format_time_ns(avg)}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# ---------------------------\n",
    "# Hop Data Structure\n",
    "# ---------------------------\n",
    "class Hop:\n",
    "    \"\"\"Represents a single retrieval hop in the multi-hop chain\"\"\"\n",
    "    def __init__(self, hop_number: int, query: str, context_from_previous: str = \"\"):\n",
    "        self.hop_number = hop_number\n",
    "        self.query = query\n",
    "        self.context_from_previous = context_from_previous\n",
    "        self.retrieved_docs = []\n",
    "        self.retrieved_content = \"\"\n",
    "        self.analysis = \"\"\n",
    "        self.follow_up_questions = []\n",
    "        self.information_gain = 0.0\n",
    "        self.elapsed_ns = 0\n",
    "        self.timestamp = time.time_ns()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Hop({self.hop_number}: '{self.query[:50]}...')\"\n",
    "\n",
    "class MultiHopChain:\n",
    "    \"\"\"Represents the complete multi-hop retrieval chain\"\"\"\n",
    "    def __init__(self, original_query: str):\n",
    "        self.original_query = original_query\n",
    "        self.hops = []\n",
    "        self.final_answer = \"\"\n",
    "        self.total_elapsed_ns = 0\n",
    "        self.information_coverage = 0.0\n",
    "    \n",
    "    def add_hop(self, hop: Hop):\n",
    "        self.hops.append(hop)\n",
    "    \n",
    "    def get_all_context(self) -> str:\n",
    "        \"\"\"Get concatenated context from all hops\"\"\"\n",
    "        return \"\\n\\n--- HOP SEPARATOR ---\\n\\n\".join([\n",
    "            f\"HOP {h.hop_number} (Query: {h.query}):\\n{h.retrieved_content}\"\n",
    "            for h in self.hops\n",
    "        ])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MultiHopChain(query='{self.original_query}', hops={len(self.hops)})\"\n",
    "\n",
    "# ---------------------------\n",
    "# Iterative Multi-Hop RAG System\n",
    "# ---------------------------\n",
    "class IterativeMultiHopRAG:\n",
    "    \"\"\"RAG system that performs multiple sequential retrievals (hops)\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm, max_hops: int = 4):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.max_hops = max_hops\n",
    "        self.retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\", \n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "    \n",
    "    def _llm_invoke(self, prompt: str, label: str) -> str:\n",
    "        \"\"\"Invoke LLM with timing\"\"\"\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(f\"llm_{label}\", elapsed)\n",
    "            return response.content if hasattr(response, 'content') else str(response)\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(f\"llm_{label}_error\", elapsed)\n",
    "            print(f\"‚ùå LLM error in {label}: {e}\")\n",
    "            return str(e)\n",
    "    \n",
    "    def retrieve_documents(self, query: str, hop_number: int) -> Tuple[List, str]:\n",
    "        \"\"\"Retrieve documents for a given query\"\"\"\n",
    "        print(f\"    üîç Retrieving documents for hop {hop_number}...\")\n",
    "        start = time.time_ns()\n",
    "        \n",
    "        try:\n",
    "            docs = self.retriever.invoke(query)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(f\"retrieval_hop_{hop_number}\", elapsed)\n",
    "            \n",
    "            # Extract content\n",
    "            content_parts = []\n",
    "            for i, doc in enumerate(docs):\n",
    "                content = getattr(doc, \"page_content\", str(doc))\n",
    "                content_parts.append(f\"[Document {i+1}]\\n{content}\")\n",
    "            \n",
    "            full_content = \"\\n\\n\".join(content_parts)\n",
    "            \n",
    "            print(f\"    ‚úÖ Retrieved {len(docs)} documents in {format_time_ns(elapsed)}\")\n",
    "            print(f\"    üìÑ Total content length: {len(full_content)} characters\")\n",
    "            \n",
    "            return docs, full_content\n",
    "            \n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(f\"retrieval_hop_{hop_number}_error\", elapsed)\n",
    "            print(f\"    ‚ùå Retrieval failed: {e}\")\n",
    "            return [], \"\"\n",
    "    \n",
    "    def analyze_information_gap(self, original_query: str, current_context: str, \n",
    "                                hop_number: int) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze what information is missing and generate follow-up questions\"\"\"\n",
    "        print(f\"    üß† Analyzing information gaps...\")\n",
    "        \n",
    "        analysis_prompt = f\"\"\"You are analyzing information gathered so far to answer a question.\n",
    "\n",
    "Original Question: {original_query}\n",
    "\n",
    "Information gathered so far (from {hop_number} hop(s)):\n",
    "{current_context[:2000]}\n",
    "\n",
    "Analyze:\n",
    "1. What key information have we found?\n",
    "2. What information is still missing to fully answer the question?\n",
    "3. What specific follow-up questions would help fill these gaps?\n",
    "4. Rate information completeness (0-100%): How much of the answer do we have?\n",
    "\n",
    "Format your response as:\n",
    "FOUND: [what we learned]\n",
    "MISSING: [what's still needed]\n",
    "FOLLOW_UP: [specific question 1] | [specific question 2] | [specific question 3]\n",
    "COMPLETENESS: [percentage]\n",
    "\n",
    "Analysis:\"\"\"\n",
    "        \n",
    "        analysis = self._llm_invoke(analysis_prompt, f\"gap_analysis_hop_{hop_number}\")\n",
    "        \n",
    "        # Parse the analysis\n",
    "        found = \"\"\n",
    "        missing = \"\"\n",
    "        follow_ups = []\n",
    "        completeness = 0\n",
    "        \n",
    "        for line in analysis.split('\\n'):\n",
    "            if line.strip().startswith('FOUND:'):\n",
    "                found = line.split('FOUND:', 1)[1].strip()\n",
    "            elif line.strip().startswith('MISSING:'):\n",
    "                missing = line.split('MISSING:', 1)[1].strip()\n",
    "            elif line.strip().startswith('FOLLOW_UP:'):\n",
    "                follow_up_text = line.split('FOLLOW_UP:', 1)[1].strip()\n",
    "                follow_ups = [q.strip() for q in follow_up_text.split('|') if q.strip()]\n",
    "            elif line.strip().startswith('COMPLETENESS:'):\n",
    "                completeness_text = line.split('COMPLETENESS:', 1)[1].strip()\n",
    "                # Extract number\n",
    "                numbers = re.findall(r'\\d+', completeness_text)\n",
    "                if numbers:\n",
    "                    completeness = int(numbers[0])\n",
    "        \n",
    "        print(f\"    üìä Information completeness: {completeness}%\")\n",
    "        print(f\"    ‚ùì Generated {len(follow_ups)} follow-up questions\")\n",
    "        \n",
    "        return {\n",
    "            'found': found,\n",
    "            'missing': missing,\n",
    "            'follow_ups': follow_ups,\n",
    "            'completeness': completeness,\n",
    "            'full_analysis': analysis\n",
    "        }\n",
    "    \n",
    "    def determine_next_hop_query(self, original_query: str, follow_up_questions: List[str],\n",
    "                                 chain: MultiHopChain) -> Optional[str]:\n",
    "        \"\"\"Determine what to search for in the next hop\"\"\"\n",
    "        if not follow_up_questions:\n",
    "            return None\n",
    "        \n",
    "        print(f\"    üéØ Determining next hop query...\")\n",
    "        \n",
    "        # Use the most promising follow-up question\n",
    "        # For simplicity, we'll use the first one, but could rank them\n",
    "        next_query = follow_up_questions[0]\n",
    "        \n",
    "        # Optionally refine it\n",
    "        refinement_prompt = f\"\"\"Given the original question and current follow-up question, create an optimal search query.\n",
    "\n",
    "Original Question: {original_query}\n",
    "\n",
    "Follow-up Question: {next_query}\n",
    "\n",
    "Previous queries used: {[h.query for h in chain.hops]}\n",
    "\n",
    "Create a concise, specific search query that will find the missing information without repeating previous searches:\n",
    "\n",
    "Search Query:\"\"\"\n",
    "        \n",
    "        refined_query = self._llm_invoke(refinement_prompt, f\"query_refinement_hop_{len(chain.hops)}\")\n",
    "        \n",
    "        # Clean up the refined query\n",
    "        refined_query = refined_query.strip().split('\\n')[0]\n",
    "        \n",
    "        print(f\"    üìù Next hop query: '{refined_query}'\")\n",
    "        \n",
    "        return refined_query\n",
    "    \n",
    "    def perform_hop(self, hop_number: int, query: str, chain: MultiHopChain) -> Hop:\n",
    "        \"\"\"Perform a single retrieval hop\"\"\"\n",
    "        print(f\"\\n  {'='*66}\")\n",
    "        print(f\"  üèÉ HOP {hop_number}: {query}\")\n",
    "        print(f\"  {'='*66}\")\n",
    "        \n",
    "        hop_start = time.time_ns()\n",
    "        \n",
    "        # Create hop object\n",
    "        context_from_previous = chain.get_all_context() if chain.hops else \"\"\n",
    "        hop = Hop(hop_number, query, context_from_previous)\n",
    "        \n",
    "        # Retrieve documents\n",
    "        docs, content = self.retrieve_documents(query, hop_number)\n",
    "        hop.retrieved_docs = docs\n",
    "        hop.retrieved_content = content\n",
    "        \n",
    "        # Quick analysis of this hop's contribution\n",
    "        if content:\n",
    "            contribution_prompt = f\"\"\"Rate how much NEW information this retrieval adds (0-100%):\n",
    "\n",
    "Query: {query}\n",
    "Retrieved: {content[:500]}...\n",
    "\n",
    "Previous context available: {'Yes' if context_from_previous else 'No'}\n",
    "\n",
    "NEW_INFORMATION_SCORE: [0-100]\"\"\"\n",
    "            \n",
    "            contribution = self._llm_invoke(contribution_prompt, f\"contribution_hop_{hop_number}\")\n",
    "            \n",
    "            # Extract score\n",
    "            numbers = re.findall(r'\\d+', contribution)\n",
    "            if numbers:\n",
    "                hop.information_gain = int(numbers[0])\n",
    "            \n",
    "            print(f\"    üí° Information gain: {hop.information_gain}%\")\n",
    "        \n",
    "        hop.elapsed_ns = time.time_ns() - hop_start\n",
    "        latency_report.add(\"hop_total\", hop.elapsed_ns)\n",
    "        \n",
    "        print(f\"  ‚úÖ Hop {hop_number} completed in {format_time_ns(hop.elapsed_ns)}\")\n",
    "        \n",
    "        return hop\n",
    "    \n",
    "    def synthesize_final_answer(self, chain: MultiHopChain) -> str:\n",
    "        \"\"\"Synthesize final answer from all hops\"\"\"\n",
    "        print(f\"\\n  {'='*66}\")\n",
    "        print(f\"  üîÑ SYNTHESIZING FINAL ANSWER FROM {len(chain.hops)} HOPS\")\n",
    "        print(f\"  {'='*66}\")\n",
    "        \n",
    "        all_context = chain.get_all_context()\n",
    "        \n",
    "        synthesis_prompt = f\"\"\"Synthesize a comprehensive answer from multiple retrieval hops.\n",
    "\n",
    "Original Question: {chain.original_query}\n",
    "\n",
    "Information gathered across {len(chain.hops)} retrieval hops:\n",
    "\n",
    "{all_context[:4000]}\n",
    "\n",
    "Instructions:\n",
    "1. Integrate information from ALL hops\n",
    "2. Resolve any contradictions by noting them\n",
    "3. Cite which hop provided key information (e.g., \"According to Hop 2...\")\n",
    "4. Provide a complete, coherent answer\n",
    "5. Note if any information is still missing\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "        \n",
    "        final_answer = self._llm_invoke(synthesis_prompt, \"final_synthesis\")\n",
    "        \n",
    "        print(f\"  ‚úÖ Final answer synthesized\")\n",
    "        print(f\"  üìù Answer length: {len(final_answer)} characters\")\n",
    "        \n",
    "        return final_answer\n",
    "    \n",
    "    def query(self, question: str, target_completeness: int = 85) -> Dict[str, Any]:\n",
    "        \"\"\"Main query method: performs iterative multi-hop retrieval\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîó ITERATIVE MULTI-HOP RAG QUERY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚ùì Question: {question}\")\n",
    "        print(f\"üéØ Target completeness: {target_completeness}%\")\n",
    "        print(f\"üìä Max hops: {self.max_hops}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        \n",
    "        # Initialize chain\n",
    "        chain = MultiHopChain(question)\n",
    "        \n",
    "        # Initial hop - use original question\n",
    "        current_query = question\n",
    "        \n",
    "        # Perform hops\n",
    "        for hop_num in range(1, self.max_hops + 1):\n",
    "            # Perform the hop\n",
    "            hop = self.perform_hop(hop_num, current_query, chain)\n",
    "            chain.add_hop(hop)\n",
    "            \n",
    "            # Analyze what we have so far\n",
    "            current_context = chain.get_all_context()\n",
    "            gap_analysis = self.analyze_information_gap(question, current_context, hop_num)\n",
    "            \n",
    "            hop.analysis = gap_analysis['full_analysis']\n",
    "            hop.follow_up_questions = gap_analysis['follow_ups']\n",
    "            chain.information_coverage = gap_analysis['completeness']\n",
    "            \n",
    "            # Check if we have enough information\n",
    "            if gap_analysis['completeness'] >= target_completeness:\n",
    "                print(f\"\\n  ‚úÖ Target completeness reached: {gap_analysis['completeness']}%\")\n",
    "                print(f\"  üéâ Stopping after {hop_num} hops\")\n",
    "                break\n",
    "            \n",
    "            # Check if we're at max hops\n",
    "            if hop_num >= self.max_hops:\n",
    "                print(f\"\\n  ‚ö†Ô∏è Max hops reached ({self.max_hops})\")\n",
    "                print(f\"  üìä Current completeness: {gap_analysis['completeness']}%\")\n",
    "                break\n",
    "            \n",
    "            # Determine next hop query\n",
    "            next_query = self.determine_next_hop_query(question, gap_analysis['follow_ups'], chain)\n",
    "            \n",
    "            if not next_query:\n",
    "                print(f\"\\n  üõë No more follow-up queries generated\")\n",
    "                print(f\"  üìä Final completeness: {gap_analysis['completeness']}%\")\n",
    "                break\n",
    "            \n",
    "            current_query = next_query\n",
    "        \n",
    "        # Synthesize final answer from all hops\n",
    "        final_answer = self.synthesize_final_answer(chain)\n",
    "        chain.final_answer = final_answer\n",
    "        \n",
    "        overall_elapsed = time.time_ns() - overall_start\n",
    "        chain.total_elapsed_ns = overall_elapsed\n",
    "        latency_report.add(\"multihop_query_total\", overall_elapsed)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üìä MULTI-HOP QUERY SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total hops performed: {len(chain.hops)}\")\n",
    "        print(f\"Information coverage: {chain.information_coverage}%\")\n",
    "        print(f\"Total time: {format_time_ns(overall_elapsed)}\")\n",
    "        print(f\"Average time per hop: {format_time_ns(overall_elapsed // len(chain.hops))}\")\n",
    "        \n",
    "        print(f\"\\nüîó HOP CHAIN:\")\n",
    "        for hop in chain.hops:\n",
    "            print(f\"  Hop {hop.hop_number}: '{hop.query[:60]}...'\")\n",
    "            print(f\"    ‚Üí Info gain: {hop.information_gain}%, Time: {format_time_ns(hop.elapsed_ns)}\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': final_answer,\n",
    "            'chain': chain,\n",
    "            'num_hops': len(chain.hops),\n",
    "            'information_coverage': chain.information_coverage,\n",
    "            'total_elapsed_ns': overall_elapsed\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# PDF Processing (same as original)\n",
    "# ---------------------------\n",
    "def load_and_process_pdf(path):\n",
    "    print(f\"üìÑ Loading PDF: {path}\")\n",
    "    start = time.time_ns()\n",
    "    \n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    \n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"pdf_load\", elapsed)\n",
    "    print(f\"‚úÖ Loaded {len(text)} characters in {format_time_ns(elapsed)}\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=100):\n",
    "    start = time.time_ns()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"chunking\", elapsed)\n",
    "    print(f\"üìÑ Created {len(chunks)} chunks in {format_time_ns(elapsed)}\")\n",
    "    return chunks\n",
    "\n",
    "def init_vectorstore(chunks, api_key, index_name=INDEX_NAME):\n",
    "    print(f\"üîß Initializing Pinecone...\")\n",
    "    start = time.time_ns()\n",
    "    \n",
    "    pc = Pinecone(api_key=api_key)\n",
    "    existing = [idx.name for idx in pc.list_indexes()]\n",
    "    \n",
    "    if index_name in existing:\n",
    "        print(f\"üóëÔ∏è Deleting existing index...\")\n",
    "        pc.delete_index(index_name)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(f\"üÜï Creating index...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=DIM,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    time.sleep(2)\n",
    "    \n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    vectorstore = PineconeVectorStore.from_texts(\n",
    "        texts=chunks,\n",
    "        embedding=embed_model,\n",
    "        index_name=index_name,\n",
    "        namespace=\"\",\n",
    "        metadatas=[{\"chunk_id\": i} for i in range(len(chunks))]\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"vectorstore_init\", elapsed)\n",
    "    print(f\"‚úÖ Vectorstore ready in {format_time_ns(elapsed)}\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üöÄ ITERATIVE MULTI-HOP RAG\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Setup\n",
    "    text = load_and_process_pdf(PDF_PATH)\n",
    "    chunks = chunk_text(text)\n",
    "    vectorstore = init_vectorstore(chunks, PINECONE_API_KEY)\n",
    "    \n",
    "    # Initialize LLM\n",
    "    print(\"\\nü§ñ Initializing LLM...\")\n",
    "    llm = ChatGroq(model_name=MODEL_NAME, temperature=0, groq_api_key=GROQ_API_KEY)\n",
    "    \n",
    "    # Initialize Multi-Hop RAG\n",
    "    multihop_rag = IterativeMultiHopRAG(vectorstore, llm, max_hops=4)\n",
    "    print(\"\\n‚úÖ Multi-Hop RAG system ready!\\n\")\n",
    "    \n",
    "    # Test Queries - these benefit from multi-hop retrieval\n",
    "    queries = [\n",
    "        \"What are the main themes in this story and how do they connect to the character development?\",\n",
    "        \"Describe the relationship between the main characters and how it evolves throughout the narrative.\",\n",
    "        \"What is the significance of love in the document and how is it portrayed in different contexts?\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for q in queries:\n",
    "        result = multihop_rag.query(q, target_completeness=85)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FINAL ANSWER:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(result['answer'][:600])\n",
    "        print(\"...\\n\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Final report\n",
    "    latency_report.pretty_print()\n",
    "    \n",
    "    # Multi-hop statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-HOP STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    total_hops = sum(r['num_hops'] for r in results)\n",
    "    avg_hops = total_hops / len(results)\n",
    "    avg_coverage = sum(r['information_coverage'] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"Total queries: {len(results)}\")\n",
    "    print(f\"Total hops: {total_hops}\")\n",
    "    print(f\"Average hops per query: {avg_hops:.1f}\")\n",
    "    print(f\"Average information coverage: {avg_coverage:.1f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n‚úÖ ITERATIVE MULTI-HOP RAG PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
