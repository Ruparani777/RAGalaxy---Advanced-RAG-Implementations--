{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "selfrag_full.py\n",
    "Full pipeline with comprehensive nanosecond latency instrumentation.\n",
    "\n",
    "- Use environment variables for secrets:\n",
    "    PINECONE_API_KEY, GROQ_API_KEY\n",
    "\n",
    "Notes:\n",
    "- This file keeps the same high-level structure as your original script but:\n",
    "  * Replaces time.time() with time.time_ns()\n",
    "  * Adds timers for each major component and LLM invocation\n",
    "  * Prints a final latency report\n",
    "  * Avoids hard-coded API keys (use env vars instead)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import pprint\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "\n",
    "# third-party imports (same as your original requirements)\n",
    "import pdfplumber\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ---------------------------\n",
    "# Config (use env vars; no secrets in source)\n",
    "# ---------------------------\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "INDEX_NAME = \"pinecone\"\n",
    "DIM = 384  # MiniLM embedding dimension\n",
    "PINECONE_API_KEY = \"pcsk_2vzzu5_BLrivtXHdVFYt1ciSqGCHBHymoRtjqXrmVmASwgewHN6FrtpFWcJdZwbiTEJNq4\"\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000  # example target for sentiment inference\n",
    "\n",
    "if PINECONE_API_KEY is None:\n",
    "    print(\"ERROR: Set PINECONE_API_KEY environment variable before running.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities: time formatting + timing helpers\n",
    "# ---------------------------\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    \"\"\"Return human-readable representation of nanoseconds.\"\"\"\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} Âµs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    \"\"\"Call fn(*args, **kwargs) and return (result, elapsed_ns).\"\"\"\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "def timer_ns(func):\n",
    "    \"\"\"Decorator that prints elapsed ns and stores last_elapsed_ns on wrapper.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"â±ï¸ {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "# A simple latency aggregator to produce final report\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    def add(self, component, ns):\n",
    "        self.store[component].append(ns)\n",
    "    def summary(self):\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // len(vals),\n",
    "                \"min_ns\": min(vals),\n",
    "                \"max_ns\": max(vals)\n",
    "            }\n",
    "        return out\n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LATENCY SUMMARY (nanoseconds)\")\n",
    "        print(\"=\"*60)\n",
    "        for comp, stats in sorted(s.items(), key=lambda p: p[0]):\n",
    "            print(f\"\\nComponent: {comp}\")\n",
    "            print(f\"  Count: {stats['count']}\")\n",
    "            print(f\"  Total: {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"  Avg:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"  Min:   {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"  Max:   {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# ---------------------------\n",
    "# PDF load / chunking / embeddings / pinecone\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def load_pdf(path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        pages = pdf.pages\n",
    "        # read page by page and time per-page (helpful for large PDFs)\n",
    "        page_texts = []\n",
    "        for i, p in enumerate(pages):\n",
    "            start_ns = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start_ns\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            page_texts.append(t)\n",
    "        text = \"\\n\".join(page_texts)\n",
    "    print(f\"ðŸ“„ Loaded PDF, total length: {len(text)} chars\")\n",
    "    return text\n",
    "\n",
    "@timer_ns\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"ðŸ“„ Total Chunks: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "@timer_ns\n",
    "def get_embeddings_model(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    # instantiate embedding model and return wrapper\n",
    "    emb = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return emb\n",
    "\n",
    "def init_pinecone(api_key, index_name=INDEX_NAME, dim=DIM):\n",
    "    start = time.time_ns()\n",
    "    pc = Pinecone(api_key=api_key)\n",
    "    existing_indexes = [idx.name for idx in pc.list_indexes()]\n",
    "    latency_report.add(\"pinecone_list_indexes\", time.time_ns() - start)\n",
    "\n",
    "    if index_name in existing_indexes:\n",
    "        print(f\"ðŸ—‘ï¸  Deleting existing index '{index_name}'...\")\n",
    "        start = time.time_ns()\n",
    "        pc.delete_index(index_name)\n",
    "        latency_report.add(\"pinecone_delete_index\", time.time_ns() - start)\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(f\"ðŸ†• Creating index '{index_name}'...\")\n",
    "    start = time.time_ns()\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dim,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    latency_report.add(\"pinecone_create_index\", time.time_ns() - start)\n",
    "    time.sleep(2)\n",
    "    print(f\"âœ… Index '{index_name}' created\")\n",
    "    return pc\n",
    "\n",
    "@timer_ns\n",
    "def create_vectorstore(embed_model, chunks, index_name=INDEX_NAME):\n",
    "    # measure embedding transformation + upsert to pinecone in blocks\n",
    "    # Note: from_texts will call embedding model internally - we measure the wrapper call\n",
    "    start = time.time_ns()\n",
    "    vectorstore = PineconeVectorStore.from_texts(\n",
    "        texts=chunks,\n",
    "        embedding=embed_model,\n",
    "        index_name=index_name,\n",
    "        namespace=\"\",\n",
    "        metadatas=[{\"source\": f\"chunk_{i}\", \"chunk_id\": i} for i in range(len(chunks))]\n",
    "    )\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"pinecone_upsert_total\", elapsed)\n",
    "    print(f\"âœ… Created vector store with {len(chunks)} chunks\")\n",
    "    return vectorstore\n",
    "\n",
    "# ---------------------------\n",
    "# Vader Sentiment (benchmark)\n",
    "# ---------------------------\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    def analyze(self, text):\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "            percentage = round((compound + 1) * 50, 2)\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "            percentage = round((1 - abs(compound)) * 50, 2)\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "            percentage = round(50 + (compound * 50), 2)\n",
    "        return {\n",
    "            'label': label,\n",
    "            'percentage': percentage,\n",
    "            'compound': compound,\n",
    "            'scores': scores\n",
    "        }\n",
    "\n",
    "def run_sentiment_benchmark(run_number, sa, examples, target_ns=TARGET_NS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ”¥ SENTIMENT RUN #{run_number}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    individual_times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start_ns = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed_ns = time.time_ns() - start_ns\n",
    "        latency_report.add(\"vader_per_example\", elapsed_ns)\n",
    "        individual_times.append(elapsed_ns)\n",
    "        status = \"âœ…\" if elapsed_ns < target_ns else \"âŒ\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed_ns):20s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    total_ns = sum(individual_times)\n",
    "    avg_ns = total_ns // len(individual_times)\n",
    "    min_ns = min(individual_times)\n",
    "    max_ns = max(individual_times)\n",
    "    under_target = sum(1 for t in individual_times if t < target_ns)\n",
    "    print(f\"\\nðŸ“Š RUN #{run_number} STATISTICS:\")\n",
    "    print(f\"   Total:        {format_time_ns(total_ns)}\")\n",
    "    print(f\"   Average:      {format_time_ns(avg_ns)}\")\n",
    "    print(f\"   Min:          {format_time_ns(min_ns)}\")\n",
    "    print(f\"   Max:          {format_time_ns(max_ns)}\")\n",
    "    print(f\"   < {target_ns}ns: {under_target}/{len(individual_times)} texts\")\n",
    "    if avg_ns < target_ns:\n",
    "        print(f\"   âœ… TARGET MET!\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  TARGET MISSED\")\n",
    "    return {\n",
    "        'run': run_number,\n",
    "        'times': individual_times,\n",
    "        'total': total_ns,\n",
    "        'avg': avg_ns,\n",
    "        'min': min_ns,\n",
    "        'max': max_ns,\n",
    "        'under_target': under_target\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# SELF-RAG with detailed LLM + retriever timings\n",
    "# ---------------------------\n",
    "class SelfRAG:\n",
    "    \"\"\"Self-Reflective RAG with full timing instrumentation.\"\"\"\n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        # create retriever object (if supported)\n",
    "        self.retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "    def _llm_invoke_timed(self, prompt, label):\n",
    "        \"\"\"Invoke LLM and record elapsed time under label.\"\"\"\n",
    "        # some LLMs provide .invoke(...) returning .content - keep robust\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label, elapsed)\n",
    "            # try to extract textual content\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label + \"_error\", elapsed)\n",
    "            print(f\"LLM invoke for {label} failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return str(e), elapsed\n",
    "\n",
    "    def retrieve_decision(self, question):\n",
    "        prompt = f\"\"\"You are a helpful assistant. Decide if you need to retrieve information from a document to answer this question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Think step by step:\n",
    "1. Can you answer this from general knowledge?\n",
    "2. Does it require specific document information?\n",
    "\n",
    "Answer with ONLY 'RETRIEVE' or 'NO_RETRIEVE' and a brief reason.\n",
    "\n",
    "Decision:\"\"\"\n",
    "        decision_text, elapsed = self._llm_invoke_timed(prompt, \"llm_retrieve_decision\")\n",
    "        needs_retrieval = 'RETRIEVE' in decision_text.upper() and 'NO_RETRIEVE' not in decision_text.upper()\n",
    "        print(f\"ðŸ¤” Retrieval Decision: {'RETRIEVE' if needs_retrieval else 'NO_RETRIEVE'}\")\n",
    "        print(f\"   Reasoning: {decision_text.strip()}\")\n",
    "        return needs_retrieval, decision_text\n",
    "\n",
    "    def retrieve_documents(self, question, k=4):\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            docs = self.retriever.invoke(question)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(\"retriever_search\", elapsed)\n",
    "            # docs might be list-like\n",
    "            count = len(docs) if hasattr(docs, \"__len__\") else 1\n",
    "            print(f\"ðŸ“š Retrieved {count} documents in {format_time_ns(elapsed)}\")\n",
    "            return docs, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(\"retriever_search_error\", elapsed)\n",
    "            print(f\"Retriever failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return [], elapsed\n",
    "\n",
    "    def generate_answer(self, question, context=\"\"):\n",
    "        if context:\n",
    "            prompt = f\"\"\"Answer the question based on the following context:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a detailed answer based on the context above.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Answer the following question based on your general knowledge:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        answer_text, elapsed = self._llm_invoke_timed(prompt, \"llm_generate_answer\")\n",
    "        print(f\"\\nðŸ’¬ Generated Answer (took {format_time_ns(elapsed)}):\\n{answer_text[:1000]}\")  # truncate long prints\n",
    "        return answer_text, elapsed\n",
    "\n",
    "    def self_critique(self, question, answer, context=\"\"):\n",
    "        critique_prompt = f\"\"\"You are a critical evaluator. Evaluate the following answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Context Available: {'Yes' if context else 'No'}\n",
    "\n",
    "Rate the answer on a scale of 1-10 and provide:\n",
    "1. Relevance Score (1-10)\n",
    "2. Completeness Score (1-10)\n",
    "3. Accuracy Assessment\n",
    "4. Should we retrieve more information? (YES/NO)\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "        critique_text, elapsed = self._llm_invoke_timed(critique_prompt, \"llm_self_critique\")\n",
    "        print(f\"\\nðŸ” Self-Critique (took {format_time_ns(elapsed)}):\\n{critique_text.strip()}\")\n",
    "        # decide if more retrieval is suggested\n",
    "        needs_more = 'YES' in critique_text.upper() and 'RETRIEVE' in critique_text.upper()\n",
    "        return critique_text, needs_more, elapsed\n",
    "\n",
    "    def query(self, question, max_iterations=2):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ðŸš€ SELF-RAG QUERY PROCESSING\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"â“ Question: {question}\\n\")\n",
    "\n",
    "        iteration = 0\n",
    "        context = \"\"\n",
    "        answer = \"\"\n",
    "        per_iteration_times = []\n",
    "\n",
    "        overall_start = time.time_ns()\n",
    "        while iteration < max_iterations:\n",
    "            iter_start = time.time_ns()\n",
    "            iteration += 1\n",
    "            print(f\"\\n--- Iteration {iteration} ---\")\n",
    "\n",
    "            # Step 1: Decide if retrieval is needed\n",
    "            if iteration == 1:\n",
    "                needs_retrieval, decision_reason = self.retrieve_decision(question)\n",
    "            else:\n",
    "                needs_retrieval = True  # force retrieval when refining\n",
    "\n",
    "            # Step 2: Retrieve if needed\n",
    "            retriever_elapsed = 0\n",
    "            if needs_retrieval:\n",
    "                docs, retriever_elapsed = self.retrieve_documents(question)\n",
    "                if docs:\n",
    "                    # join docs' content (robust extraction)\n",
    "                    context = \"\\n\\n\".join([\n",
    "                        getattr(doc, \"page_content\", None) or getattr(doc, \"content\", None) or str(doc)\n",
    "                        for doc in docs\n",
    "                    ])\n",
    "                    latency_report.add(\"context_build\", len(context))  # note: storing char count as metric\n",
    "                    print(f\"ðŸ“ Context length: {len(context)} characters\")\n",
    "\n",
    "            # Step 3: Generate answer\n",
    "            gen_answer, gen_elapsed = self.generate_answer(question, context)\n",
    "\n",
    "            # Step 4: Self-critique\n",
    "            critique_text, needs_more, critique_elapsed = self.self_critique(question, gen_answer, context)\n",
    "\n",
    "            iter_elapsed = time.time_ns() - iter_start\n",
    "            per_iteration_times.append(iter_elapsed)\n",
    "            latency_report.add(\"selfrag_iteration\", iter_elapsed)\n",
    "            print(f\"\\nâ±ï¸ Iteration {iteration} total time: {format_time_ns(iter_elapsed)}\")\n",
    "\n",
    "            # Step 5: Decide loop break\n",
    "            if not needs_more or iteration >= max_iterations:\n",
    "                print(f\"\\nâœ… Self-RAG completed after {iteration} iteration(s)\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"\\nðŸ”„ Refinement needed, starting iteration {iteration + 1}...\")\n",
    "\n",
    "        total_query_ns = time.time_ns() - overall_start\n",
    "        latency_report.add(\"selfrag_query_total\", total_query_ns)\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': gen_answer,\n",
    "            'context': context,\n",
    "            'iterations': iteration,\n",
    "            'critique': critique_text,\n",
    "            'per_iteration_times': per_iteration_times,\n",
    "            'total_query_ns': total_query_ns\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline orchestration\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸš€ SELF-RAG PIPELINE + FULL LATENCY INSTRUMENTATION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Phase 1: Load PDF + chunk + embeddings + pinecone init + upsert\n",
    "    start_total = time.time_ns()\n",
    "    try:\n",
    "        pdf_text, t_pdf = timed_call(load_pdf, PDF_PATH)\n",
    "        latency_report.add(\"pdf_load\", t_pdf)\n",
    "\n",
    "        chunks, t_chunks = timed_call(chunk_text, pdf_text, 1000, 100)\n",
    "        latency_report.add(\"chunking\", t_chunks)\n",
    "\n",
    "        embed_model, t_emb_load = timed_call(get_embeddings_model, \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        latency_report.add(\"embedding_model_init\", t_emb_load)\n",
    "\n",
    "        # pinecone init + index create\n",
    "        pc_start = time.time_ns()\n",
    "        pc = init_pinecone(PINECONE_API_KEY, INDEX_NAME, DIM)\n",
    "        pc_elapsed = time.time_ns() - pc_start\n",
    "        latency_report.add(\"pinecone_init_total\", pc_elapsed)\n",
    "\n",
    "        # create vectorstore (embedding + upsert)\n",
    "        vs_start = time.time_ns()\n",
    "        vectorstore = create_vectorstore(embed_model, chunks, INDEX_NAME)\n",
    "        vs_elapsed = time.time_ns() - vs_start\n",
    "        latency_report.add(\"vectorstore_create_total\", vs_elapsed)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in Phase 1:\", e)\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Initialize LLM\n",
    "    try:\n",
    "        start_ns = time.time_ns()\n",
    "        llm = ChatGroq(model_name=MODEL_NAME, temperature=0, groq_api_key=GROQ_API_KEY)\n",
    "        elapsed_ns = time.time_ns() - start_ns\n",
    "        latency_report.add(\"llm_init\", elapsed_ns)\n",
    "        print(f\"âœ… LLM initialized in {format_time_ns(elapsed_ns)}\")\n",
    "    except Exception as e:\n",
    "        print(\"LLM init failed:\", e)\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Initialize Self-RAG\n",
    "    self_rag = SelfRAG(vectorstore, llm)\n",
    "    print(\"\\nâœ… Self-RAG system initialized!\")\n",
    "\n",
    "    # Phase 2: Self-RAG queries (example set)\n",
    "    print(\"\\n\\nðŸ“š PHASE 2: SELF-RAG QUERIES\")\n",
    "    queries = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Summarize the key events in the document.\",\n",
    "        \"What is the capital of France?\"  # general knowledge\n",
    "    ]\n",
    "    rag_results = []\n",
    "    for q in queries:\n",
    "        q_start = time.time_ns()\n",
    "        res = self_rag.query(q, max_iterations=2)\n",
    "        q_elapsed = time.time_ns() - q_start\n",
    "        latency_report.add(\"query_loop\", q_elapsed)\n",
    "        rag_results.append(res)\n",
    "        print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "    # Phase 3: VADER Sentiment Benchmark\n",
    "    print(\"\\nðŸ“š PHASE 3: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"ðŸŽ¯ TARGET: < {TARGET_NS} ns per analysis\\n\")\n",
    "\n",
    "    sa_start = time.time_ns()\n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    sa_init_ns = time.time_ns() - sa_start\n",
    "    latency_report.add(\"vader_init\", sa_init_ns)\n",
    "    print(f\"âœ… VADER INIT TIME: {format_time_ns(sa_init_ns)}\\n\")\n",
    "\n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    runs = []\n",
    "    for run in range(1, 4):\n",
    "        r = run_sentiment_benchmark(run, sa, examples, TARGET_NS)\n",
    "        runs.append(r)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # Final aggregation & report\n",
    "    overall_ns = time.time_ns() - start_total\n",
    "    latency_report.add(\"pipeline_total\", overall_ns)\n",
    "    print(\"\\nðŸ“ˆ AGGREGATE STATISTICS\")\n",
    "    print(f\"   Full pipeline time: {format_time_ns(overall_ns)}\")\n",
    "    print(f\"   Queries executed: {len(queries)}\")\n",
    "    latency_report.pretty_print()\n",
    "\n",
    "    # Optionally pretty-print RAG results\n",
    "    print(\"\\nSample RAG results (truncated answers):\")\n",
    "    for r in rag_results:\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Q: {r['question']}\")\n",
    "        ans_preview = (r['answer'] or \"\")[:800]\n",
    "        print(f\"A (preview): {ans_preview}\")\n",
    "        print(f\"Iterations: {r['iterations']}, Query time: {format_time_ns(r['total_query_ns'])}\")\n",
    "    print(\"\\nâœ… PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
