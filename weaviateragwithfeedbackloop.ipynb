{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "weaviate_feedback_rag.py (COMPLETED)\n",
    "RAG with Feedback Loops and comprehensive nanosecond latency instrumentation.\n",
    "\n",
    "Notes:\n",
    "- This file completes the pipeline: PDF load -> chunk -> embed -> upsert -> feedback RAG loop.\n",
    "- The LLM wrapper attempts to use langchain_groq.ChatGroq when available; otherwise falls back to a safe echo stub for offline testing.\n",
    "- All calls are timed and recorded in LatencyReport.\n",
    "\n",
    "Run:\n",
    "    python weaviate_feedback_rag_completed.py --pdf \"Data/ECHOES OF HER LOVE.pdf\" --test_question \"Who is the protagonist?\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import pdfplumber\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Optional LLM import (may not be installed in all environments)\n",
    "try:\n",
    "    from langchain_groq import ChatGroq\n",
    "except Exception:\n",
    "    ChatGroq = None\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION_NAME = \"FeedbackRAG_Documents\"\n",
    "DIM = 384  # MiniLM embedding dimension\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000\n",
    "MAX_FEEDBACK_LOOPS = 3  # Maximum number of feedback iterations\n",
    "\n",
    "# Weaviate credentials (keep safe)\n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\", \"21ookhjbswyl5urlawqmxw.c0.asia-southeast1.gcp.weaviate.cloud\")\n",
    "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\", \"NTVWQ1dZVDI1bkptcndrZF9JRTFySVg3TEFBc1R5V0luUEtHaU9MajB6am5VQkc3aG5yVkgwWkFQVDc0PV92MjAw\")\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Quality thresholds\n",
    "RELEVANCE_THRESHOLD = 7  # Out of 10\n",
    "COMPLETENESS_THRESHOLD = 7  # Out of 10\n",
    "CONFIDENCE_THRESHOLD = 6  # Out of 10\n",
    "\n",
    "# =========================================================\n",
    "# LATENCY UTILITIES (same as provided)\n",
    "# =========================================================\n",
    "\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    \"\"\"Return human-readable representation of nanoseconds.\"\"\"\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    \"\"\"Call fn(*args, **kwargs) and return (result, elapsed_ns).\"\"\"\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "\n",
    "def timer_ns(func):\n",
    "    \"\"\"Decorator that prints elapsed ns and stores last_elapsed_ns on wrapper.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"‚è±Ô∏è  {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class LatencyReport:\n",
    "    \"\"\"Aggregates and reports latency metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    \n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    \n",
    "    def summary(self) -> Dict:\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // len(vals) if vals else 0,\n",
    "                \"min_ns\": min(vals) if vals else 0,\n",
    "                \"max_ns\": max(vals) if vals else 0\n",
    "            }\n",
    "        return out\n",
    "    \n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY (nanoseconds)\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items(), key=lambda p: p[0]):\n",
    "            print(f\"\\nüìä Component: {comp}\")\n",
    "            print(f\"   Count:     {stats['count']}\")\n",
    "            print(f\"   Total:     {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"   Average:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"   Min:       {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"   Max:       {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# =========================================================\n",
    "# PDF / chunk / embeddings / weaviate functions\n",
    "# =========================================================\n",
    "\n",
    "@timer_ns\n",
    "def load_pdf(path: str) -> str:\n",
    "    \"\"\"Load PDF with per-page timing\"\"\"\n",
    "    print(f\"üìÑ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, p in enumerate(pdf.pages):\n",
    "            start_ns = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start_ns\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            text += t + \"\\n\"\n",
    "    print(f\"‚úÖ Loaded PDF: {len(text)} characters from {len(pdf.pages)} pages\")\n",
    "    return text\n",
    "\n",
    "\n",
    "@timer_ns\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    \"\"\"Chunk text with timing\"\"\"\n",
    "    print(f\"‚úÇÔ∏è  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "@timer_ns\n",
    "def load_embeddings(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    \"\"\"Load embedding model with timing\"\"\"\n",
    "    print(f\"üî¢ Loading embeddings model: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    print(f\"‚úÖ Embeddings model loaded\")\n",
    "    return embedder\n",
    "\n",
    "\n",
    "@timer_ns\n",
    "def init_weaviate(url: str, api_key: str, collection_name: str = COLLECTION_NAME) -> weaviate.WeaviateClient:\n",
    "    \"\"\"Initialize Weaviate client and collection with timing\"\"\"\n",
    "    print(f\"üóÉÔ∏è  Initializing Weaviate connection to {url}\")\n",
    "    start = time.time_ns()\n",
    "\n",
    "    # Connect to Weaviate Cloud\n",
    "    client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=url,\n",
    "        auth_credentials=Auth.api_key(api_key)\n",
    "    )\n",
    "\n",
    "    connect_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_connect\", connect_time)\n",
    "    print(f\"‚úÖ Connected to Weaviate ({format_time_ns(connect_time)})\")\n",
    "\n",
    "    # Delete collection if exists\n",
    "    try:\n",
    "        if client.collections.exists(collection_name):\n",
    "            start = time.time_ns()\n",
    "            client.collections.delete(collection_name)\n",
    "            delete_time = time.time_ns() - start\n",
    "            latency_report.add(\"weaviate_delete_collection\", delete_time)\n",
    "            print(f\"üóëÔ∏è  Deleted existing collection '{collection_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Collection check/delete: {e}\")\n",
    "\n",
    "    # Create collection\n",
    "    start = time.time_ns()\n",
    "    try:\n",
    "        client.collections.create(\n",
    "            name=collection_name,\n",
    "            vectorizer_config=None,  # We'll provide vectors manually\n",
    "            properties=[\n",
    "                {\"name\": \"text\", \"dataType\": [\"text\"]},\n",
    "                {\"name\": \"chunk_id\", \"dataType\": [\"int\"]},\n",
    "                {\"name\": \"source\", \"dataType\": [\"text\"]}\n",
    "            ]\n",
    "        )\n",
    "        create_time = time.time_ns() - start\n",
    "        latency_report.add(\"weaviate_create_collection\", create_time)\n",
    "        print(f\"‚úÖ Collection '{collection_name}' created ({format_time_ns(create_time)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Collection creation: {e}\")\n",
    "\n",
    "    return client\n",
    "\n",
    "\n",
    "@timer_ns\n",
    "def insert_chunks(client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                  chunks: List[str], collection_name: str = COLLECTION_NAME) -> None:\n",
    "    \"\"\"Insert chunks into Weaviate with detailed timing\"\"\"\n",
    "    print(f\"‚¨ÜÔ∏è  Inserting {len(chunks)} chunks into Weaviate...\")\n",
    "\n",
    "    # Encode chunks (batch embedding)\n",
    "    print(f\"   üî¢ Encoding {len(chunks)} chunks...\")\n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_encode_batch\", encode_time)\n",
    "    print(f\"   ‚úÖ Encoded in {format_time_ns(encode_time)}\")\n",
    "\n",
    "    # Get collection\n",
    "    collection = client.collections.get(collection_name)\n",
    "\n",
    "    # Insert objects with vectors\n",
    "    print(f\"   üíæ Upserting to Weaviate...\")\n",
    "    start = time.time_ns()\n",
    "\n",
    "    with collection.batch.dynamic() as batch:\n",
    "        for i, (chunk, vector) in enumerate(zip(chunks, vectors)):\n",
    "            batch.add_object(\n",
    "                properties={\n",
    "                    \"text\": chunk,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"source\": f\"chunk_{i}\"\n",
    "                },\n",
    "                vector=vector.tolist()\n",
    "            )\n",
    "\n",
    "    upsert_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_upsert\", upsert_time)\n",
    "    print(f\"   ‚úÖ Upserted in {format_time_ns(upsert_time)}\")\n",
    "\n",
    "    print(f\"‚úÖ All chunks inserted successfully!\")\n",
    "\n",
    "\n",
    "# search_weaviate unchanged\n",
    "\n",
    "def search_weaviate(client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                    query: str, limit: int = 4, collection_name: str = COLLECTION_NAME) -> Tuple[List[str], int]:\n",
    "    \"\"\"Search Weaviate with timing\"\"\"\n",
    "    # Encode query\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"query_embedding\", encode_time)\n",
    "\n",
    "    # Query Weaviate\n",
    "    start = time.time_ns()\n",
    "    collection = client.collections.get(collection_name)\n",
    "\n",
    "    response = collection.query.near_vector(\n",
    "        near_vector=qvec.tolist(),\n",
    "        limit=limit,\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "\n",
    "    search_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_search\", search_time)\n",
    "\n",
    "    # Extract texts\n",
    "    hits = [obj.properties.get(\"text\", \"\") for obj in response.objects]\n",
    "\n",
    "    total_time = encode_time + search_time\n",
    "\n",
    "    return hits, total_time\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# FEEDBACK QUALITY ASSESSMENT + FeedbackRAG\n",
    "# (copy as provided, minor addition: type hints and small fixes)\n",
    "# =========================================================\n",
    "\n",
    "class FeedbackAssessment:\n",
    "    def __init__(self, relevance: int, completeness: int, confidence: int,\n",
    "                 issues: List[str], suggestions: List[str]):\n",
    "        self.relevance = relevance\n",
    "        self.completeness = completeness\n",
    "        self.confidence = confidence\n",
    "        self.issues = issues\n",
    "        self.suggestions = suggestions\n",
    "    \n",
    "    def is_satisfactory(self) -> bool:\n",
    "        return (self.relevance >= RELEVANCE_THRESHOLD and \n",
    "                self.completeness >= COMPLETENESS_THRESHOLD and\n",
    "                self.confidence >= CONFIDENCE_THRESHOLD)\n",
    "    \n",
    "    def get_score_summary(self) -> str:\n",
    "        return f\"R:{self.relevance}/10, C:{self.completeness}/10, Conf:{self.confidence}/10\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"FeedbackAssessment(relevance={self.relevance}, \"\n",
    "                f\"completeness={self.completeness}, confidence={self.confidence})\")\n",
    "\n",
    "\n",
    "class LLMWrapper:\n",
    "    \"\"\"Small wrapper that exposes an .invoke(prompt) method used by FeedbackRAG.\n",
    "\n",
    "    It tries a few ways to call the real LLM (ChatGroq or similar). If none\n",
    "    available, it returns a safe placeholder response so the rest of the\n",
    "    pipeline can be tested offline.\n",
    "    \"\"\"\n",
    "    def __init__(self, groq_api_key: str = None, model: str = MODEL_NAME):\n",
    "        self.model = model\n",
    "        self.groq_api_key = groq_api_key\n",
    "        self.client = None\n",
    "        if ChatGroq and groq_api_key:\n",
    "            try:\n",
    "                # Try to instantiate ChatGroq (API may differ between versions)\n",
    "                self.client = ChatGroq(api_key=groq_api_key, model=self.model)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    # alternative constructor\n",
    "                    self.client = ChatGroq(api_key=groq_api_key)\n",
    "                except Exception:\n",
    "                    self.client = None\n",
    "\n",
    "    def invoke(self, prompt: str) -> Any:\n",
    "        # If we have a ChatGroq-like client, try multiple call patterns\n",
    "        if self.client is not None:\n",
    "            try:\n",
    "                if hasattr(self.client, 'invoke'):\n",
    "                    return self.client.invoke(prompt)\n",
    "                if hasattr(self.client, 'generate'):\n",
    "                    # some wrappers accept a list of messages or prompts\n",
    "                    out = self.client.generate([prompt])\n",
    "                    # attempt to extract textual content\n",
    "                    if hasattr(out, 'generations'):\n",
    "                        return out.generations[0].text\n",
    "                    return str(out)\n",
    "                if callable(self.client):\n",
    "                    return self.client(prompt)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è LLM client call failed: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "        # Fallback: very simple deterministic \"LLM\" for testing\n",
    "        class _Resp:\n",
    "            def __init__(self, content):\n",
    "                self.content = content\n",
    "            def __str__(self):\n",
    "                return self.content\n",
    "\n",
    "        # Heuristic stub: echo prompt head + a canned completion\n",
    "        head = prompt.strip().split('\\n')[:6]\n",
    "        preview = '\\n'.join(head)\n",
    "        stub = (\n",
    "            f\"[STUB LLM RESPONSE]\\n\\nPrompt preview:\\n{preview}\\n\\n\"\n",
    "            \"Note: this is a fallback response (no Groq key detected).\"\n",
    "        )\n",
    "        return _Resp(stub)\n",
    "\n",
    "\n",
    "class FeedbackRAG:\n",
    "    # (Use exactly the class provided by the user, but accept LLMWrapper and other types)\n",
    "    def __init__(self, llm, client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                 collection_name: str = COLLECTION_NAME, max_loops: int = MAX_FEEDBACK_LOOPS):\n",
    "        self.llm = llm\n",
    "        self.client = client\n",
    "        self.embedder = embedder\n",
    "        self.collection_name = collection_name\n",
    "        self.max_loops = max_loops\n",
    "\n",
    "    def _llm_invoke_timed(self, prompt: str, label: str) -> Tuple[str, int]:\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label, elapsed)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label + \"_error\", elapsed)\n",
    "            print(f\"‚ùå LLM invoke for {label} failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return str(e), elapsed\n",
    "\n",
    "    def retrieve_documents(self, query: str, k: int = 4) -> Tuple[str, int]:\n",
    "        print(f\"      üîç Retrieving documents...\")\n",
    "        print(f\"         Query: {query[:70]}{'...' if len(query) > 70 else ''}\")\n",
    "        hits, elapsed = search_weaviate(self.client, self.embedder, query, k, self.collection_name)\n",
    "        context = \"\\n\\n\".join(hits)\n",
    "        print(f\"      ‚úÖ Retrieved {len(hits)} docs ({len(context)} chars) in {format_time_ns(elapsed)}\")\n",
    "        return context, elapsed\n",
    "\n",
    "    def generate_answer(self, query: str, context: str, previous_feedback: str = \"\") -> Tuple[str, int]:\n",
    "        if previous_feedback:\n",
    "            prompt = f\"\"\"You are improving a previous answer based on feedback.\n",
    "\n",
    "Previous Feedback Issues:\n",
    "{previous_feedback}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Instructions: Generate an IMPROVED answer that specifically addresses the feedback issues. Be thorough, relevant, and well-supported by the context.\n",
    "\n",
    "Improved Answer:\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Answer the following question based on the retrieved context.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Provide a clear, comprehensive, and well-structured answer based on the context above.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        print(f\"      üí° Generating answer...\")\n",
    "        answer, elapsed = self._llm_invoke_timed(prompt, \"llm_generate_answer\")\n",
    "        print(f\"      ‚úÖ Generated ({len(answer)} chars) in {format_time_ns(elapsed)}\")\n",
    "        return answer, elapsed\n",
    "\n",
    "    def assess_answer_quality(self, query: str, answer: str, context: str,\n",
    "                              loop_num: int) -> Tuple[FeedbackAssessment, str, int]:\n",
    "        prompt = f\"\"\"You are a strict quality evaluator. Assess this answer critically.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Context Length: {len(context)} characters\n",
    "\n",
    "Evaluate on these criteria (1-10 scale, be strict):\n",
    "\n",
    "1. RELEVANCE: Does it directly answer the question?\n",
    "2. COMPLETENESS: Does it cover all aspects needed?\n",
    "3. CONFIDENCE: Is it well-supported by context?\n",
    "\n",
    "Format your response EXACTLY as:\n",
    "RELEVANCE: [number 1-10]\n",
    "COMPLETENESS: [number 1-10]\n",
    "CONFIDENCE: [number 1-10]\n",
    "\n",
    "ISSUES:\n",
    "- [specific problem 1]\n",
    "- [specific problem 2]\n",
    "\n",
    "SUGGESTIONS:\n",
    "- [improvement 1]\n",
    "- [improvement 2]\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "        print(f\"      üîç Assessing quality...\")\n",
    "        feedback_text, elapsed = self._llm_invoke_timed(prompt, f\"llm_feedback_assessment_{loop_num}\")\n",
    "\n",
    "        # Parse feedback with robust extraction\n",
    "        relevance, completeness, confidence = 5, 5, 5  # defaults\n",
    "        issues, suggestions = [], []\n",
    "\n",
    "        lines = feedback_text.split('\\n')\n",
    "        current_section = None\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            upper_line = line.upper()\n",
    "\n",
    "            if 'RELEVANCE:' in upper_line or 'RELEVANCE =' in upper_line:\n",
    "                try:\n",
    "                    nums = [int(c) for c in line if c.isdigit()]\n",
    "                    if nums:\n",
    "                        relevance = min(10, max(1, nums[0] if len(nums) == 1 else int(''.join(map(str, nums[:2])))))\n",
    "                except:\n",
    "                    pass\n",
    "            elif 'COMPLETENESS:' in upper_line or 'COMPLETENESS =' in upper_line:\n",
    "                try:\n",
    "                    nums = [int(c) for c in line if c.isdigit()]\n",
    "                    if nums:\n",
    "                        completeness = min(10, max(1, nums[0] if len(nums) == 1 else int(''.join(map(str, nums[:2])))))\n",
    "                except:\n",
    "                    pass\n",
    "            elif 'CONFIDENCE:' in upper_line or 'CONFIDENCE =' in upper_line:\n",
    "                try:\n",
    "                    nums = [int(c) for c in line if c.isdigit()]\n",
    "                    if nums:\n",
    "                        confidence = min(10, max(1, nums[0] if len(nums) == 1 else int(''.join(map(str, nums[:2])))))\n",
    "                except:\n",
    "                    pass\n",
    "            elif 'ISSUES:' in upper_line:\n",
    "                current_section = 'issues'\n",
    "            elif 'SUGGESTIONS:' in upper_line or 'IMPROVEMENTS:' in upper_line:\n",
    "                current_section = 'suggestions'\n",
    "            elif line.startswith('-') and current_section == 'issues':\n",
    "                issues.append(line[1:].strip())\n",
    "            elif line.startswith('-') and current_section == 'suggestions':\n",
    "                suggestions.append(line[1:].strip())\n",
    "\n",
    "        assessment = FeedbackAssessment(relevance, completeness, confidence, issues, suggestions)\n",
    "\n",
    "        print(f\"      üìä Scores: {assessment.get_score_summary()}\")\n",
    "        print(f\"      ‚úÖ Assessment done in {format_time_ns(elapsed)}\")\n",
    "\n",
    "        return assessment, feedback_text, elapsed\n",
    "\n",
    "    def reformulate_query(self, original_query: str, feedback: FeedbackAssessment,\n",
    "                         loop_num: int) -> Tuple[str, int]:\n",
    "        issues_text = '\\n'.join(f'- {issue}' for issue in feedback.issues[:3]) if feedback.issues else '- Information lacks depth'\n",
    "        suggestions_text = '\\n'.join(f'- {sug}' for sug in feedback.suggestions[:3]) if feedback.suggestions else '- Need more specific information'\n",
    "\n",
    "        prompt = f\"\"\"Reformulate this search query to get better retrieval results.\n",
    "\n",
    "Original Question: {original_query}\n",
    "\n",
    "Current Problems:\n",
    "{issues_text}\n",
    "\n",
    "What's Needed:\n",
    "{suggestions_text}\n",
    "\n",
    "Task: Create a MORE SPECIFIC search query that will find the missing information. Focus on:\n",
    "1. Key terms from issues/suggestions\n",
    "2. More specific aspects needed\n",
    "3. Alternative phrasings\n",
    "\n",
    "Keep it concise (under 150 chars).\n",
    "\n",
    "Reformulated Query:\"\"\"\n",
    "\n",
    "        print(f\"      üîÑ Reformulating query...\")\n",
    "        reformulated, elapsed = self._llm_invoke_timed(prompt, f\"llm_query_reformulation_{loop_num}\")\n",
    "\n",
    "        # Clean up\n",
    "        reformulated = reformulated.strip().split('\\n')[0][:150]\n",
    "        if not reformulated or len(reformulated) < 10:\n",
    "            reformulated = original_query  # Fallback\n",
    "\n",
    "        print(f\"      ‚úÖ New query: {reformulated[:70]}{'...' if len(reformulated) > 70 else ''}\")\n",
    "        print(f\"      ‚è±Ô∏è  Reformulation: {format_time_ns(elapsed)}\")\n",
    "\n",
    "        return reformulated, elapsed\n",
    "\n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîÅ RAG WITH FEEDBACK LOOPS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "\n",
    "        overall_start = time.time_ns()\n",
    "\n",
    "        current_query = question\n",
    "        context = \"\"\n",
    "        answer = \"\"\n",
    "        feedback_history = []\n",
    "        loop_results = []\n",
    "\n",
    "        for loop_num in range(1, self.max_loops + 1):\n",
    "            loop_start = time.time_ns()\n",
    "            print(f\"\\n{'‚îÄ'*70}\")\n",
    "            print(f\"üîÑ FEEDBACK LOOP {loop_num}/{self.max_loops}\")\n",
    "            print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "            # Step 1: Retrieve documents\n",
    "            context, retrieval_time = self.retrieve_documents(current_query, k=4)\n",
    "            latency_report.add(f\"loop_{loop_num}_retrieval\", retrieval_time)\n",
    "\n",
    "            # Step 2: Generate answer\n",
    "            previous_feedback_summary = \"\"\n",
    "            if feedback_history:\n",
    "                prev = feedback_history[-1]['assessment']\n",
    "                issues_summary = '; '.join(prev.issues[:2]) if prev.issues else 'General improvements needed'\n",
    "                previous_feedback_summary = f\"Previous issues: {issues_summary}\"\n",
    "\n",
    "            answer, generation_time = self.generate_answer(question, context, previous_feedback_summary)\n",
    "            latency_report.add(f\"loop_{loop_num}_generation\", generation_time)\n",
    "\n",
    "            # Step 3: Assess quality\n",
    "            assessment, feedback_text, assessment_time = self.assess_answer_quality(\n",
    "                question, answer, context, loop_num\n",
    "            )\n",
    "            latency_report.add(f\"loop_{loop_num}_assessment\", assessment_time)\n",
    "\n",
    "            # Record loop results\n",
    "            loop_elapsed = time.time_ns() - loop_start\n",
    "            loop_result = {\n",
    "                'loop_num': loop_num,\n",
    "                'query': current_query,\n",
    "                'context_length': len(context),\n",
    "                'answer': answer,\n",
    "                'assessment': assessment,\n",
    "                'feedback_text': feedback_text,\n",
    "                'time_ns': loop_elapsed\n",
    "            }\n",
    "            loop_results.append(loop_result)\n",
    "            feedback_history.append({'assessment': assessment, 'text': feedback_text})\n",
    "\n",
    "            latency_report.add(\"feedback_loop_iteration\", loop_elapsed)\n",
    "            print(f\"\\n      ‚è±Ô∏è  Loop {loop_num} total: {format_time_ns(loop_elapsed)}\")\n",
    "\n",
    "            # Step 4: Check if satisfactory\n",
    "            if assessment.is_satisfactory():\n",
    "                print(f\"\\n      ‚úÖ Quality thresholds MET!\")\n",
    "                print(f\"      üéØ Success in {loop_num} loop(s)\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"\\n      ‚ö†Ô∏è  Below threshold:\")\n",
    "                if assessment.relevance < RELEVANCE_THRESHOLD:\n",
    "                    print(f\"         ‚ùå Relevance: {assessment.relevance}/10 (need ‚â•{RELEVANCE_THRESHOLD})\")\n",
    "                if assessment.completeness < COMPLETENESS_THRESHOLD:\n",
    "                    print(f\"         ‚ùå Completeness: {assessment.completeness}/10 (need ‚â•{COMPLETENESS_THRESHOLD})\")\n",
    "                if assessment.confidence < CONFIDENCE_THRESHOLD:\n",
    "                    print(f\"         ‚ùå Confidence: {assessment.confidence}/10 (need ‚â•{CONFIDENCE_THRESHOLD})\")\n",
    "\n",
    "                if assessment.issues:\n",
    "                    print(f\"         Issues: {assessment.issues[0][:60]}{'...' if len(assessment.issues[0]) > 60 else ''}\")\n",
    "\n",
    "                if loop_num < self.max_loops:\n",
    "                    # Reformulate for next iteration\n",
    "                    current_query, reform_time = self.reformulate_query(\n",
    "                        question, assessment, loop_num\n",
    "                    )\n",
    "                    latency_report.add(f\"loop_{loop_num}_reformulation\", reform_time)\n",
    "                    print(f\"      üîÑ Will retry with improved query...\")\n",
    "                else:\n",
    "                    print(f\"      ‚ö†Ô∏è  Max loops reached. Using best available answer.\")\n",
    "\n",
    "        total_query_ns = time.time_ns() - overall_start\n",
    "        latency_report.add(\"feedback_rag_total\", total_query_ns)\n",
    "\n",
    "        # Display final results\n",
    "        final_assessment = loop_results[-1]['assessment']\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üí¨ FINAL ANSWER (after {len(loop_results)} loop(s)):\" )\n",
    "        print(f\"{'='*70}\")\n",
    "        print(answer[:800])\n",
    "        if len(answer) > 800:\n",
    "            print(f\"... [truncated, full length: {len(answer)} chars]\")\n",
    "\n",
    "        print(f\"\\nüìä FINAL QUALITY SCORES:\")\n",
    "        print(f\"   Relevance:    {final_assessment.relevance}/10 {'‚úÖ' if final_assessment.relevance >= RELEVANCE_THRESHOLD else '‚ùå'}\")\n",
    "        print(f\"   Completeness: {final_assessment.completeness}/10 {'‚úÖ' if final_assessment.completeness >= COMPLETENESS_THRESHOLD else '‚ùå'}\")\n",
    "        print(f\"   Confidence:   {final_assessment.confidence}/10 {'‚úÖ' if final_assessment.confidence >= CONFIDENCE_THRESHOLD else '‚ùå'}\")\n",
    "        print(f\"   Overall:      {'‚úÖ SATISFACTORY' if final_assessment.is_satisfactory() else '‚ö†Ô∏è  NEEDS IMPROVEMENT'}\")\n",
    "\n",
    "        print(f\"\\n‚è±Ô∏è  TIMING METRICS:\")\n",
    "        print(f\"   Total time:   {format_time_ns(total_query_ns)}\")\n",
    "        print(f\"   Loops:        {len(loop_results)}\")\n",
    "        print(f\"   Avg/loop:     {format_time_ns(sum(r['time_ns'] for r in loop_results) // len(loop_results))}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'loop_results': loop_results,\n",
    "            'final_answer': answer,\n",
    "            'final_assessment': final_assessment,\n",
    "            'num_loops': len(loop_results),\n",
    "            'satisfied': final_assessment.is_satisfactory(),\n",
    "            'total_query_ns': total_query_ns\n",
    "        }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# VADER SENTIMENT BENCHMARK (unchanged)\n",
    "# =========================================================\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        \n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "            percentage = round((compound + 1) * 50, 2)\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "            percentage = round((1 - abs(compound)) * 50, 2)\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "            percentage = round(50 + (compound * 50), 2)\n",
    "        \n",
    "        return {\n",
    "            'label': label,\n",
    "            'percentage': percentage,\n",
    "            'compound': compound,\n",
    "            'scores': scores\n",
    "        }\n",
    "\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str],\n",
    "                            target_ns: int = TARGET_NS, run_number: int = 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî• SENTIMENT BENCHMARK RUN #{run_number}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üéØ TARGET: < {target_ns} ns per analysis\\n\")\n",
    "\n",
    "    individual_times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start_ns = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed_ns = time.time_ns() - start_ns\n",
    "        latency_report.add(\"vader_analysis\", elapsed_ns)\n",
    "        individual_times.append(elapsed_ns)\n",
    "\n",
    "        status = \"‚úÖ\" if elapsed_ns < target_ns else \"‚ùå\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed_ns):25s} {status} | {result['label']:8s} | \\\"{text[:40]}...\\\"\")\n",
    "\n",
    "    total_ns = sum(individual_times)\n",
    "    avg_ns = total_ns // len(individual_times)\n",
    "    min_ns = min(individual_times)\n",
    "    max_ns = max(individual_times)\n",
    "    under_target = sum(1 for t in individual_times if t < target_ns)\n",
    "\n",
    "    print(f\"\\nüìä RUN #{run_number} STATISTICS:\")\n",
    "    print(f\"   Total:     {format_time_ns(total_ns)}\")\n",
    "    print(f\"   Average:   {format_time_ns(avg_ns)}\")\n",
    "    print(f\"   Min:       {format_time_ns(min_ns)}\")\n",
    "    print(f\"   Max:       {format_time_ns(max_ns)}\")\n",
    "    print(f\"   Success:   {under_target}/{len(individual_times)} under target\")\n",
    "\n",
    "    if avg_ns < target_ns:\n",
    "        print(f\"   ‚úÖ TARGET MET!\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  TARGET MISSED\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# MAIN: orchestration and CLI\n",
    "# =========================================================\n",
    "\n",
    "def build_and_index(pdf_path: str, client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                    collection_name: str = COLLECTION_NAME):\n",
    "    text = load_pdf(pdf_path)\n",
    "    chunks = chunk_text(text)\n",
    "    insert_chunks(client, embedder, chunks, collection_name)\n",
    "    return len(chunks)\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Weaviate Feedback RAG pipeline\")\n",
    "    parser.add_argument('--pdf', type=str, default=PDF_PATH, help='Path to PDF to index')\n",
    "    parser.add_argument('--test_question', type=str, default=None, help='Example question to run through RAG')\n",
    "    parser.add_argument('--skip_index', action='store_true', help='Skip indexing and assume collection already exists')\n",
    "    parser.add_argument('--run_sentiment', action='store_true', help='Run sentiment benchmark')\n",
    "    args = parser.parse_args(argv)\n",
    "\n",
    "    # Load embedder\n",
    "    embedder = load_embeddings()\n",
    "\n",
    "    # Init weaviate\n",
    "    if not WEAVIATE_API_KEY or not WEAVIATE_URL:\n",
    "        print(\"‚ö†Ô∏è  Weaviate URL / API key not set. Exiting.\")\n",
    "        return\n",
    "\n",
    "    client = init_weaviate(WEAVIATE_URL, WEAVIATE_API_KEY)\n",
    "\n",
    "    if not args.skip_index:\n",
    "        print(\"\\nIndexing PDF into Weaviate (this may take a while)...\")\n",
    "        try:\n",
    "            n_chunks = build_and_index(args.pdf, client, embedder)\n",
    "            print(f\"Indexed {n_chunks} chunks into collection {COLLECTION_NAME}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Indexing failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return\n",
    "    else:\n",
    "        print(\"Skipping indexing as requested (--skip_index)\")\n",
    "\n",
    "    # Instantiate LLM wrapper\n",
    "    llm = LLMWrapper(groq_api_key=GROQ_API_KEY)\n",
    "\n",
    "    # Create RAG system\n",
    "    rag = FeedbackRAG(llm, client, embedder)\n",
    "\n",
    "    # Optional sentiment benchmark\n",
    "    if args.run_sentiment:\n",
    "        sa = VaderSentimentAnalyzer()\n",
    "        examples = [\n",
    "            \"I absolutely loved the story, it moved me to tears.\",\n",
    "            \"This is the worst experience I've ever had.\",\n",
    "            \"It was okay, not great but not terrible either.\",\n",
    "        ]\n",
    "        run_sentiment_benchmark(sa, examples)\n",
    "\n",
    "    # Run test question or interactive loop\n",
    "    if args.test_question:\n",
    "        result = rag.query(args.test_question)\n",
    "        latency_report.pretty_print()\n",
    "    else:\n",
    "        print(\"\\nInteractive mode. Type a question (or 'exit'):\")\n",
    "        while True:\n",
    "            try:\n",
    "                q = input('> ').strip()\n",
    "            except (EOFError, KeyboardInterrupt):\n",
    "                print('\\nExiting.')\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            if q.lower() in ('exit', 'quit'):\n",
    "                break\n",
    "            try:\n",
    "                rag.query(q)\n",
    "                latency_report.pretty_print()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error running query: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
