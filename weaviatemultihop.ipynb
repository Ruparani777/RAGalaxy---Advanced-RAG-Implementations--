{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70aebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "weaviate_multihop_rag.py\n",
    "Multi-Hop RAG (Iterative RAG) with Weaviate and comprehensive nanosecond latency instrumentation.\n",
    "\n",
    "Features:\n",
    "- Multi-hop iterative retrieval following reasoning chains\n",
    "- Query decomposition into sub-questions\n",
    "- Progressive knowledge building across hops\n",
    "- Weaviate vector database integration\n",
    "- Full pipeline timing with per-hop latency tracking\n",
    "- Detailed performance metrics and reports\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import pdfplumber\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION_NAME = \"MultiHopRAG_Documents\"\n",
    "DIM = 384  # MiniLM embedding dimension\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000\n",
    "MAX_HOPS = 3  # Maximum number of retrieval hops\n",
    "\n",
    "# Weaviate credentials\n",
    "WEAVIATE_URL = \"21ookhjbswyl5urlawqmxw.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
    "WEAVIATE_API_KEY = \"NTVWQ1dZVDI1bkptcndrZF9JRTFySVg3TEFBc1R5V0luUEtHaU9MajB6am5VQkc3aG5yVkgwWkFQVDc0PV92MjAw\"\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"âŒ ERROR: Set GROQ_API_KEY environment variable!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =========================================================\n",
    "# LATENCY UTILITIES\n",
    "# =========================================================\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    \"\"\"Return human-readable representation of nanoseconds.\"\"\"\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} Âµs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    \"\"\"Call fn(*args, **kwargs) and return (result, elapsed_ns).\"\"\"\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "def timer_ns(func):\n",
    "    \"\"\"Decorator that prints elapsed ns and stores last_elapsed_ns on wrapper.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"â±ï¸  {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "class LatencyReport:\n",
    "    \"\"\"Aggregates and reports latency metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    \n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    \n",
    "    def summary(self) -> Dict:\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // len(vals) if vals else 0,\n",
    "                \"min_ns\": min(vals) if vals else 0,\n",
    "                \"max_ns\": max(vals) if vals else 0\n",
    "            }\n",
    "        return out\n",
    "    \n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY (nanoseconds)\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items(), key=lambda p: p[0]):\n",
    "            print(f\"\\nðŸ“Š Component: {comp}\")\n",
    "            print(f\"   Count:     {stats['count']}\")\n",
    "            print(f\"   Total:     {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"   Average:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"   Min:       {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"   Max:       {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# =========================================================\n",
    "# PDF LOAD WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def load_pdf(path: str) -> str:\n",
    "    \"\"\"Load PDF with per-page timing\"\"\"\n",
    "    print(f\"ðŸ“„ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    \n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, p in enumerate(pdf.pages):\n",
    "            start_ns = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start_ns\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            text += t + \"\\n\"\n",
    "    \n",
    "    print(f\"âœ… Loaded PDF: {len(text)} characters from {len(pdf.pages)} pages\")\n",
    "    return text\n",
    "\n",
    "# =========================================================\n",
    "# CHUNK TEXT WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    \"\"\"Chunk text with timing\"\"\"\n",
    "    print(f\"âœ‚ï¸  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"âœ… Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# =========================================================\n",
    "# LOAD EMBEDDINGS WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def load_embeddings(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    \"\"\"Load embedding model with timing\"\"\"\n",
    "    print(f\"ðŸ”¢ Loading embeddings model: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    print(f\"âœ… Embeddings model loaded\")\n",
    "    return embedder\n",
    "\n",
    "# =========================================================\n",
    "# INIT WEAVIATE WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def init_weaviate(url: str, api_key: str, collection_name: str = COLLECTION_NAME) -> weaviate.WeaviateClient:\n",
    "    \"\"\"Initialize Weaviate client and collection with timing\"\"\"\n",
    "    print(f\"ðŸ—ƒï¸  Initializing Weaviate connection to {url}\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    \n",
    "    # Connect to Weaviate Cloud\n",
    "    client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=url,\n",
    "        auth_credentials=Auth.api_key(api_key)\n",
    "    )\n",
    "    \n",
    "    connect_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_connect\", connect_time)\n",
    "    print(f\"âœ… Connected to Weaviate ({format_time_ns(connect_time)})\")\n",
    "    \n",
    "    # Delete collection if exists\n",
    "    try:\n",
    "        if client.collections.exists(collection_name):\n",
    "            start = time.time_ns()\n",
    "            client.collections.delete(collection_name)\n",
    "            delete_time = time.time_ns() - start\n",
    "            latency_report.add(\"weaviate_delete_collection\", delete_time)\n",
    "            print(f\"ðŸ—‘ï¸  Deleted existing collection '{collection_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Collection check/delete: {e}\")\n",
    "    \n",
    "    # Create collection\n",
    "    start = time.time_ns()\n",
    "    try:\n",
    "        client.collections.create(\n",
    "            name=collection_name,\n",
    "            vectorizer_config=None,  # We'll provide vectors manually\n",
    "            properties=[\n",
    "                {\"name\": \"text\", \"dataType\": [\"text\"]},\n",
    "                {\"name\": \"chunk_id\", \"dataType\": [\"int\"]},\n",
    "                {\"name\": \"source\", \"dataType\": [\"text\"]}\n",
    "            ]\n",
    "        )\n",
    "        create_time = time.time_ns() - start\n",
    "        latency_report.add(\"weaviate_create_collection\", create_time)\n",
    "        print(f\"âœ… Collection '{collection_name}' created ({format_time_ns(create_time)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Collection creation: {e}\")\n",
    "    \n",
    "    return client\n",
    "\n",
    "# =========================================================\n",
    "# INSERT CHUNKS WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def insert_chunks(client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                  chunks: List[str], collection_name: str = COLLECTION_NAME) -> None:\n",
    "    \"\"\"Insert chunks into Weaviate with detailed timing\"\"\"\n",
    "    print(f\"â¬†ï¸  Inserting {len(chunks)} chunks into Weaviate...\")\n",
    "    \n",
    "    # Encode chunks (batch embedding)\n",
    "    print(f\"   ðŸ”¢ Encoding {len(chunks)} chunks...\")\n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_encode_batch\", encode_time)\n",
    "    print(f\"   âœ… Encoded in {format_time_ns(encode_time)}\")\n",
    "    \n",
    "    # Get collection\n",
    "    collection = client.collections.get(collection_name)\n",
    "    \n",
    "    # Insert objects with vectors\n",
    "    print(f\"   ðŸ’¾ Upserting to Weaviate...\")\n",
    "    start = time.time_ns()\n",
    "    \n",
    "    with collection.batch.dynamic() as batch:\n",
    "        for i, (chunk, vector) in enumerate(zip(chunks, vectors)):\n",
    "            batch.add_object(\n",
    "                properties={\n",
    "                    \"text\": chunk,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"source\": f\"chunk_{i}\"\n",
    "                },\n",
    "                vector=vector.tolist()\n",
    "            )\n",
    "    \n",
    "    upsert_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_upsert\", upsert_time)\n",
    "    print(f\"   âœ… Upserted in {format_time_ns(upsert_time)}\")\n",
    "    \n",
    "    print(f\"âœ… All chunks inserted successfully!\")\n",
    "\n",
    "# =========================================================\n",
    "# SEARCH WEAVIATE WITH TIMING\n",
    "# =========================================================\n",
    "def search_weaviate(client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                    query: str, limit: int = 4, collection_name: str = COLLECTION_NAME) -> Tuple[List[str], int]:\n",
    "    \"\"\"Search Weaviate with timing\"\"\"\n",
    "    \n",
    "    # Encode query\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"query_embedding\", encode_time)\n",
    "    \n",
    "    # Query Weaviate\n",
    "    start = time.time_ns()\n",
    "    collection = client.collections.get(collection_name)\n",
    "    \n",
    "    response = collection.query.near_vector(\n",
    "        near_vector=qvec.tolist(),\n",
    "        limit=limit,\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "    \n",
    "    search_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_search\", search_time)\n",
    "    \n",
    "    # Extract texts\n",
    "    hits = [obj.properties.get(\"text\", \"\") for obj in response.objects]\n",
    "    \n",
    "    total_time = encode_time + search_time\n",
    "    \n",
    "    return hits, total_time\n",
    "\n",
    "# =========================================================\n",
    "# MULTI-HOP RAG\n",
    "# =========================================================\n",
    "class MultiHopRAG:\n",
    "    \"\"\"Multi-Hop RAG system with iterative retrieval and comprehensive timing\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                 collection_name: str = COLLECTION_NAME, max_hops: int = MAX_HOPS):\n",
    "        self.llm = llm\n",
    "        self.client = client\n",
    "        self.embedder = embedder\n",
    "        self.collection_name = collection_name\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def _llm_invoke_timed(self, prompt: str, label: str) -> Tuple[str, int]:\n",
    "        \"\"\"Invoke LLM with timing\"\"\"\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label, elapsed)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label + \"_error\", elapsed)\n",
    "            print(f\"LLM invoke for {label} failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return str(e), elapsed\n",
    "    \n",
    "    def decompose_query(self, query: str) -> Tuple[List[str], int]:\n",
    "        \"\"\"Decompose complex query into sub-questions for multi-hop retrieval\"\"\"\n",
    "        prompt = f\"\"\"You are a query analyzer. Break down the following complex question into 2-3 simpler sub-questions that need to be answered sequentially to address the main question.\n",
    "\n",
    "Main Question: {query}\n",
    "\n",
    "Rules:\n",
    "1. Each sub-question should build on the previous one\n",
    "2. Sub-questions should be specific and focused\n",
    "3. Number each sub-question (1., 2., 3.)\n",
    "4. Keep sub-questions concise\n",
    "\n",
    "Sub-questions:\"\"\"\n",
    "        \n",
    "        print(f\"ðŸ” Decomposing query into sub-questions...\")\n",
    "        response_text, elapsed = self._llm_invoke_timed(prompt, \"llm_query_decomposition\")\n",
    "        \n",
    "        # Parse sub-questions\n",
    "        sub_questions = []\n",
    "        lines = response_text.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # Match patterns like \"1.\", \"1)\", \"Q1:\", etc.\n",
    "            if line and (line[0].isdigit() or line.startswith('Q')):\n",
    "                # Remove numbering prefix\n",
    "                clean = line.split('.', 1)[-1].split(')', 1)[-1].split(':', 1)[-1].strip()\n",
    "                if clean and len(clean) > 10:  # Valid question\n",
    "                    sub_questions.append(clean)\n",
    "        \n",
    "        # Fallback: use original query if decomposition fails\n",
    "        if not sub_questions:\n",
    "            sub_questions = [query]\n",
    "        \n",
    "        print(f\"   âœ… Generated {len(sub_questions)} sub-questions in {format_time_ns(elapsed)}\")\n",
    "        for i, sq in enumerate(sub_questions, 1):\n",
    "            print(f\"      {i}. {sq[:80]}{'...' if len(sq) > 80 else ''}\")\n",
    "        \n",
    "        return sub_questions, elapsed\n",
    "    \n",
    "    def retrieve_hop(self, query: str, hop_num: int, k: int = 4) -> Tuple[str, int]:\n",
    "        \"\"\"Retrieve documents for a specific hop\"\"\"\n",
    "        print(f\"\\n   ðŸ” HOP {hop_num}: Retrieving documents...\")\n",
    "        print(f\"      Query: {query[:100]}{'...' if len(query) > 100 else ''}\")\n",
    "        \n",
    "        hits, elapsed = search_weaviate(self.client, self.embedder, query, k, self.collection_name)\n",
    "        context = \"\\n\\n\".join(hits)\n",
    "        \n",
    "        print(f\"      âœ… Retrieved {len(hits)} documents ({len(context)} chars) in {format_time_ns(elapsed)}\")\n",
    "        latency_report.add(f\"hop_{hop_num}_retrieval\", elapsed)\n",
    "        \n",
    "        return context, elapsed\n",
    "    \n",
    "    def synthesize_hop(self, sub_question: str, context: str, accumulated_knowledge: str, \n",
    "                       hop_num: int) -> Tuple[str, int]:\n",
    "        \"\"\"Synthesize answer for current hop using context and accumulated knowledge\"\"\"\n",
    "        if accumulated_knowledge:\n",
    "            prompt = f\"\"\"You are synthesizing information across multiple retrieval steps.\n",
    "\n",
    "Previous Knowledge:\n",
    "{accumulated_knowledge}\n",
    "\n",
    "Current Sub-Question: {sub_question}\n",
    "\n",
    "New Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Task: Answer the current sub-question using BOTH the previous knowledge and the new context. Build upon what you already know.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Answer the following question based on the retrieved context:\n",
    "\n",
    "Question: {sub_question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Provide a focused answer based on the context above.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        print(f\"      ðŸ’¡ Synthesizing answer for HOP {hop_num}...\")\n",
    "        answer, elapsed = self._llm_invoke_timed(prompt, f\"llm_hop_{hop_num}_synthesis\")\n",
    "        \n",
    "        print(f\"      âœ… Synthesized in {format_time_ns(elapsed)} ({len(answer)} chars)\")\n",
    "        latency_report.add(f\"hop_{hop_num}_synthesis\", elapsed)\n",
    "        \n",
    "        return answer, elapsed\n",
    "    \n",
    "    def needs_next_hop(self, current_answer: str, remaining_questions: List[str], \n",
    "                       hop_num: int) -> Tuple[bool, str, int]:\n",
    "        \"\"\"Decide if next hop is needed\"\"\"\n",
    "        if hop_num >= self.max_hops or not remaining_questions:\n",
    "            return False, \"Max hops reached or no remaining questions\", 0\n",
    "        \n",
    "        prompt = f\"\"\"You are evaluating if we need more information retrieval.\n",
    "\n",
    "Current Answer Summary:\n",
    "{current_answer[:500]}\n",
    "\n",
    "Remaining Sub-Questions:\n",
    "{chr(10).join(f'{i+1}. {q}' for i, q in enumerate(remaining_questions[:3]))}\n",
    "\n",
    "Question: Do we have enough information to answer the remaining questions, or do we need another retrieval hop?\n",
    "\n",
    "Answer with:\n",
    "- \"CONTINUE\" if we need more retrieval\n",
    "- \"STOP\" if we have enough information\n",
    "- Provide a brief reason\n",
    "\n",
    "Decision:\"\"\"\n",
    "        \n",
    "        print(f\"\\n   ðŸ¤” Evaluating need for HOP {hop_num + 1}...\")\n",
    "        decision_text, elapsed = self._llm_invoke_timed(prompt, f\"llm_hop_{hop_num}_decision\")\n",
    "        \n",
    "        needs_more = 'CONTINUE' in decision_text.upper() and hop_num < self.max_hops\n",
    "        \n",
    "        decision_label = \"CONTINUE\" if needs_more else \"STOP\"\n",
    "        print(f\"      Decision: {decision_label}\")\n",
    "        print(f\"      Reasoning: {decision_text.strip()[:100]}...\")\n",
    "        \n",
    "        return needs_more, decision_text, elapsed\n",
    "    \n",
    "    def generate_final_answer(self, original_query: str, accumulated_knowledge: str) -> Tuple[str, int]:\n",
    "        \"\"\"Generate final comprehensive answer\"\"\"\n",
    "        prompt = f\"\"\"You are providing a final comprehensive answer based on multi-hop retrieval.\n",
    "\n",
    "Original Question: {original_query}\n",
    "\n",
    "Accumulated Knowledge from Multiple Retrieval Hops:\n",
    "{accumulated_knowledge}\n",
    "\n",
    "Task: Provide a complete, well-structured answer to the original question using all the information gathered across multiple retrieval steps. Synthesize the information coherently.\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "        \n",
    "        print(f\"\\n   ðŸŽ¯ Generating final comprehensive answer...\")\n",
    "        answer, elapsed = self._llm_invoke_timed(prompt, \"llm_final_answer\")\n",
    "        \n",
    "        print(f\"      âœ… Final answer generated in {format_time_ns(elapsed)}\")\n",
    "        \n",
    "        return answer, elapsed\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process query with Multi-Hop RAG pipeline\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ðŸš€ MULTI-HOP RAG QUERY PROCESSING\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"â“ Original Question: {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        \n",
    "        # Step 1: Query Decomposition\n",
    "        sub_questions, decomp_time = self.decompose_query(question)\n",
    "        \n",
    "        # Step 2: Multi-Hop Retrieval and Synthesis\n",
    "        accumulated_knowledge = \"\"\n",
    "        hop_results = []\n",
    "        per_hop_times = []\n",
    "        \n",
    "        for hop_num in range(1, min(len(sub_questions) + 1, self.max_hops + 1)):\n",
    "            hop_start = time.time_ns()\n",
    "            print(f\"\\n{'â”€'*70}\")\n",
    "            print(f\"ðŸ”„ HOP {hop_num}/{min(len(sub_questions), self.max_hops)}\")\n",
    "            print(f\"{'â”€'*70}\")\n",
    "            \n",
    "            current_question = sub_questions[hop_num - 1]\n",
    "            \n",
    "            # Retrieve documents for current hop\n",
    "            context, retrieval_time = self.retrieve_hop(current_question, hop_num)\n",
    "            \n",
    "            # Synthesize answer for current hop\n",
    "            hop_answer, synthesis_time = self.synthesize_hop(\n",
    "                current_question, context, accumulated_knowledge, hop_num\n",
    "            )\n",
    "            \n",
    "            # Update accumulated knowledge\n",
    "            accumulated_knowledge += f\"\\n\\n[Hop {hop_num} - {current_question}]\\n{hop_answer}\"\n",
    "            \n",
    "            hop_elapsed = time.time_ns() - hop_start\n",
    "            per_hop_times.append(hop_elapsed)\n",
    "            latency_report.add(\"multihop_hop_total\", hop_elapsed)\n",
    "            \n",
    "            hop_results.append({\n",
    "                'hop_num': hop_num,\n",
    "                'sub_question': current_question,\n",
    "                'context': context,\n",
    "                'answer': hop_answer,\n",
    "                'time_ns': hop_elapsed\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n   â±ï¸  HOP {hop_num} total time: {format_time_ns(hop_elapsed)}\")\n",
    "            \n",
    "            # Check if we need next hop\n",
    "            remaining = sub_questions[hop_num:]\n",
    "            if remaining:\n",
    "                needs_more, decision, decision_time = self.needs_next_hop(\n",
    "                    hop_answer, remaining, hop_num\n",
    "                )\n",
    "                if not needs_more:\n",
    "                    print(f\"\\n   âœ… Multi-hop retrieval complete after {hop_num} hops\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"\\n   âœ… All sub-questions processed\")\n",
    "        \n",
    "        # Step 3: Generate Final Answer\n",
    "        final_answer, final_time = self.generate_final_answer(question, accumulated_knowledge)\n",
    "        \n",
    "        total_query_ns = time.time_ns() - overall_start\n",
    "        latency_report.add(\"multihop_query_total\", total_query_ns)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ðŸ’¬ FINAL ANSWER:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(final_answer[:800])\n",
    "        if len(final_answer) > 800:\n",
    "            print(\"...\")\n",
    "        print(f\"\\nâ±ï¸  Total query time: {format_time_ns(total_query_ns)}\")\n",
    "        print(f\"â±ï¸  Number of hops: {len(hop_results)}\")\n",
    "        print(f\"â±ï¸  Average hop time: {format_time_ns(sum(per_hop_times) // len(per_hop_times))}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'sub_questions': sub_questions,\n",
    "            'hop_results': hop_results,\n",
    "            'accumulated_knowledge': accumulated_knowledge,\n",
    "            'final_answer': final_answer,\n",
    "            'num_hops': len(hop_results),\n",
    "            'per_hop_times': per_hop_times,\n",
    "            'total_query_ns': total_query_ns\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# VADER SENTIMENT BENCHMARK\n",
    "# =========================================================\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        \n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "            percentage = round((compound + 1) * 50, 2)\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "            percentage = round((1 - abs(compound)) * 50, 2)\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "            percentage = round(50 + (compound * 50), 2)\n",
    "        \n",
    "        return {\n",
    "            'label': label,\n",
    "            'percentage': percentage,\n",
    "            'compound': compound,\n",
    "            'scores': scores\n",
    "        }\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str],\n",
    "                            target_ns: int = TARGET_NS, run_number: int = 1):\n",
    "    \"\"\"Run sentiment analysis benchmark\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ”¥ SENTIMENT BENCHMARK RUN #{run_number}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"ðŸŽ¯ TARGET: < {target_ns} ns per analysis\\n\")\n",
    "    \n",
    "    individual_times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start_ns = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed_ns = time.time_ns() - start_ns\n",
    "        latency_report.add(\"vader_per_example\", elapsed_ns)\n",
    "        individual_times.append(elapsed_ns)\n",
    "        \n",
    "        status = \"âœ…\" if elapsed_ns < target_ns else \"âŒ\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed_ns):25s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    \n",
    "    total_ns = sum(individual_times)\n",
    "    avg_ns = total_ns // len(individual_times)\n",
    "    min_ns = min(individual_times)\n",
    "    max_ns = max(individual_times)\n",
    "    under_target = sum(1 for t in individual_times if t < target_ns)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š RUN #{run_number} STATISTICS:\")\n",
    "    print(f\"   Total:        {format_time_ns(total_ns)}\")\n",
    "    print(f\"   Average:      {format_time_ns(avg_ns)}\")\n",
    "    print(f\"   Min:          {format_time_ns(min_ns)}\")\n",
    "    print(f\"   Max:          {format_time_ns(max_ns)}\")\n",
    "    print(f\"   < {target_ns}ns: {under_target}/{len(individual_times)} texts\")\n",
    "    \n",
    "    if avg_ns < target_ns:\n",
    "        print(f\"   âœ… TARGET MET!\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  TARGET MISSED\")\n",
    "\n",
    "# =========================================================\n",
    "# MAIN PROGRAM\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸš€ MULTI-HOP RAG (ITERATIVE RAG) + FULL LATENCY INSTRUMENTATION\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    pipeline_start = time.time_ns()\n",
    "    \n",
    "    # Phase 1: Load and prepare data\n",
    "    print(\"ðŸ“š PHASE 1: DATA PREPARATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    text, load_time = timed_call(load_pdf, PDF_PATH)\n",
    "    latency_report.add(\"pipeline_pdf_load\", load_time)\n",
    "    \n",
    "    chunks, chunk_time = timed_call(chunk_text, text, 1000, 100)\n",
    "    latency_report.add(\"pipeline_chunking\", chunk_time)\n",
    "    \n",
    "    embedder, embed_time = timed_call(load_embeddings)\n",
    "    latency_report.add(\"pipeline_embeddings_load\", embed_time)\n",
    "    \n",
    "    weaviate_client, weaviate_time = timed_call(init_weaviate, WEAVIATE_URL, WEAVIATE_API_KEY, COLLECTION_NAME)\n",
    "    latency_report.add(\"pipeline_weaviate_init\", weaviate_time)\n",
    "    \n",
    "    insert_time_start = time.time_ns()\n",
    "    insert_chunks(weaviate_client, embedder, chunks, COLLECTION_NAME)\n",
    "    insert_time = time.time_ns() - insert_time_start\n",
    "    latency_report.add(\"pipeline_insert_chunks\", insert_time)\n",
    "    \n",
    "    # Phase 2: Initialize LLM\n",
    "    print(f\"\\nðŸ“š PHASE 2: LLM INITIALIZATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    llm_start = time.time_ns()\n",
    "    llm = ChatGroq(\n",
    "        model_name=MODEL_NAME,\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        temperature=0\n",
    "    )\n",
    "    llm_time = time.time_ns() - llm_start\n",
    "    latency_report.add(\"llm_init\", llm_time)\n",
    "    print(f\"âœ… LLM initialized in {format_time_ns(llm_time)}\")\n",
    "    \n",
    "    # Initialize Multi-Hop RAG\n",
    "    multihop_rag = MultiHopRAG(llm, weaviate_client, embedder, COLLECTION_NAME, MAX_HOPS)\n",
    "    print(f\"\\nâœ… Multi-Hop RAG system initialized (max hops: {MAX_HOPS})!\")\n",
    "    \n",
    "    # Phase 3: Run queries\n",
    "    print(f\"\\nðŸ“š PHASE 3: MULTI-HOP RAG QUERIES\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    queries = [\n",
    "        \"What are the main themes in this story and how do they relate to the characters' development?\",\n",
    "        \"Summarize the key events and explain their significance to the overall narrative.\",\n",
    "        \"Who are the main characters and what are their relationships with each other?\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for q in queries:\n",
    "        result = multihop_rag.query(q)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Phase 4: Sentiment benchmark\n",
    "    print(f\"\\nðŸ“š PHASE 4: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    sa_start = time.time_ns()\n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    sa_init = time.time_ns() - sa_start\n",
    "    latency_report.add(\"vader_init\", sa_init)\n",
    "    print(f\"âœ… VADER INIT TIME: {format_time_ns(sa_init)}\\n\")\n",
    "    \n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    \n",
    "    for run in range(1, 3):\n",
    "        run_sentiment_benchmark(sa, examples, TARGET_NS, run)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Final summary\n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency_report.add(\"pipeline_total\", pipeline_total)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ“ˆ MULTI-HOP RAG PIPELINE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total pipeline time: {format_time_ns(pipeline_total)}\")\n",
    "    print(f\"Queries executed: {len(queries)}\")\n",
    "    print(f\"Total hops across all queries: {sum(r['num_hops'] for r in results)}\")\n",
    "    print(f\"Average hops per query: {sum(r['num_hops'] for r in results) / len(results):.1f}\")\n",
    "    print(f\"Average query time: {format_time_ns(sum(r['total_query_ns'] for r in results) // len(results))}\")\n",
    "    \n",
    "    # Per-query breakdown\n",
    "    print(f\"\\nðŸ“Š PER-QUERY BREAKDOWN:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n   Query {i}: {result['question'][:60]}...\")\n",
    "        print(f\"      Hops: {result['num_hops']}\")\n",
    "        print(f\"      Time: {format_time_ns(result['total_query_ns'])}\")\n",
    "        print(f\"      Avg hop time: {format_time_ns(sum(result['per_hop_times']) // len(result['per_hop_times']))}\")\n",
    "    \n",
    "    # Detailed latency report\n",
    "    latency_report.pretty_print()\n",
    "    \n",
    "    # Cleanup\n",
    "    weaviate_client.close()\n",
    "    \n",
    "    print(\"âœ… MULTI-HOP RAG PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâš ï¸  Interrupted by user\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nâŒ Fatal error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
