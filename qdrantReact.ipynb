{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "qdrant_react_rag.py\n",
    "ReAct RAG (Reasoning + Acting) with Qdrant and comprehensive nanosecond latency instrumentation.\n",
    "\n",
    "ReAct Pattern:\n",
    "1. THOUGHT: Reason about what to do next\n",
    "2. ACTION: Execute an action (search, calculate, etc.)\n",
    "3. OBSERVATION: Observe the result\n",
    "4. Repeat until answer is found\n",
    "\n",
    "Features:\n",
    "- Full pipeline timing (PDF load, chunking, embeddings, vectorstore)\n",
    "- Per-component latency tracking\n",
    "- Detailed ReAct cycle metrics\n",
    "- Comprehensive latency reports\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import pdfplumber\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION = \"rag_collection\"\n",
    "DIM = 384\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"‚ùå ERROR: Set GROQ_API_KEY environment variable!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =========================================================\n",
    "# LATENCY UTILITIES\n",
    "# =========================================================\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    \"\"\"Return human-readable representation of nanoseconds.\"\"\"\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    \"\"\"Call fn(*args, **kwargs) and return (result, elapsed_ns).\"\"\"\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "def timer_ns(func):\n",
    "    \"\"\"Decorator that prints elapsed ns and stores last_elapsed_ns on wrapper.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"‚è±Ô∏è  {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "class LatencyReport:\n",
    "    \"\"\"Aggregates and reports latency metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    \n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    \n",
    "    def summary(self) -> Dict:\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // len(vals) if vals else 0,\n",
    "                \"min_ns\": min(vals) if vals else 0,\n",
    "                \"max_ns\": max(vals) if vals else 0\n",
    "            }\n",
    "        return out\n",
    "    \n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY (nanoseconds)\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items(), key=lambda p: p[0]):\n",
    "            print(f\"\\nüìä Component: {comp}\")\n",
    "            print(f\"   Count:     {stats['count']}\")\n",
    "            print(f\"   Total:     {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"   Average:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"   Min:       {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"   Max:       {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# =========================================================\n",
    "# LOAD PDF WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def load_pdf(path: str) -> str:\n",
    "    \"\"\"Load PDF with per-page timing\"\"\"\n",
    "    print(f\"üìÑ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    \n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        pages = pdf.pages\n",
    "        for i, p in enumerate(pages):\n",
    "            start_ns = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start_ns\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            text += t + \"\\n\"\n",
    "    \n",
    "    print(f\"‚úÖ Loaded PDF: {len(text)} characters from {len(pages)} pages\")\n",
    "    return text\n",
    "\n",
    "# =========================================================\n",
    "# CHUNK TEXT WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    \"\"\"Chunk text with timing\"\"\"\n",
    "    print(f\"‚úÇÔ∏è  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# =========================================================\n",
    "# LOAD EMBEDDINGS WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def load_embeddings(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    \"\"\"Load embedding model with timing\"\"\"\n",
    "    print(f\"üî¢ Loading embeddings model: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    print(f\"‚úÖ Embeddings model loaded\")\n",
    "    return embedder\n",
    "\n",
    "# =========================================================\n",
    "# INIT QDRANT WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def init_qdrant(collection_name: str = COLLECTION, dim: int = DIM) -> QdrantClient:\n",
    "    \"\"\"Initialize Qdrant with timing\"\"\"\n",
    "    print(f\"üóÉÔ∏è  Initializing Qdrant in-memory DB\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    qdrant = QdrantClient(\":memory:\")\n",
    "    init_time = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_client_init\", init_time)\n",
    "    \n",
    "    # Remove previous collection if exists\n",
    "    if qdrant.collection_exists(collection_name):\n",
    "        start = time.time_ns()\n",
    "        qdrant.delete_collection(collection_name)\n",
    "        delete_time = time.time_ns() - start\n",
    "        latency_report.add(\"qdrant_delete_collection\", delete_time)\n",
    "    \n",
    "    # Create collection\n",
    "    start = time.time_ns()\n",
    "    qdrant.create_collection(\n",
    "        collection_name,\n",
    "        vectors_config=VectorParams(size=dim, distance=Distance.COSINE)\n",
    "    )\n",
    "    create_time = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_create_collection\", create_time)\n",
    "    \n",
    "    print(f\"‚úÖ Qdrant collection '{collection_name}' ready\")\n",
    "    return qdrant\n",
    "\n",
    "# =========================================================\n",
    "# INSERT CHUNKS WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def insert_chunks(qdrant: QdrantClient, embedder: SentenceTransformer, \n",
    "                  chunks: List[str], collection_name: str = COLLECTION) -> None:\n",
    "    \"\"\"Insert chunks into Qdrant with detailed timing\"\"\"\n",
    "    print(f\"‚¨ÜÔ∏è  Inserting {len(chunks)} chunks into Qdrant...\")\n",
    "    \n",
    "    # Encode chunks (batch embedding)\n",
    "    print(f\"   üî¢ Encoding {len(chunks)} chunks...\")\n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_encode_batch\", encode_time)\n",
    "    print(f\"   ‚úÖ Encoded in {format_time_ns(encode_time)}\")\n",
    "    \n",
    "    # Create points\n",
    "    print(f\"   üì¶ Creating point structures...\")\n",
    "    start = time.time_ns()\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=vectors[i].tolist(),\n",
    "            payload={\"text\": chunks[i], \"chunk_id\": i}\n",
    "        )\n",
    "        for i in range(len(chunks))\n",
    "    ]\n",
    "    point_creation_time = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_point_creation\", point_creation_time)\n",
    "    print(f\"   ‚úÖ Points created in {format_time_ns(point_creation_time)}\")\n",
    "    \n",
    "    # Upsert to Qdrant\n",
    "    print(f\"   üíæ Upserting to Qdrant...\")\n",
    "    start = time.time_ns()\n",
    "    qdrant.upsert(collection_name=collection_name, points=points)\n",
    "    upsert_time = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_upsert\", upsert_time)\n",
    "    print(f\"   ‚úÖ Upserted in {format_time_ns(upsert_time)}\")\n",
    "    \n",
    "    print(f\"‚úÖ All chunks inserted successfully!\")\n",
    "\n",
    "# =========================================================\n",
    "# SEARCH QDRANT WITH TIMING\n",
    "# =========================================================\n",
    "def search_qdrant(qdrant: QdrantClient, embedder: SentenceTransformer, \n",
    "                  query: str, limit: int = 4, collection_name: str = COLLECTION) -> Tuple[List[str], int]:\n",
    "    \"\"\"Search Qdrant with timing\"\"\"\n",
    "    \n",
    "    # Encode query\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"query_embedding\", encode_time)\n",
    "    \n",
    "    # Query Qdrant\n",
    "    start = time.time_ns()\n",
    "    response = qdrant.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qvec.tolist(),\n",
    "        limit=limit\n",
    "    )\n",
    "    search_time = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_search\", search_time)\n",
    "    \n",
    "    # Extract texts\n",
    "    hits = [p.payload.get(\"text\", \"\") for p in response.points]\n",
    "    \n",
    "    total_time = encode_time + search_time\n",
    "    \n",
    "    return hits, total_time\n",
    "\n",
    "# =========================================================\n",
    "# ReAct RAG SYSTEM\n",
    "# =========================================================\n",
    "class ReActRAG:\n",
    "    \"\"\"ReAct RAG system (Reasoning + Acting) with comprehensive timing\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, qdrant: QdrantClient, embedder: SentenceTransformer, \n",
    "                 collection_name: str = COLLECTION):\n",
    "        self.llm = llm\n",
    "        self.qdrant = qdrant\n",
    "        self.embedder = embedder\n",
    "        self.collection_name = collection_name\n",
    "        self.max_steps = 5\n",
    "    \n",
    "    def _llm_invoke(self, prompt: str, label: str) -> Tuple[str, int]:\n",
    "        \"\"\"Invoke LLM with timing\"\"\"\n",
    "        start = time.time_ns()\n",
    "        resp = self.llm.invoke(prompt)\n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(f\"llm_{label}\", elapsed)\n",
    "        \n",
    "        content = resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "        return content, elapsed\n",
    "    \n",
    "    def _parse_react_response(self, response: str) -> Dict[str, str]:\n",
    "        \"\"\"Parse ReAct format: Thought, Action, etc.\"\"\"\n",
    "        result = {\n",
    "            'thought': '',\n",
    "            'action': '',\n",
    "            'action_input': '',\n",
    "            'answer': ''\n",
    "        }\n",
    "        \n",
    "        # Extract Thought\n",
    "        thought_match = re.search(r'Thought[:\\s]+(.+?)(?=Action[:\\s]|Answer[:\\s]|$)', response, re.IGNORECASE | re.DOTALL)\n",
    "        if thought_match:\n",
    "            result['thought'] = thought_match.group(1).strip()\n",
    "        \n",
    "        # Extract Action\n",
    "        action_match = re.search(r'Action[:\\s]+(\\w+)', response, re.IGNORECASE)\n",
    "        if action_match:\n",
    "            result['action'] = action_match.group(1).strip().lower()\n",
    "        \n",
    "        # Extract Action Input\n",
    "        input_match = re.search(r'Action Input[:\\s]+(.+?)(?=Thought[:\\s]|Action[:\\s]|Answer[:\\s]|$)', response, re.IGNORECASE | re.DOTALL)\n",
    "        if input_match:\n",
    "            result['action_input'] = input_match.group(1).strip()\n",
    "        \n",
    "        # Extract Answer (final answer)\n",
    "        answer_match = re.search(r'Answer[:\\s]+(.+)', response, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            result['answer'] = answer_match.group(1).strip()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def execute_action(self, action: str, action_input: str) -> Tuple[str, int]:\n",
    "        \"\"\"Execute an action and return observation\"\"\"\n",
    "        action = action.lower()\n",
    "        \n",
    "        if action == \"search\":\n",
    "            # Search the document\n",
    "            print(f\"      üîç ACTION: Searching for '{action_input[:50]}...'\")\n",
    "            hits, elapsed = search_qdrant(self.qdrant, self.embedder, action_input, 3, self.collection_name)\n",
    "            observation = \"\\n\\n\".join(hits)\n",
    "            print(f\"      ‚úÖ Found {len(hits)} relevant passages in {format_time_ns(elapsed)}\")\n",
    "            return observation, elapsed\n",
    "        \n",
    "        elif action == \"lookup\":\n",
    "            # Lookup specific information\n",
    "            print(f\"      üîé ACTION: Looking up '{action_input[:50]}...'\")\n",
    "            hits, elapsed = search_qdrant(self.qdrant, self.embedder, action_input, 2, self.collection_name)\n",
    "            observation = \" \".join(hits)\n",
    "            print(f\"      ‚úÖ Lookup completed in {format_time_ns(elapsed)}\")\n",
    "            return observation, elapsed\n",
    "        \n",
    "        elif action == \"finish\":\n",
    "            # Final answer\n",
    "            print(f\"      ‚úì ACTION: Finish with answer\")\n",
    "            return action_input, 0\n",
    "        \n",
    "        else:\n",
    "            # Unknown action\n",
    "            print(f\"      ‚ö†Ô∏è Unknown action: {action}\")\n",
    "            return f\"Unknown action: {action}. Available actions: search, lookup, finish\", 0\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process query using ReAct pattern: Thought -> Action -> Observation\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üß† ReAct RAG QUERY PROCESSING\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        \n",
    "        # Track ReAct cycles\n",
    "        react_history = []\n",
    "        step = 0\n",
    "        \n",
    "        # Build the ReAct prompt\n",
    "        system_prompt = \"\"\"You are an AI assistant using the ReAct (Reasoning + Acting) framework to answer questions.\n",
    "\n",
    "Available Actions:\n",
    "- search: Search the document for relevant information (use for broad queries)\n",
    "- lookup: Look up specific details (use for precise information)\n",
    "- finish: Provide the final answer when you have enough information\n",
    "\n",
    "You must use this format:\n",
    "\n",
    "Thought: [Your reasoning about what to do next]\n",
    "Action: [One of: search, lookup, finish]\n",
    "Action Input: [The input for the action]\n",
    "\n",
    "After each action, you'll receive an Observation. Then continue:\n",
    "\n",
    "Thought: [Reason about the observation]\n",
    "Action: [Next action]\n",
    "Action Input: [Input for next action]\n",
    "\n",
    "When you have enough information:\n",
    "Thought: [Your final reasoning]\n",
    "Action: finish\n",
    "Action Input: [Your comprehensive final answer]\n",
    "\n",
    "Begin!\"\"\"\n",
    "        \n",
    "        conversation = f\"{system_prompt}\\n\\nQuestion: {question}\\n\\n\"\n",
    "        \n",
    "        while step < self.max_steps:\n",
    "            step += 1\n",
    "            print(f\"\\n{'‚îÄ'*70}\")\n",
    "            print(f\"üîÑ ReAct Step {step}\")\n",
    "            print(f\"{'‚îÄ'*70}\")\n",
    "            \n",
    "            step_start = time.time_ns()\n",
    "            \n",
    "            # Get LLM reasoning and action\n",
    "            prompt = conversation + \"Thought:\"\n",
    "            response, llm_time = self._llm_invoke(prompt, f\"react_step_{step}\")\n",
    "            \n",
    "            # Parse the response\n",
    "            parsed = self._parse_react_response(\"Thought:\" + response)\n",
    "            \n",
    "            thought = parsed['thought']\n",
    "            action = parsed['action']\n",
    "            action_input = parsed['action_input']\n",
    "            final_answer = parsed['answer']\n",
    "            \n",
    "            print(f\"   üí≠ THOUGHT: {thought[:150]}...\")\n",
    "            \n",
    "            # Check if we have a final answer\n",
    "            if final_answer or action == 'finish':\n",
    "                answer = final_answer if final_answer else action_input\n",
    "                print(f\"   ‚úÖ FINAL ANSWER REACHED\")\n",
    "                \n",
    "                step_elapsed = time.time_ns() - step_start\n",
    "                latency_report.add(\"react_step_total\", step_elapsed)\n",
    "                \n",
    "                react_history.append({\n",
    "                    'step': step,\n",
    "                    'thought': thought,\n",
    "                    'action': 'finish',\n",
    "                    'action_input': answer,\n",
    "                    'observation': '',\n",
    "                    'elapsed_ns': step_elapsed\n",
    "                })\n",
    "                \n",
    "                break\n",
    "            \n",
    "            if action:\n",
    "                print(f\"   üé¨ ACTION: {action}\")\n",
    "                print(f\"   üì• INPUT: {action_input[:100]}...\")\n",
    "                \n",
    "                # Execute action\n",
    "                observation, action_time = self.execute_action(action, action_input)\n",
    "                \n",
    "                print(f\"   üëÅÔ∏è OBSERVATION: {observation[:150]}...\")\n",
    "                \n",
    "                # Add to conversation history\n",
    "                conversation += f\"Thought: {thought}\\n\"\n",
    "                conversation += f\"Action: {action}\\n\"\n",
    "                conversation += f\"Action Input: {action_input}\\n\"\n",
    "                conversation += f\"Observation: {observation}\\n\\n\"\n",
    "                \n",
    "                step_elapsed = time.time_ns() - step_start\n",
    "                latency_report.add(\"react_step_total\", step_elapsed)\n",
    "                \n",
    "                react_history.append({\n",
    "                    'step': step,\n",
    "                    'thought': thought,\n",
    "                    'action': action,\n",
    "                    'action_input': action_input,\n",
    "                    'observation': observation,\n",
    "                    'elapsed_ns': step_elapsed\n",
    "                })\n",
    "                \n",
    "                print(f\"   ‚è±Ô∏è Step {step} time: {format_time_ns(step_elapsed)}\")\n",
    "                \n",
    "                # Check if this was finish action\n",
    "                if action == 'finish':\n",
    "                    answer = action_input\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è No valid action found in response\")\n",
    "                # Try to extract any answer-like content\n",
    "                if final_answer:\n",
    "                    answer = final_answer\n",
    "                    break\n",
    "                # Continue to next step\n",
    "                conversation += f\"Thought: {thought}\\n\\n\"\n",
    "        \n",
    "        # If we exhausted steps without finish, use last observation or thought\n",
    "        if step >= self.max_steps and 'answer' not in locals():\n",
    "            print(f\"\\n   ‚ö†Ô∏è Max steps reached without final answer\")\n",
    "            if react_history:\n",
    "                last_obs = react_history[-1].get('observation', '')\n",
    "                answer = f\"Based on the search: {last_obs[:500]}\" if last_obs else \"Unable to determine answer\"\n",
    "            else:\n",
    "                answer = \"Unable to determine answer within step limit\"\n",
    "        \n",
    "        total_query_ns = time.time_ns() - overall_start\n",
    "        latency_report.add(\"react_query_total\", total_query_ns)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üìä ReAct SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total steps: {step}\")\n",
    "        print(f\"Total time: {format_time_ns(total_query_ns)}\")\n",
    "        print(f\"\\nüí¨ FINAL ANSWER:\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        print(answer[:800])\n",
    "        if len(answer) > 800:\n",
    "            print(\"...\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'steps': step,\n",
    "            'react_history': react_history,\n",
    "            'total_query_ns': total_query_ns\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# VADER SENTIMENT BENCHMARK\n",
    "# =========================================================\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        \n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "            percentage = round((compound + 1) * 50, 2)\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "            percentage = round((1 - abs(compound)) * 50, 2)\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "            percentage = round(50 + (compound * 50), 2)\n",
    "        \n",
    "        return {\n",
    "            'label': label,\n",
    "            'percentage': percentage,\n",
    "            'compound': compound,\n",
    "            'scores': scores\n",
    "        }\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str], \n",
    "                            target_ns: int = 200_000, run_number: int = 1):\n",
    "    \"\"\"Run sentiment analysis benchmark\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî• SENTIMENT BENCHMARK RUN #{run_number}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üéØ TARGET: < {target_ns} ns per analysis\\n\")\n",
    "    \n",
    "    individual_times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start_ns = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed_ns = time.time_ns() - start_ns\n",
    "        latency_report.add(\"vader_per_example\", elapsed_ns)\n",
    "        individual_times.append(elapsed_ns)\n",
    "        \n",
    "        status = \"‚úÖ\" if elapsed_ns < target_ns else \"‚ùå\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed_ns):25s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    \n",
    "    total_ns = sum(individual_times)\n",
    "    avg_ns = total_ns // len(individual_times)\n",
    "    min_ns = min(individual_times)\n",
    "    max_ns = max(individual_times)\n",
    "    under_target = sum(1 for t in individual_times if t < target_ns)\n",
    "    \n",
    "    print(f\"\\nüìä RUN #{run_number} STATISTICS:\")\n",
    "    print(f\"   Total:        {format_time_ns(total_ns)}\")\n",
    "    print(f\"   Average:      {format_time_ns(avg_ns)}\")\n",
    "    print(f\"   Min:          {format_time_ns(min_ns)}\")\n",
    "    print(f\"   Max:          {format_time_ns(max_ns)}\")\n",
    "    print(f\"   < {target_ns}ns: {under_target}/{len(individual_times)} texts\")\n",
    "    \n",
    "    if avg_ns < target_ns:\n",
    "        print(f\"   ‚úÖ TARGET MET!\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  TARGET MISSED\")\n",
    "\n",
    "# =========================================================\n",
    "# MAIN PROGRAM\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üß† ReAct RAG (Reasoning + Acting) + FULL LATENCY INSTRUMENTATION\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    pipeline_start = time.time_ns()\n",
    "    \n",
    "    # Phase 1: Load and prepare data\n",
    "    print(\"üìö PHASE 1: DATA PREPARATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    text, load_time = timed_call(load_pdf, PDF_PATH)\n",
    "    latency_report.add(\"pipeline_pdf_load\", load_time)\n",
    "    \n",
    "    chunks, chunk_time = timed_call(chunk_text, text, 1000, 100)\n",
    "    latency_report.add(\"pipeline_chunking\", chunk_time)\n",
    "    \n",
    "    embedder, embed_time = timed_call(load_embeddings, \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    latency_report.add(\"pipeline_embeddings_load\", embed_time)\n",
    "    \n",
    "    qdrant, qdrant_time = timed_call(init_qdrant, COLLECTION, DIM)\n",
    "    latency_report.add(\"pipeline_qdrant_init\", qdrant_time)\n",
    "    \n",
    "    insert_time_start = time.time_ns()\n",
    "    insert_chunks(qdrant, embedder, chunks, COLLECTION)\n",
    "    insert_time = time.time_ns() - insert_time_start\n",
    "    latency_report.add(\"pipeline_insert_chunks\", insert_time)\n",
    "    \n",
    "    # Phase 2: Initialize LLM\n",
    "    print(f\"\\nüìö PHASE 2: LLM INITIALIZATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    llm_start = time.time_ns()\n",
    "    llm = ChatGroq(\n",
    "        model_name=MODEL_NAME,\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        temperature=0\n",
    "    )\n",
    "    llm_time = time.time_ns() - llm_start\n",
    "    latency_report.add(\"llm_init\", llm_time)\n",
    "    print(f\"‚úÖ LLM initialized in {format_time_ns(llm_time)}\")\n",
    "    \n",
    "    # Initialize ReAct RAG\n",
    "    react_rag = ReActRAG(llm, qdrant, embedder, COLLECTION)\n",
    "    print(f\"\\n‚úÖ ReAct RAG system initialized!\")\n",
    "    \n",
    "    # Phase 3: Run ReAct queries\n",
    "    print(f\"\\nüìö PHASE 3: ReAct RAG QUERIES\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    queries = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Who are the main characters and what happens to them?\",\n",
    "        \"What is the central conflict in the document?\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for q in queries:\n",
    "        result = react_rag.query(q)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Phase 4: Sentiment benchmark\n",
    "    print(f\"\\nüìö PHASE 4: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    sa_start = time.time_ns()\n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    sa_init = time.time_ns() - sa_start\n",
    "    latency_report.add(\"vader_init\", sa_init)\n",
    "    print(f\"‚úÖ VADER INIT TIME: {format_time_ns(sa_init)}\\n\")\n",
    "    \n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    \n",
    "    for run in range(1, 2):\n",
    "        run_sentiment_benchmark(sa, examples, 200_000, run)\n",
    "    \n",
    "    # Final summary\n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency_report.add(\"pipeline_total\", pipeline_total)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìà PIPELINE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total pipeline time: {format_time_ns(pipeline_total)}\")\n",
    "    print(f\"Queries executed: {len(queries)}\")\n",
    "    print(f\"Average query time: {format_time_ns(sum(r['total_query_ns'] for r in results) // len(results))}\")\n",
    "    print(f\"Average steps per query: {sum(r['steps'] for r in results) / len(results):.1f}\")\n",
    "    \n",
    "    print(f\"\\nüß† ReAct Statistics:\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  Query {i}: {r['steps']} steps, {format_time_ns(r['total_query_ns'])}\")\n",
    "    \n",
    "    # Detailed latency report\n",
    "    latency_report.pretty_print()\n",
    "    \n",
    "    print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Interrupted by user\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Fatal error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
