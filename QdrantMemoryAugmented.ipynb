{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "qdrant_memory_augmented_rag.py\n",
    "Memory-Augmented RAG with Qdrant and comprehensive latency instrumentation.\n",
    "\n",
    "Memory Features:\n",
    "- Short-term Memory: Last 10 conversation turns\n",
    "- Long-term Memory: Extracted facts with mention counts\n",
    "- Working Memory: Current context buffer\n",
    "- Conversation continuity across queries\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict, deque\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import pdfplumber\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION = \"memory_rag_collection\"\n",
    "DIM = 384\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "SHORT_TERM_MEMORY_SIZE = 10\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"‚ùå ERROR: Set GROQ_API_KEY environment variable!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =========================================================\n",
    "# LATENCY UTILITIES\n",
    "# =========================================================\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "def timer_ns(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"‚è±Ô∏è  {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    \n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    \n",
    "    def summary(self) -> Dict:\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // len(vals) if vals else 0,\n",
    "                \"min_ns\": min(vals) if vals else 0,\n",
    "                \"max_ns\": max(vals) if vals else 0\n",
    "            }\n",
    "        return out\n",
    "    \n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY (nanoseconds)\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items(), key=lambda p: p[0]):\n",
    "            print(f\"\\nüìä Component: {comp}\")\n",
    "            print(f\"   Count:     {stats['count']}\")\n",
    "            print(f\"   Total:     {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"   Average:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"   Min:       {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"   Max:       {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# =========================================================\n",
    "# MEMORY STRUCTURES\n",
    "# =========================================================\n",
    "@dataclass\n",
    "class ConversationTurn:\n",
    "    \"\"\"Single conversation turn\"\"\"\n",
    "    timestamp: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    context_used: str\n",
    "    turn_id: int\n",
    "    \n",
    "    def to_text(self):\n",
    "        return f\"[Turn {self.turn_id}]\\nQ: {self.question}\\nA: {self.answer[:200]}...\"\n",
    "\n",
    "@dataclass\n",
    "class LongTermFact:\n",
    "    \"\"\"Persistent fact or preference\"\"\"\n",
    "    fact: str\n",
    "    category: str  # preference, entity, relationship, summary\n",
    "    confidence: float\n",
    "    mentions: int\n",
    "    first_seen: str\n",
    "    last_updated: str\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"\n",
    "    Manages multiple memory types:\n",
    "    - Short-term: Recent conversation history\n",
    "    - Long-term: Persistent facts and preferences\n",
    "    - Working: Current context buffer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_short_term=SHORT_TERM_MEMORY_SIZE):\n",
    "        self.max_short_term = max_short_term\n",
    "        self.short_term_memory: deque = deque(maxlen=max_short_term)\n",
    "        self.long_term_memory: Dict[str, LongTermFact] = {}\n",
    "        self.working_memory: Dict = {\n",
    "            'current_topic': None,\n",
    "            'entities_mentioned': set(),\n",
    "            'temp_context': []\n",
    "        }\n",
    "        self.turn_counter = 0\n",
    "        \n",
    "        print(f\"üß† Memory Manager initialized (capacity: {max_short_term} turns)\")\n",
    "    \n",
    "    def add_conversation_turn(self, question: str, answer: str, context: str = \"\"):\n",
    "        \"\"\"Add a conversation turn to short-term memory\"\"\"\n",
    "        start = time.time_ns()\n",
    "        \n",
    "        turn = ConversationTurn(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            context_used=context[:500],  # Store abbreviated context\n",
    "            turn_id=self.turn_counter\n",
    "        )\n",
    "        \n",
    "        self.short_term_memory.append(turn)\n",
    "        self.turn_counter += 1\n",
    "        \n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"memory_add_turn\", elapsed)\n",
    "        \n",
    "        print(f\"üíæ Added turn to short-term memory (Turn #{turn.turn_id})\")\n",
    "    \n",
    "    def extract_and_store_facts(self, llm, question: str, answer: str):\n",
    "        \"\"\"Extract facts from conversation and store in long-term memory\"\"\"\n",
    "        start = time.time_ns()\n",
    "        \n",
    "        prompt = f\"\"\"Extract key facts, preferences, or important information from this conversation.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Extract any:\n",
    "- User preferences\n",
    "- Important entities mentioned\n",
    "- Key facts or relationships\n",
    "- Topic summaries\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "  \"facts\": [\n",
    "    {{\"text\": \"fact text\", \"category\": \"preference|entity|relationship|summary\", \"confidence\": 0.8}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "If no important facts, return {{\"facts\": []}}\n",
    "\n",
    "Extraction:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            # Try to parse JSON\n",
    "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                data = json.loads(json_match.group())\n",
    "                facts = data.get('facts', [])\n",
    "                \n",
    "                for fact_data in facts:\n",
    "                    fact_text = fact_data['text']\n",
    "                    category = fact_data.get('category', 'summary')\n",
    "                    confidence = fact_data.get('confidence', 0.7)\n",
    "                    \n",
    "                    # Check if fact already exists\n",
    "                    if fact_text in self.long_term_memory:\n",
    "                        # Update existing fact\n",
    "                        self.long_term_memory[fact_text].mentions += 1\n",
    "                        self.long_term_memory[fact_text].last_updated = datetime.now().isoformat()\n",
    "                        self.long_term_memory[fact_text].confidence = max(\n",
    "                            self.long_term_memory[fact_text].confidence,\n",
    "                            confidence\n",
    "                        )\n",
    "                    else:\n",
    "                        # Create new fact\n",
    "                        self.long_term_memory[fact_text] = LongTermFact(\n",
    "                            fact=fact_text,\n",
    "                            category=category,\n",
    "                            confidence=confidence,\n",
    "                            mentions=1,\n",
    "                            first_seen=datetime.now().isoformat(),\n",
    "                            last_updated=datetime.now().isoformat()\n",
    "                        )\n",
    "                \n",
    "                if facts:\n",
    "                    print(f\"üß† Extracted {len(facts)} facts to long-term memory\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Fact extraction failed: {e}\")\n",
    "        \n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"memory_extract_facts\", elapsed)\n",
    "    \n",
    "    def get_relevant_short_term_context(self, current_question: str, k: int = 3) -> str:\n",
    "        \"\"\"Get relevant recent conversation turns\"\"\"\n",
    "        if not self.short_term_memory:\n",
    "            return \"\"\n",
    "        \n",
    "        # Get last k turns\n",
    "        recent_turns = list(self.short_term_memory)[-k:]\n",
    "        \n",
    "        context_parts = []\n",
    "        for turn in recent_turns:\n",
    "            context_parts.append(turn.to_text())\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def get_relevant_long_term_facts(self, query: str, k: int = 5) -> str:\n",
    "        \"\"\"Get relevant facts from long-term memory\"\"\"\n",
    "        if not self.long_term_memory:\n",
    "            return \"\"\n",
    "        \n",
    "        # Simple relevance: sort by mentions and confidence\n",
    "        sorted_facts = sorted(\n",
    "            self.long_term_memory.values(),\n",
    "            key=lambda f: (f.mentions * f.confidence),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        top_facts = sorted_facts[:k]\n",
    "        \n",
    "        if not top_facts:\n",
    "            return \"\"\n",
    "        \n",
    "        fact_texts = [\n",
    "            f\"‚Ä¢ {fact.fact} ({fact.category}, {fact.mentions}x, conf: {fact.confidence:.2f})\"\n",
    "            for fact in top_facts\n",
    "        ]\n",
    "        \n",
    "        return \"Long-term Memory Facts:\\n\" + \"\\n\".join(fact_texts)\n",
    "    \n",
    "    def get_memory_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of current memory state\"\"\"\n",
    "        return {\n",
    "            'short_term_size': len(self.short_term_memory),\n",
    "            'long_term_facts': len(self.long_term_memory),\n",
    "            'total_turns': self.turn_counter,\n",
    "            'current_topic': self.working_memory['current_topic'],\n",
    "            'entities_tracked': len(self.working_memory['entities_mentioned'])\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# PDF/CHUNKING/EMBEDDINGS/QDRANT\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def load_pdf(path: str) -> str:\n",
    "    print(f\"üìÑ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for p in pdf.pages:\n",
    "            start_ns = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start_ns\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            text += t + \"\\n\"\n",
    "    print(f\"‚úÖ Loaded PDF: {len(text)} characters\")\n",
    "    return text\n",
    "\n",
    "@timer_ns\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    print(f\"‚úÇÔ∏è  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "@timer_ns\n",
    "def load_embeddings(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    print(f\"üî¢ Loading embeddings: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    print(f\"‚úÖ Embeddings loaded\")\n",
    "    return embedder\n",
    "\n",
    "@timer_ns\n",
    "def init_qdrant(collection_name: str = COLLECTION, dim: int = DIM) -> QdrantClient:\n",
    "    print(f\"üóÉÔ∏è  Initializing Qdrant in-memory DB\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    qdrant = QdrantClient(\":memory:\")\n",
    "    init_time = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_client_init\", init_time)\n",
    "    \n",
    "    if qdrant.collection_exists(collection_name):\n",
    "        qdrant.delete_collection(collection_name)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    qdrant.create_collection(\n",
    "        collection_name,\n",
    "        vectors_config=VectorParams(size=dim, distance=Distance.COSINE)\n",
    "    )\n",
    "    create_time = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_create_collection\", create_time)\n",
    "    \n",
    "    print(f\"‚úÖ Qdrant collection '{collection_name}' ready\")\n",
    "    return qdrant\n",
    "\n",
    "@timer_ns\n",
    "def insert_chunks(qdrant: QdrantClient, embedder: SentenceTransformer, \n",
    "                  chunks: List[str], collection_name: str = COLLECTION) -> None:\n",
    "    print(f\"‚¨ÜÔ∏è  Inserting {len(chunks)} chunks...\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_encode_batch\", encode_time)\n",
    "    print(f\"   ‚úÖ Encoded in {format_time_ns(encode_time)}\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=vectors[i].tolist(),\n",
    "            payload={\"text\": chunks[i], \"chunk_id\": i}\n",
    "        )\n",
    "        for i in range(len(chunks))\n",
    "    ]\n",
    "    point_time = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_point_creation\", point_time)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    qdrant.upsert(collection_name=collection_name, points=points)\n",
    "    upsert_time = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_upsert\", upsert_time)\n",
    "    \n",
    "    print(f\"‚úÖ Chunks inserted!\")\n",
    "\n",
    "def search_qdrant(qdrant: QdrantClient, embedder: SentenceTransformer, \n",
    "                  query: str, limit: int = 4, collection_name: str = COLLECTION) -> Tuple[List[str], int]:\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"query_embedding\", encode_time)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    response = qdrant.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qvec.tolist(),\n",
    "        limit=limit\n",
    "    )\n",
    "    search_time = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_search\", search_time)\n",
    "    \n",
    "    hits = [p.payload.get(\"text\", \"\") for p in response.points]\n",
    "    total_time = encode_time + search_time\n",
    "    \n",
    "    return hits, total_time\n",
    "\n",
    "# =========================================================\n",
    "# VADER SENTIMENT\n",
    "# =========================================================\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        \n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "        \n",
    "        return {'label': label, 'compound': compound, 'scores': scores}\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str], \n",
    "                            target_ns: int = 200_000, run_number: int = 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî• SENTIMENT BENCHMARK RUN #{run_number}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üéØ TARGET: < {target_ns} ns per analysis\\n\")\n",
    "    \n",
    "    times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"vader_per_example\", elapsed)\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        status = \"‚úÖ\" if elapsed < target_ns else \"‚ùå\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed):25s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    \n",
    "    avg = sum(times) // len(times)\n",
    "    print(f\"\\nüìä RUN #{run_number} STATISTICS:\")\n",
    "    print(f\"   Average:      {format_time_ns(avg)}\")\n",
    "    print(f\"   Min:          {format_time_ns(min(times))}\")\n",
    "    print(f\"   Max:          {format_time_ns(max(times))}\")\n",
    "    print(f\"   {'‚úÖ TARGET MET!' if avg < target_ns else '‚ö†Ô∏è  TARGET MISSED'}\")\n",
    "\n",
    "# =========================================================\n",
    "# MEMORY-AUGMENTED RAG\n",
    "# =========================================================\n",
    "class MemoryAugmentedRAG:\n",
    "    \"\"\"RAG with integrated memory system\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, qdrant: QdrantClient, embedder: SentenceTransformer, \n",
    "                 memory: MemoryManager, collection_name: str = COLLECTION):\n",
    "        self.llm = llm\n",
    "        self.qdrant = qdrant\n",
    "        self.embedder = embedder\n",
    "        self.memory = memory\n",
    "        self.collection_name = collection_name\n",
    "    \n",
    "    def query(self, question: str, use_memory: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Query with memory augmentation\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üß† MEMORY-AUGMENTED RAG QUERY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        \n",
    "        # Step 1: Get memory context\n",
    "        memory_context = \"\"\n",
    "        if use_memory:\n",
    "            print(\"üìñ Retrieving from memory...\")\n",
    "            \n",
    "            short_term = self.memory.get_relevant_short_term_context(question, k=3)\n",
    "            long_term = self.memory.get_relevant_long_term_facts(question, k=5)\n",
    "            \n",
    "            if short_term or long_term:\n",
    "                memory_context = f\"CONVERSATION MEMORY:\\n\"\n",
    "                if short_term:\n",
    "                    memory_context += f\"\\nRecent Conversation:\\n{short_term}\\n\"\n",
    "                    print(f\"   ‚úì Short-term: {len(short_term)} chars\")\n",
    "                if long_term:\n",
    "                    memory_context += f\"\\n{long_term}\\n\"\n",
    "                    print(f\"   ‚úì Long-term: {len(self.memory.long_term_memory)} facts\")\n",
    "        \n",
    "        # Step 2: Retrieve from Qdrant\n",
    "        print(\"üìö Retrieving from documents...\")\n",
    "        start = time.time_ns()\n",
    "        hits, ret_time = search_qdrant(self.qdrant, self.embedder, question, 4, self.collection_name)\n",
    "        doc_context = \"\\n\\n\".join(hits)\n",
    "        print(f\"   ‚úì Retrieved: {len(hits)} docs ({format_time_ns(ret_time)})\")\n",
    "        \n",
    "        # Step 3: Generate answer\n",
    "        print(\"üí≠ Generating answer...\")\n",
    "        \n",
    "        full_context = \"\"\n",
    "        if memory_context:\n",
    "            full_context += memory_context + \"\\n\\n\"\n",
    "        if doc_context:\n",
    "            full_context += f\"DOCUMENT CONTEXT:\\n{doc_context}\"\n",
    "        \n",
    "        if not full_context:\n",
    "            full_context = \"No relevant context available.\"\n",
    "        \n",
    "        prompt = f\"\"\"Use conversation history and documents to answer.\n",
    "\n",
    "{full_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Based on the context (including conversation history and facts), provide a comprehensive answer.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        start = time.time_ns()\n",
    "        response = self.llm.invoke(prompt)\n",
    "        gen_time = time.time_ns() - start\n",
    "        latency_report.add(\"llm_generate_answer\", gen_time)\n",
    "        \n",
    "        answer = response.content if hasattr(response, 'content') else str(response)\n",
    "        \n",
    "        print(f\"\\nüí¨ ANSWER ({format_time_ns(gen_time)}):\")\n",
    "        print(answer[:500])\n",
    "        if len(answer) > 500:\n",
    "            print(\"...\")\n",
    "        \n",
    "        # Step 4: Extract and store facts\n",
    "        if use_memory:\n",
    "            print(\"\\nüß† Extracting facts for long-term memory...\")\n",
    "            self.memory.extract_and_store_facts(self.llm, question, answer)\n",
    "        \n",
    "        # Step 5: Add to short-term memory\n",
    "        if use_memory:\n",
    "            self.memory.add_conversation_turn(question, answer, doc_context[:500])\n",
    "        \n",
    "        # Step 6: Get memory summary\n",
    "        mem_summary = self.memory.get_memory_summary()\n",
    "        \n",
    "        total = time.time_ns() - overall_start\n",
    "        latency_report.add(\"memory_rag_query_total\", total)\n",
    "        \n",
    "        print(f\"\\nüìä Memory State:\")\n",
    "        print(f\"   Short-term: {mem_summary['short_term_size']} turns\")\n",
    "        print(f\"   Long-term: {mem_summary['long_term_facts']} facts\")\n",
    "        print(f\"   Total turns: {mem_summary['total_turns']}\")\n",
    "        print(f\"\\n‚è±Ô∏è  Total query time: {format_time_ns(total)}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'memory_used': use_memory,\n",
    "            'memory_summary': mem_summary,\n",
    "            'total_time': total\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üß† MEMORY-AUGMENTED RAG + QDRANT\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    pipeline_start = time.time_ns()\n",
    "    \n",
    "    # Phase 1: Data preparation\n",
    "    print(\"üìö PHASE 1: DATA PREPARATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    text, load_time = timed_call(load_pdf, PDF_PATH)\n",
    "    latency_report.add(\"pipeline_pdf_load\", load_time)\n",
    "    \n",
    "    chunks, chunk_time = timed_call(chunk_text, text, 1000, 100)\n",
    "    latency_report.add(\"pipeline_chunking\", chunk_time)\n",
    "    \n",
    "    embedder, embed_time = timed_call(load_embeddings)\n",
    "    latency_report.add(\"pipeline_embeddings_load\", embed_time)\n",
    "    \n",
    "    qdrant, qdrant_time = timed_call(init_qdrant, COLLECTION, DIM)\n",
    "    latency_report.add(\"pipeline_qdrant_init\", qdrant_time)\n",
    "    \n",
    "    insert_start = time.time_ns()\n",
    "    insert_chunks(qdrant, embedder, chunks, COLLECTION)\n",
    "    insert_time = time.time_ns() - insert_start\n",
    "    latency_report.add(\"pipeline_insert_chunks\", insert_time)\n",
    "    \n",
    "    # Phase 2: Initialize LLM\n",
    "    print(f\"\\nüìö PHASE 2: LLM INITIALIZATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    llm_start = time.time_ns()\n",
    "    llm = ChatGroq(model_name=MODEL_NAME, groq_api_key=GROQ_API_KEY, temperature=0)\n",
    "    llm_time = time.time_ns() - llm_start\n",
    "    latency_report.add(\"llm_init\", llm_time)\n",
    "    print(f\"‚úÖ LLM initialized in {format_time_ns(llm_time)}\")\n",
    "    \n",
    "    # Initialize Memory Manager\n",
    "    memory = MemoryManager()\n",
    "    \n",
    "    # Initialize Memory-Augmented RAG\n",
    "    mem_rag = MemoryAugmentedRAG(llm, qdrant, embedder, memory, COLLECTION)\n",
    "    print(f\"\\n‚úÖ Memory-Augmented RAG system initialized!\")\n",
    "    \n",
    "    # Phase 3: Multi-turn conversation\n",
    "    print(f\"\\nüìö PHASE 3: MEMORY-AUGMENTED RAG QUERIES\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    conversation = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Tell me more about the love theme you mentioned.\",\n",
    "        \"What other themes did you discuss earlier?\",\n",
    "        \"Summarize everything we talked about.\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for i, q in enumerate(conversation, 1):\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(f\"CONVERSATION TURN {i}/{len(conversation)}\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        result = mem_rag.query(q, use_memory=True)\n",
    "        results.append(result)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Phase 4: Sentiment benchmark\n",
    "    print(f\"\\nüìö PHASE 4: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    sa_start = time.time_ns()\n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    sa_init = time.time_ns() - sa_start\n",
    "    latency_report.add(\"vader_init\", sa_init)\n",
    "    print(f\"‚úÖ VADER INIT: {format_time_ns(sa_init)}\\n\")\n",
    "    \n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    \n",
    "    run_sentiment_benchmark(sa, examples, 200_000, 1)\n",
    "    \n",
    "    # Final summary\n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency_report.add(\"pipeline_total\", pipeline_total)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìà PIPELINE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    final_mem = memory.get_memory_summary()\n",
    "    print(f\"Total pipeline time: {format_time_ns(pipeline_total)}\")\n",
    "    print(f\"Queries executed: {len(conversation)}\")\n",
    "    print(f\"Average query time: {format_time_ns(sum(r['total_time'] for r in results) // len(results))}\")\n",
    "    print(f\"\\nüß† FINAL MEMORY STATE:\")\n",
    "    print(f\"   Short-term: {final_mem['short_term_size']}/{SHORT_TERM_MEMORY_SIZE} turns\")\n",
    "    print(f\"   Long-term: {final_mem['long_term_facts']} facts\")\n",
    "    print(f\"   Total turns: {final_mem['total_turns']}\")\n",
    "    \n",
    "    if memory.long_term_memory:\n",
    "        print(f\"\\nüß† Learned Facts:\")\n",
    "        for i, (text, fact) in enumerate(list(memory.long_term_memory.items())[:5], 1):\n",
    "            print(f\"   {i}. {text[:70]}... ({fact.mentions}x)\")\n",
    "    \n",
    "    latency_report.pretty_print()\n",
    "    print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Interrupted by user\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Fatal error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
