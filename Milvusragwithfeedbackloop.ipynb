{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdddd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    utility,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION_NAME = \"FeedbackRAG_Documents\"\n",
    "DIM = 384\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000\n",
    "\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"âŒ ERROR: Set GROQ_API_KEY environment variable!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------------------\n",
    "# LATENCY UTILITIES\n",
    "# ---------------------------\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} Âµs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "def timer_ns(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"â±ï¸  {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    def summary(self) -> Dict:\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // len(vals) if vals else 0,\n",
    "                \"min_ns\": min(vals) if vals else 0,\n",
    "                \"max_ns\": max(vals) if vals else 0\n",
    "            }\n",
    "        return out\n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY (nanoseconds)\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items(), key=lambda p: p[0]):\n",
    "            print(f\"\\nğŸ“Š Component: {comp}\")\n",
    "            print(f\"   Count:     {stats['count']}\")\n",
    "            print(f\"   Total:     {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"   Average:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"   Min:       {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"   Max:       {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# ---------------------------\n",
    "# PDF Loader\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def load_pdf(path: str) -> str:\n",
    "    print(f\"ğŸ“„ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, p in enumerate(pdf.pages):\n",
    "            start_ns = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start_ns\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            text += t + \"\\n\"\n",
    "    print(f\"âœ… Loaded PDF: {len(text)} characters from {len(pdf.pages)} pages\")\n",
    "    return text\n",
    "\n",
    "# ---------------------------\n",
    "# Chunker\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    print(\"âœ‚ï¸  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"âœ… Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# ---------------------------\n",
    "# Embeddings loader\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def load_embeddings(model_name: str = EMBED_MODEL) -> SentenceTransformer:\n",
    "    print(f\"ğŸ”¢ Loading embeddings model: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    print(\"âœ… Embeddings model loaded\")\n",
    "    return embedder\n",
    "\n",
    "# ---------------------------\n",
    "# Milvus init (AUTO-ID primary key)\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def init_milvus(host: str, port: str, collection_name: str = COLLECTION_NAME, dim: int = DIM) -> Collection:\n",
    "    print(f\"ğŸ—ƒï¸  Initializing Milvus connection to {host}:{port}\")\n",
    "    connections.connect(host=host, port=port)\n",
    "\n",
    "    # drop if exists\n",
    "    try:\n",
    "        if utility.has_collection(collection_name):\n",
    "            utility.drop_collection(collection_name)\n",
    "            print(f\"ğŸ—‘ï¸  Deleted existing collection '{collection_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Collection check/delete: {e}\")\n",
    "\n",
    "    # primary key auto-id field first\n",
    "    chunk_id_field = FieldSchema(name=\"chunk_id\", dtype=DataType.INT64, is_primary=True, auto_id=True)\n",
    "    text_field = FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535)\n",
    "    source_field = FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=1024)\n",
    "    embedding_field = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "\n",
    "    schema = CollectionSchema(fields=[chunk_id_field, text_field, source_field, embedding_field],\n",
    "                              description=\"Feedback-Loop RAG document chunks\")\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    # create index and load\n",
    "    index_params = {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 1024}}\n",
    "    try:\n",
    "        collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  create_index: {e}\")\n",
    "    try:\n",
    "        collection.load()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  load: {e}\")\n",
    "\n",
    "    print(f\"âœ… Milvus collection '{collection_name}' ready\")\n",
    "    return collection\n",
    "\n",
    "# ---------------------------\n",
    "# Insert chunks (no manual IDs)\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def insert_chunks(collection: Collection, embedder: SentenceTransformer, chunks: List[str]) -> None:\n",
    "    print(f\"â¬†ï¸  Inserting {len(chunks)} chunks into Milvus...\")\n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_encode_batch\", encode_time)\n",
    "    print(f\"   âœ… Encoded in {format_time_ns(encode_time)}\")\n",
    "\n",
    "    texts = chunks\n",
    "    sources = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "    embeddings = [v.tolist() if hasattr(v, \"tolist\") else list(v) for v in vectors]\n",
    "\n",
    "    start = time.time_ns()\n",
    "    collection.insert([texts, sources, embeddings])\n",
    "    insert_time = time.time_ns() - start\n",
    "    latency_report.add(\"milvus_insert\", insert_time)\n",
    "    print(f\"   âœ… Inserted {len(chunks)} vectors in {format_time_ns(insert_time)}\")\n",
    "\n",
    "    start = time.time_ns()\n",
    "    collection.flush()\n",
    "    flush_time = time.time_ns() - start\n",
    "    latency_report.add(\"milvus_flush\", flush_time)\n",
    "    print(f\"   âœ… Flushed collection ({format_time_ns(flush_time)})\")\n",
    "\n",
    "# ---------------------------\n",
    "# Search\n",
    "# ---------------------------\n",
    "def search_milvus(collection: Collection, embedder: SentenceTransformer, query: str, limit: int = 4) -> Tuple[List[str], int]:\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"query_embedding\", encode_time)\n",
    "\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    start = time.time_ns()\n",
    "    try:\n",
    "        results = collection.search(data=[qvec.tolist()], anns_field=\"embedding\", param=search_params, limit=limit,\n",
    "                                    output_fields=[\"text\", \"source\", \"chunk_id\"])\n",
    "        search_time = time.time_ns() - start\n",
    "        latency_report.add(\"milvus_search\", search_time)\n",
    "\n",
    "        hits = []\n",
    "        for hit in results[0]:\n",
    "            try:\n",
    "                ent = getattr(hit, \"entity\", None) or getattr(hit, \"_fields\", None) or {}\n",
    "                if isinstance(ent, dict):\n",
    "                    txt = ent.get(\"text\", \"\")\n",
    "                else:\n",
    "                    txt = str(hit)\n",
    "                hits.append(txt)\n",
    "            except Exception:\n",
    "                hits.append(\"\")\n",
    "    except Exception as e:\n",
    "        search_time = time.time_ns() - start\n",
    "        latency_report.add(\"milvus_search_error\", search_time)\n",
    "        print(f\"âš ï¸ Milvus search failed: {e}\")\n",
    "        hits = []\n",
    "    total_time = encode_time + (latency_report.store.get(\"milvus_search\", [-1])[-1] if latency_report.store.get(\"milvus_search\") else 0)\n",
    "    return hits, total_time\n",
    "\n",
    "# ---------------------------\n",
    "# RAG with Feedback Loop\n",
    "# ---------------------------\n",
    "class FeedbackLoopRAG:\n",
    "    \"\"\"\n",
    "    RAG with Feedback Loop System\n",
    "    \n",
    "    Workflow:\n",
    "    1. RETRIEVE: Get relevant documents\n",
    "    2. GENERATE: Create initial answer\n",
    "    3. EVALUATE: Assess answer quality with multiple criteria\n",
    "    4. FEEDBACK: Generate specific improvement suggestions\n",
    "    5. REFINE: Use feedback to improve retrieval/generation\n",
    "    6. REPEAT: Continue loop until quality threshold met\n",
    "    \n",
    "    The feedback loop enables continuous improvement through:\n",
    "    - Relevance scoring\n",
    "    - Completeness checking\n",
    "    - Accuracy assessment\n",
    "    - Context sufficiency evaluation\n",
    "    - Query refinement suggestions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, collection: Collection, embedder: SentenceTransformer):\n",
    "        self.llm = llm\n",
    "        self.collection = collection\n",
    "        self.embedder = embedder\n",
    "        self.feedback_history = []\n",
    "\n",
    "    def _llm_invoke_timed(self, prompt: str, label: str) -> Tuple[str, int]:\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label, elapsed)\n",
    "            content = response.content if hasattr(response, \"content\") else str(response)\n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label + \"_error\", elapsed)\n",
    "            print(f\"LLM invoke for {label} failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return str(e), elapsed\n",
    "\n",
    "    def retrieve_documents(self, query: str, k: int = 4, iteration: int = 1) -> Tuple[List[str], str, int]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents with optional query refinement based on feedback\n",
    "        \"\"\"\n",
    "        print(f\"   ğŸ” Retrieving documents (k={k})...\")\n",
    "        \n",
    "        hits, elapsed = search_milvus(self.collection, self.embedder, query, k)\n",
    "        context = \"\\n\\n---\\n\\n\".join([f\"[Document {i+1}]\\n{hit}\" for i, hit in enumerate(hits)])\n",
    "        \n",
    "        print(f\"   âœ… Retrieved {len(hits)} documents ({format_time_ns(elapsed)})\")\n",
    "        print(f\"   ğŸ“ Total context length: {len(context)} characters\")\n",
    "        \n",
    "        return hits, context, elapsed\n",
    "\n",
    "    def generate_answer(self, query: str, context: str, feedback: Optional[str] = None, iteration: int = 1) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Generate answer from context, incorporating feedback from previous iteration\n",
    "        \"\"\"\n",
    "        print(f\"   âœï¸  Generating answer...\")\n",
    "        \n",
    "        if feedback and iteration > 1:\n",
    "            prompt = f\"\"\"You are improving your previous answer based on feedback.\n",
    "\n",
    "Original Question: {query}\n",
    "\n",
    "Context from Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Previous Iteration Feedback:\n",
    "{feedback}\n",
    "\n",
    "Instructions:\n",
    "1. Address the feedback points specifically\n",
    "2. Use the retrieved context effectively\n",
    "3. Ensure completeness and accuracy\n",
    "4. Provide a well-structured, comprehensive answer\n",
    "\n",
    "Improved Answer:\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Answer the following question based on the retrieved context.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Answer based on the provided context\n",
    "2. Be specific and detailed\n",
    "3. If the context is insufficient, acknowledge it\n",
    "4. Structure your response clearly\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        answer, elapsed = self._llm_invoke_timed(prompt, \"llm_generate_answer\")\n",
    "        print(f\"   âœ… Answer generated ({format_time_ns(elapsed)})\")\n",
    "        print(f\"   ğŸ“ Answer length: {len(answer)} characters\")\n",
    "        \n",
    "        return answer, elapsed\n",
    "\n",
    "    def evaluate_answer(self, query: str, answer: str, context: str, iteration: int) -> Tuple[Dict[str, Any], int]:\n",
    "        \"\"\"\n",
    "        Evaluate answer quality across multiple dimensions\n",
    "        Returns scores and detailed assessment\n",
    "        \"\"\"\n",
    "        print(f\"   ğŸ“Š Evaluating answer quality...\")\n",
    "        \n",
    "        eval_prompt = f\"\"\"You are an expert evaluator. Assess this answer across multiple quality dimensions.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Generated Answer:\n",
    "{answer}\n",
    "\n",
    "Available Context:\n",
    "{context}\n",
    "\n",
    "Evaluate the answer on the following criteria (score 1-10 for each):\n",
    "\n",
    "1. RELEVANCE: Does the answer directly address the question?\n",
    "2. COMPLETENESS: Are all aspects of the question covered?\n",
    "3. ACCURACY: Is the information correct based on context?\n",
    "4. CONTEXT_USAGE: How well does it utilize the retrieved context?\n",
    "5. CLARITY: Is the answer clear and well-structured?\n",
    "\n",
    "Provide your evaluation in this EXACT format:\n",
    "RELEVANCE: [score]/10 - [brief reason]\n",
    "COMPLETENESS: [score]/10 - [brief reason]\n",
    "ACCURACY: [score]/10 - [brief reason]\n",
    "CONTEXT_USAGE: [score]/10 - [brief reason]\n",
    "CLARITY: [score]/10 - [brief reason]\n",
    "\n",
    "OVERALL_SCORE: [average]/10\n",
    "PASS: [YES/NO] (YES if overall >= 7.5, NO otherwise)\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "        \n",
    "        eval_text, elapsed = self._llm_invoke_timed(eval_prompt, \"llm_evaluate\")\n",
    "        \n",
    "        # Parse evaluation scores\n",
    "        scores = self._parse_evaluation(eval_text)\n",
    "        \n",
    "        print(f\"   âœ… Evaluation complete ({format_time_ns(elapsed)})\")\n",
    "        print(f\"   ğŸ“ˆ Overall Score: {scores['overall_score']:.1f}/10\")\n",
    "        print(f\"   {'âœ… PASS' if scores['pass_threshold'] else 'âŒ NEEDS IMPROVEMENT'}\")\n",
    "        \n",
    "        return scores, elapsed\n",
    "\n",
    "    def _parse_evaluation(self, eval_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse evaluation text into structured scores\"\"\"\n",
    "        scores = {\n",
    "            'relevance': 5.0,\n",
    "            'completeness': 5.0,\n",
    "            'accuracy': 5.0,\n",
    "            'context_usage': 5.0,\n",
    "            'clarity': 5.0,\n",
    "            'overall_score': 5.0,\n",
    "            'pass_threshold': False,\n",
    "            'raw_evaluation': eval_text\n",
    "        }\n",
    "        \n",
    "        lines = eval_text.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if ':' in line:\n",
    "                key_part = line.split(':')[0].upper().strip()\n",
    "                value_part = line.split(':')[1].strip()\n",
    "                \n",
    "                # Extract numeric score\n",
    "                score_str = value_part.split('/')[0].strip()\n",
    "                try:\n",
    "                    score = float(score_str)\n",
    "                    if 'RELEVANCE' in key_part:\n",
    "                        scores['relevance'] = score\n",
    "                    elif 'COMPLETENESS' in key_part:\n",
    "                        scores['completeness'] = score\n",
    "                    elif 'ACCURACY' in key_part:\n",
    "                        scores['accuracy'] = score\n",
    "                    elif 'CONTEXT' in key_part or 'USAGE' in key_part:\n",
    "                        scores['context_usage'] = score\n",
    "                    elif 'CLARITY' in key_part:\n",
    "                        scores['clarity'] = score\n",
    "                    elif 'OVERALL' in key_part:\n",
    "                        scores['overall_score'] = score\n",
    "                except (ValueError, IndexError):\n",
    "                    pass\n",
    "            \n",
    "            if 'PASS:' in line.upper():\n",
    "                scores['pass_threshold'] = 'YES' in line.upper()\n",
    "        \n",
    "        # Calculate overall if not explicitly provided\n",
    "        if scores['overall_score'] == 5.0:\n",
    "            scores['overall_score'] = (\n",
    "                scores['relevance'] + \n",
    "                scores['completeness'] + \n",
    "                scores['accuracy'] + \n",
    "                scores['context_usage'] + \n",
    "                scores['clarity']\n",
    "            ) / 5.0\n",
    "        \n",
    "        # Set pass threshold if not explicitly provided\n",
    "        if not scores['pass_threshold']:\n",
    "            scores['pass_threshold'] = scores['overall_score'] >= 7.5\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def generate_feedback(self, query: str, answer: str, context: str, evaluation: Dict[str, Any], iteration: int) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Generate specific, actionable feedback for improvement\n",
    "        \"\"\"\n",
    "        print(f\"   ğŸ’¬ Generating feedback...\")\n",
    "        \n",
    "        scores_text = f\"\"\"\n",
    "RELEVANCE: {evaluation['relevance']}/10\n",
    "COMPLETENESS: {evaluation['completeness']}/10\n",
    "ACCURACY: {evaluation['accuracy']}/10\n",
    "CONTEXT_USAGE: {evaluation['context_usage']}/10\n",
    "CLARITY: {evaluation['clarity']}/10\n",
    "OVERALL: {evaluation['overall_score']:.1f}/10\n",
    "\"\"\"\n",
    "        \n",
    "        feedback_prompt = f\"\"\"You are providing constructive feedback to improve an answer.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Current Answer:\n",
    "{answer}\n",
    "\n",
    "Quality Scores:\n",
    "{scores_text}\n",
    "\n",
    "Task: Generate specific, actionable feedback to improve the answer. Focus on:\n",
    "\n",
    "1. RETRIEVAL IMPROVEMENTS:\n",
    "   - Should we search for different/additional information?\n",
    "   - Are there specific topics or keywords to focus on?\n",
    "   - Do we need more context or different sources?\n",
    "\n",
    "2. ANSWER IMPROVEMENTS:\n",
    "   - What specific aspects are missing or incomplete?\n",
    "   - Which parts need more detail or clarification?\n",
    "   - How can we better structure the response?\n",
    "\n",
    "3. SPECIFIC ACTIONS:\n",
    "   - List 2-3 concrete steps to improve the answer\n",
    "   - Suggest refined search queries if needed\n",
    "   - Identify gaps to address in next iteration\n",
    "\n",
    "Provide clear, actionable feedback:\"\"\"\n",
    "        \n",
    "        feedback, elapsed = self._llm_invoke_timed(feedback_prompt, \"llm_generate_feedback\")\n",
    "        \n",
    "        print(f\"   âœ… Feedback generated ({format_time_ns(elapsed)})\")\n",
    "        \n",
    "        return feedback, elapsed\n",
    "\n",
    "    def refine_query(self, original_query: str, feedback: str, iteration: int) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Refine the search query based on feedback to get better retrieval results\n",
    "        \"\"\"\n",
    "        print(f\"   ğŸ”„ Refining search query...\")\n",
    "        \n",
    "        refine_prompt = f\"\"\"Based on feedback, create a refined search query to retrieve better information.\n",
    "\n",
    "Original Query: {original_query}\n",
    "\n",
    "Feedback:\n",
    "{feedback}\n",
    "\n",
    "Instructions:\n",
    "1. Identify key missing information or gaps\n",
    "2. Create a more specific search query\n",
    "3. Focus on the most important aspects to retrieve\n",
    "4. Keep the query concise but informative\n",
    "\n",
    "If the original query is already optimal, return it unchanged.\n",
    "\n",
    "Refined Query:\"\"\"\n",
    "        \n",
    "        refined_query, elapsed = self._llm_invoke_timed(refine_prompt, \"llm_refine_query\")\n",
    "        \n",
    "        # Clean up the refined query\n",
    "        refined_query = refined_query.strip().strip('\"').strip(\"'\")\n",
    "        \n",
    "        if refined_query and refined_query != original_query:\n",
    "            print(f\"   âœ… Query refined ({format_time_ns(elapsed)})\")\n",
    "            print(f\"   ğŸ” New query: \\\"{refined_query[:80]}...\\\"\")\n",
    "        else:\n",
    "            print(f\"   â„¹ï¸  Query unchanged ({format_time_ns(elapsed)})\")\n",
    "            refined_query = original_query\n",
    "        \n",
    "        return refined_query, elapsed\n",
    "\n",
    "    def query(self, question: str, max_iterations: int = 3, quality_threshold: float = 7.5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main feedback loop query execution\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ”„ RAG WITH FEEDBACK LOOP\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"â“ Question: {question}\")\n",
    "        print(f\"ğŸ¯ Quality Threshold: {quality_threshold}/10\")\n",
    "        print(f\"ğŸ” Max Iterations: {max_iterations}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        iteration = 0\n",
    "        current_query = question\n",
    "        previous_feedback = None\n",
    "        \n",
    "        best_answer = \"\"\n",
    "        best_score = 0.0\n",
    "        all_iterations = []\n",
    "        \n",
    "        while iteration < max_iterations:\n",
    "            iter_start = time.time_ns()\n",
    "            iteration += 1\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"ğŸ“ ITERATION {iteration}/{max_iterations}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # STEP 1: RETRIEVE (with query refinement if feedback exists)\n",
    "            print(f\"\\nğŸ” STEP 1: RETRIEVAL\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            if iteration > 1 and previous_feedback:\n",
    "                refined_query, refine_time = self.refine_query(question, previous_feedback, iteration)\n",
    "                current_query = refined_query\n",
    "            \n",
    "            # Increase k for later iterations to get more context\n",
    "            k = 4 if iteration == 1 else min(6, 4 + iteration - 1)\n",
    "            hits, context, retrieval_time = self.retrieve_documents(current_query, k, iteration)\n",
    "            \n",
    "            # STEP 2: GENERATE\n",
    "            print(f\"\\nâœï¸  STEP 2: GENERATION\")\n",
    "            print(\"-\" * 70)\n",
    "            answer, gen_time = self.generate_answer(question, context, previous_feedback, iteration)\n",
    "            \n",
    "            # STEP 3: EVALUATE\n",
    "            print(f\"\\nğŸ“Š STEP 3: EVALUATION\")\n",
    "            print(\"-\" * 70)\n",
    "            evaluation, eval_time = self.evaluate_answer(question, answer, context, iteration)\n",
    "            \n",
    "            # Track best answer so far\n",
    "            if evaluation['overall_score'] > best_score:\n",
    "                best_score = evaluation['overall_score']\n",
    "                best_answer = answer\n",
    "            \n",
    "            # STEP 4: FEEDBACK (if not passing)\n",
    "            print(f\"\\nğŸ’¬ STEP 4: FEEDBACK\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            if evaluation['pass_threshold']:\n",
    "                print(\"   âœ… Quality threshold met! No feedback needed.\")\n",
    "                feedback = \"Quality threshold met. Answer is satisfactory.\"\n",
    "                feedback_time = 0\n",
    "            else:\n",
    "                feedback, feedback_time = self.generate_feedback(\n",
    "                    question, answer, context, evaluation, iteration\n",
    "                )\n",
    "                print(f\"   ğŸ“ Feedback generated for next iteration\")\n",
    "            \n",
    "            iter_elapsed = time.time_ns() - iter_start\n",
    "            latency_report.add(\"feedback_loop_iteration\", iter_elapsed)\n",
    "            \n",
    "            # Store iteration details\n",
    "            iteration_data = {\n",
    "                'iteration': iteration,\n",
    "                'query_used': current_query,\n",
    "                'documents_retrieved': len(hits),\n",
    "                'context_length': len(context),\n",
    "                'answer': answer,\n",
    "                'answer_length': len(answer),\n",
    "                'evaluation': evaluation,\n",
    "                'feedback': feedback,\n",
    "                'time_ns': iter_elapsed,\n",
    "                'component_times': {\n",
    "                    'retrieval': retrieval_time,\n",
    "                    'generation': gen_time,\n",
    "                    'evaluation': eval_time,\n",
    "                    'feedback': feedback_time\n",
    "                }\n",
    "            }\n",
    "            all_iterations.append(iteration_data)\n",
    "            \n",
    "            print(f\"\\nâ±ï¸  Iteration {iteration} completed in {format_time_ns(iter_elapsed)}\")\n",
    "            print(f\"   ğŸ“Š Iteration Score: {evaluation['overall_score']:.1f}/10\")\n",
    "            \n",
    "            # Check if we should continue\n",
    "            if evaluation['pass_threshold']:\n",
    "                print(f\"\\nâœ… Quality threshold met! Stopping after {iteration} iteration(s)\")\n",
    "                break\n",
    "            elif iteration >= max_iterations:\n",
    "                print(f\"\\nâš ï¸  Max iterations reached. Using best answer (score: {best_score:.1f}/10)\")\n",
    "                answer = best_answer\n",
    "                break\n",
    "            else:\n",
    "                print(f\"\\nğŸ”„ Continuing to iteration {iteration + 1} with feedback...\")\n",
    "                previous_feedback = feedback\n",
    "        \n",
    "        total_query_ns = time.time_ns() - overall_start\n",
    "        latency_report.add(\"feedback_loop_query_total\", total_query_ns)\n",
    "        \n",
    "        # Display final answer\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ’¬ FINAL ANSWER:\")\n",
    "        print(\"=\"*70)\n",
    "        print(answer[:800])\n",
    "        if len(answer) > 800:\n",
    "            print(\"...\")\n",
    "        print(f\"\\nğŸ“Š Final Score: {best_score:.1f}/10\")\n",
    "        print(f\"ğŸ” Iterations Used: {iteration}/{max_iterations}\")\n",
    "        print(f\"â±ï¸  Total Time: {format_time_ns(total_query_ns)}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"final_answer\": answer,\n",
    "            \"best_score\": best_score,\n",
    "            \"iterations_used\": iteration,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"all_iterations\": all_iterations,\n",
    "            \"quality_threshold\": quality_threshold,\n",
    "            \"threshold_met\": best_score >= quality_threshold,\n",
    "            \"total_query_ns\": total_query_ns,\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# Vader Sentiment\n",
    "# ---------------------------\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores[\"compound\"]\n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "            percentage = round((compound + 1) * 50, 2)\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "            percentage = round((1 - abs(compound)) * 50, 2)\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "            percentage = round(50 + (compound * 50), 2)\n",
    "        return {\"label\": label, \"percentage\": percentage, \"compound\": compound, \"scores\": scores}\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str],\n",
    "                            target_ns: int = TARGET_NS, run_number: int = 1):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ğŸ”¥ SENTIMENT BENCHMARK RUN #{run_number}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"ğŸ¯ TARGET: < {target_ns} ns per analysis\\n\")\n",
    "    individual_times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start_ns = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed_ns = time.time_ns() - start_ns\n",
    "        latency_report.add(\"vader_per_example\", elapsed_ns)\n",
    "        individual_times.append(elapsed_ns)\n",
    "        status = \"âœ…\" if elapsed_ns < target_ns else \"âŒ\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed_ns):25s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    total_ns = sum(individual_times)\n",
    "    avg_ns = total_ns // len(individual_times)\n",
    "    min_ns = min(individual_times)\n",
    "    max_ns = max(individual_times)\n",
    "    under_target = sum(1 for t in individual_times if t < target_ns)\n",
    "    print(\"\\nğŸ“Š RUN #{run_number} STATISTICS:\")\n",
    "    print(f\"   Total:        {format_time_ns(total_ns)}\")\n",
    "    print(f\"   Average:      {format_time_ns(avg_ns)}\")\n",
    "    print(f\"   Min:          {format_time_ns(min_ns)}\")\n",
    "    print(f\"   Max:          {format_time_ns(max_ns)}\")\n",
    "    print(f\"   < {target_ns}ns: {under_target}/{len(individual_times)} texts\")\n",
    "    if avg_ns < target_ns:\n",
    "        print(\"   âœ… TARGET MET!\")\n",
    "    else:\n",
    "        print(\"   âš ï¸  TARGET MISSED\")\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ”„ RAG WITH FEEDBACK LOOP + FULL LATENCY INSTRUMENTATION\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    pipeline_start = time.time_ns()\n",
    "\n",
    "    # Phase 1: Load and prepare data\n",
    "    print(\"ğŸ“š PHASE 1: DATA PREPARATION\")\n",
    "    print(\"-\" * 70)\n",
    "    text, load_time = timed_call(load_pdf, PDF_PATH)\n",
    "    latency_report.add(\"pipeline_pdf_load\", load_time)\n",
    "\n",
    "    chunks, chunk_time = timed_call(chunk_text, text, 1000, 100)\n",
    "    latency_report.add(\"pipeline_chunking\", chunk_time)\n",
    "\n",
    "    embedder, embed_time = timed_call(load_embeddings, EMBED_MODEL)\n",
    "    latency_report.add(\"pipeline_embeddings_load\", embed_time)\n",
    "\n",
    "    milvus_collection, milvus_time = timed_call(init_milvus, MILVUS_HOST, MILVUS_PORT, COLLECTION_NAME, DIM)\n",
    "    latency_report.add(\"pipeline_milvus_init\", milvus_time)\n",
    "\n",
    "    insert_time_start = time.time_ns()\n",
    "    insert_chunks(milvus_collection, embedder, chunks)\n",
    "    insert_time = time.time_ns() - insert_time_start\n",
    "    latency_report.add(\"pipeline_insert_chunks\", insert_time)\n",
    "\n",
    "    # Phase 2: Initialize LLM\n",
    "    print(\"\\nğŸ“š PHASE 2: LLM INITIALIZATION\")\n",
    "    print(\"-\" * 70)\n",
    "    llm_start = time.time_ns()\n",
    "    llm = ChatGroq(model_name=MODEL_NAME, groq_api_key=GROQ_API_KEY, temperature=0)\n",
    "    llm_time = time.time_ns() - llm_start\n",
    "    latency_report.add(\"llm_init\", llm_time)\n",
    "    print(f\"âœ… LLM initialized in {format_time_ns(llm_time)}\")\n",
    "\n",
    "    # Initialize Feedback Loop RAG\n",
    "    feedback_rag = FeedbackLoopRAG(llm, milvus_collection, embedder)\n",
    "    print(\"\\nâœ… RAG with Feedback Loop system initialized!\")\n",
    "\n",
    "    # Phase 3: Run queries\n",
    "    print(\"\\nğŸ“š PHASE 3: FEEDBACK LOOP RAG QUERIES\")\n",
    "    print(\"-\" * 70)\n",
    "    queries = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Describe the character development and relationships throughout the narrative.\",\n",
    "        \"Analyze the emotional journey and key turning points in the plot.\"\n",
    "    ]\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        result = feedback_rag.query(q, max_iterations=3, quality_threshold=7.5)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print iteration summary\n",
    "        print(f\"\\nğŸ“ˆ QUERY SUMMARY:\")\n",
    "        print(f\"   Question: {q[:60]}...\")\n",
    "        print(f\"   Final Score: {result['best_score']:.1f}/10\")\n",
    "        print(f\"   Threshold Met: {'âœ… YES' if result['threshold_met'] else 'âŒ NO'}\")\n",
    "        print(f\"   Iterations: {result['iterations_used']}/{result['max_iterations']}\")\n",
    "        print(f\"   Total Time: {format_time_ns(result['total_query_ns'])}\")\n",
    "\n",
    "    # Phase 4: Sentiment benchmark\n",
    "    print(\"\\nğŸ“š PHASE 4: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"-\" * 70)\n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    sa_init = 0\n",
    "    latency_report.add(\"vader_init\", sa_init)\n",
    "    print(f\"âœ… VADER INIT TIME: {format_time_ns(sa_init)}\\n\")\n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    for run in range(1, 3):\n",
    "        run_sentiment_benchmark(sa, examples, TARGET_NS, run)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency_report.add(\"pipeline_total\", pipeline_total)\n",
    "\n",
    "    # Final Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“ˆ PIPELINE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total pipeline time: {format_time_ns(pipeline_total)}\")\n",
    "    if results:\n",
    "        print(f\"\\nQueries executed: {len(queries)}\")\n",
    "        avg_time = sum(r['total_query_ns'] for r in results) // len(results)\n",
    "        print(f\"Average query time: {format_time_ns(avg_time)}\")\n",
    "        \n",
    "        avg_score = sum(r['best_score'] for r in results) / len(results)\n",
    "        print(f\"Average quality score: {avg_score:.2f}/10\")\n",
    "        \n",
    "        threshold_met = sum(1 for r in results if r['threshold_met'])\n",
    "        print(f\"Threshold met: {threshold_met}/{len(results)} queries\")\n",
    "        \n",
    "        total_iterations = sum(r['iterations_used'] for r in results)\n",
    "        print(f\"Total iterations used: {total_iterations}\")\n",
    "    \n",
    "    latency_report.pretty_print()\n",
    "\n",
    "    try:\n",
    "        connections.disconnect()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"âœ… PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâš ï¸  Interrupted by user\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nâŒ Fatal error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
