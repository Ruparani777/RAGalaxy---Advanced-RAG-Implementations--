{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aececd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import json\n",
    "\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    utility,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION_NAME = \"PlanSolveRAG_Documents\"\n",
    "DIM = 384\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000\n",
    "\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"‚ùå ERROR: Set GROQ_API_KEY environment variable!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------------------\n",
    "# LATENCY UTILITIES\n",
    "# ---------------------------\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "def timer_ns(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"‚è±Ô∏è  {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    def summary(self) -> Dict:\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // len(vals) if vals else 0,\n",
    "                \"min_ns\": min(vals) if vals else 0,\n",
    "                \"max_ns\": max(vals) if vals else 0\n",
    "            }\n",
    "        return out\n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY (nanoseconds)\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items(), key=lambda p: p[0]):\n",
    "            print(f\"\\nüìä Component: {comp}\")\n",
    "            print(f\"   Count:     {stats['count']}\")\n",
    "            print(f\"   Total:     {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"   Average:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"   Min:       {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"   Max:       {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# ---------------------------\n",
    "# PDF Loader\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def load_pdf(path: str) -> str:\n",
    "    print(f\"üìÑ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, p in enumerate(pdf.pages):\n",
    "            start_ns = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start_ns\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            text += t + \"\\n\"\n",
    "    print(f\"‚úÖ Loaded PDF: {len(text)} characters from {len(pdf.pages)} pages\")\n",
    "    return text\n",
    "\n",
    "# ---------------------------\n",
    "# Chunker\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    print(\"‚úÇÔ∏è  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# ---------------------------\n",
    "# Embeddings loader\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def load_embeddings(model_name: str = EMBED_MODEL) -> SentenceTransformer:\n",
    "    print(f\"üî¢ Loading embeddings model: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    print(\"‚úÖ Embeddings model loaded\")\n",
    "    return embedder\n",
    "\n",
    "# ---------------------------\n",
    "# Milvus init (AUTO-ID primary key)\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def init_milvus(host: str, port: str, collection_name: str = COLLECTION_NAME, dim: int = DIM) -> Collection:\n",
    "    print(f\"üóÉÔ∏è  Initializing Milvus connection to {host}:{port}\")\n",
    "    connections.connect(host=host, port=port)\n",
    "\n",
    "    # drop if exists\n",
    "    try:\n",
    "        if utility.has_collection(collection_name):\n",
    "            utility.drop_collection(collection_name)\n",
    "            print(f\"üóëÔ∏è  Deleted existing collection '{collection_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Collection check/delete: {e}\")\n",
    "\n",
    "    # primary key auto-id field first\n",
    "    chunk_id_field = FieldSchema(name=\"chunk_id\", dtype=DataType.INT64, is_primary=True, auto_id=True)\n",
    "    text_field = FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535)\n",
    "    source_field = FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=1024)\n",
    "    embedding_field = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "\n",
    "    schema = CollectionSchema(fields=[chunk_id_field, text_field, source_field, embedding_field],\n",
    "                              description=\"Plan-and-Solve RAG document chunks\")\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    # create index and load\n",
    "    index_params = {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 1024}}\n",
    "    try:\n",
    "        collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  create_index: {e}\")\n",
    "    try:\n",
    "        collection.load()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  load: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Milvus collection '{collection_name}' ready\")\n",
    "    return collection\n",
    "\n",
    "# ---------------------------\n",
    "# Insert chunks (no manual IDs)\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def insert_chunks(collection: Collection, embedder: SentenceTransformer, chunks: List[str]) -> None:\n",
    "    print(f\"‚¨ÜÔ∏è  Inserting {len(chunks)} chunks into Milvus...\")\n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_encode_batch\", encode_time)\n",
    "    print(f\"   ‚úÖ Encoded in {format_time_ns(encode_time)}\")\n",
    "\n",
    "    texts = chunks\n",
    "    sources = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "    embeddings = [v.tolist() if hasattr(v, \"tolist\") else list(v) for v in vectors]\n",
    "\n",
    "    start = time.time_ns()\n",
    "    collection.insert([texts, sources, embeddings])\n",
    "    insert_time = time.time_ns() - start\n",
    "    latency_report.add(\"milvus_insert\", insert_time)\n",
    "    print(f\"   ‚úÖ Inserted {len(chunks)} vectors in {format_time_ns(insert_time)}\")\n",
    "\n",
    "    start = time.time_ns()\n",
    "    collection.flush()\n",
    "    flush_time = time.time_ns() - start\n",
    "    latency_report.add(\"milvus_flush\", flush_time)\n",
    "    print(f\"   ‚úÖ Flushed collection ({format_time_ns(flush_time)})\")\n",
    "\n",
    "# ---------------------------\n",
    "# Search\n",
    "# ---------------------------\n",
    "def search_milvus(collection: Collection, embedder: SentenceTransformer, query: str, limit: int = 4) -> Tuple[List[str], int]:\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"query_embedding\", encode_time)\n",
    "\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    start = time.time_ns()\n",
    "    try:\n",
    "        results = collection.search(data=[qvec.tolist()], anns_field=\"embedding\", param=search_params, limit=limit,\n",
    "                                    output_fields=[\"text\", \"source\", \"chunk_id\"])\n",
    "        search_time = time.time_ns() - start\n",
    "        latency_report.add(\"milvus_search\", search_time)\n",
    "\n",
    "        hits = []\n",
    "        for hit in results[0]:\n",
    "            try:\n",
    "                ent = getattr(hit, \"entity\", None) or getattr(hit, \"_fields\", None) or {}\n",
    "                if isinstance(ent, dict):\n",
    "                    txt = ent.get(\"text\", \"\")\n",
    "                else:\n",
    "                    txt = str(hit)\n",
    "                hits.append(txt)\n",
    "            except Exception:\n",
    "                hits.append(\"\")\n",
    "    except Exception as e:\n",
    "        search_time = time.time_ns() - start\n",
    "        latency_report.add(\"milvus_search_error\", search_time)\n",
    "        print(f\"‚ö†Ô∏è Milvus search failed: {e}\")\n",
    "        hits = []\n",
    "    total_time = encode_time + (latency_report.store.get(\"milvus_search\", [-1])[-1] if latency_report.store.get(\"milvus_search\") else 0)\n",
    "    return hits, total_time\n",
    "\n",
    "# ---------------------------\n",
    "# Plan-and-Solve Agentic RAG\n",
    "# ---------------------------\n",
    "class PlanAndSolveRAG:\n",
    "    \"\"\"\n",
    "    Plan-and-Solve Agentic RAG System\n",
    "    \n",
    "    Workflow:\n",
    "    1. PLAN: Decompose complex query into sub-tasks\n",
    "    2. SOLVE: Execute each sub-task with retrieval\n",
    "    3. SYNTHESIZE: Combine results into final answer\n",
    "    4. VERIFY: Check completeness and quality\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, collection: Collection, embedder: SentenceTransformer):\n",
    "        self.llm = llm\n",
    "        self.collection = collection\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def _llm_invoke_timed(self, prompt: str, label: str) -> Tuple[str, int]:\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label, elapsed)\n",
    "            content = response.content if hasattr(response, \"content\") else str(response)\n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label + \"_error\", elapsed)\n",
    "            print(f\"LLM invoke for {label} failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return str(e), elapsed\n",
    "\n",
    "    def plan_decomposition(self, query: str) -> Tuple[List[Dict[str, str]], int]:\n",
    "        \"\"\"\n",
    "        STEP 1: Decompose the query into sub-tasks\n",
    "        Returns: List of sub-tasks with types and queries\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are a strategic planning agent. Analyze this query and decompose it into logical sub-tasks.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Decompose this into 2-4 sub-tasks. For each sub-task, specify:\n",
    "1. task_id: (e.g., \"task_1\", \"task_2\")\n",
    "2. task_type: (\"retrieve\" for document search, \"analyze\" for reasoning, \"synthesize\" for combining)\n",
    "3. description: What needs to be done\n",
    "4. query: The specific question or search query for this sub-task\n",
    "\n",
    "Format your response as a numbered list:\n",
    "1. [task_id] (task_type): description\n",
    "   Query: specific query\n",
    "\n",
    "Example:\n",
    "1. [task_1] (retrieve): Find information about main themes\n",
    "   Query: What are the main themes?\n",
    "2. [task_2] (analyze): Identify character relationships\n",
    "   Query: How do characters relate to each other?\n",
    "\n",
    "Sub-tasks:\"\"\"\n",
    "        \n",
    "        response, elapsed = self._llm_invoke_timed(prompt, \"llm_plan_decomposition\")\n",
    "        \n",
    "        # Parse the response into structured sub-tasks\n",
    "        sub_tasks = self._parse_plan(response)\n",
    "        \n",
    "        print(f\"üìã PLAN: Generated {len(sub_tasks)} sub-tasks\")\n",
    "        for i, task in enumerate(sub_tasks, 1):\n",
    "            print(f\"   [{i}] {task['task_type'].upper()}: {task['description'][:60]}...\")\n",
    "        \n",
    "        return sub_tasks, elapsed\n",
    "\n",
    "    def _parse_plan(self, plan_text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Parse the plan text into structured sub-tasks\"\"\"\n",
    "        sub_tasks = []\n",
    "        lines = plan_text.strip().split('\\n')\n",
    "        \n",
    "        current_task = {}\n",
    "        task_counter = 1\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Check if line starts with a number (new task)\n",
    "            if line[0].isdigit() and '.' in line[:3]:\n",
    "                # Save previous task if exists\n",
    "                if current_task and 'description' in current_task:\n",
    "                    sub_tasks.append(current_task)\n",
    "                \n",
    "                # Start new task\n",
    "                current_task = {\n",
    "                    'task_id': f\"task_{task_counter}\",\n",
    "                    'task_type': 'retrieve',  # default\n",
    "                    'description': '',\n",
    "                    'query': ''\n",
    "                }\n",
    "                task_counter += 1\n",
    "                \n",
    "                # Parse task type and description\n",
    "                if '(retrieve)' in line.lower():\n",
    "                    current_task['task_type'] = 'retrieve'\n",
    "                elif '(analyze)' in line.lower():\n",
    "                    current_task['task_type'] = 'analyze'\n",
    "                elif '(synthesize)' in line.lower():\n",
    "                    current_task['task_type'] = 'synthesize'\n",
    "                \n",
    "                # Extract description (after the colon)\n",
    "                if ':' in line:\n",
    "                    current_task['description'] = line.split(':', 1)[1].strip()\n",
    "                else:\n",
    "                    current_task['description'] = line\n",
    "                    \n",
    "            elif line.lower().startswith('query:'):\n",
    "                # Extract query\n",
    "                current_task['query'] = line.split(':', 1)[1].strip()\n",
    "        \n",
    "        # Add last task\n",
    "        if current_task and 'description' in current_task:\n",
    "            sub_tasks.append(current_task)\n",
    "        \n",
    "        # If parsing failed, create a default task\n",
    "        if not sub_tasks:\n",
    "            sub_tasks = [{\n",
    "                'task_id': 'task_1',\n",
    "                'task_type': 'retrieve',\n",
    "                'description': 'Answer the query',\n",
    "                'query': plan_text[:200]\n",
    "            }]\n",
    "        \n",
    "        return sub_tasks\n",
    "\n",
    "    def solve_subtask(self, sub_task: Dict[str, str]) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        STEP 2: Solve a single sub-task\n",
    "        \"\"\"\n",
    "        task_type = sub_task['task_type']\n",
    "        query = sub_task['query'] or sub_task['description']\n",
    "        \n",
    "        print(f\"\\n   üîß Solving: {sub_task['task_id']} ({task_type})\")\n",
    "        \n",
    "        total_time = 0\n",
    "        \n",
    "        if task_type == 'retrieve':\n",
    "            # Retrieve relevant documents\n",
    "            context, retrieval_time = self.retrieve_documents(query, k=3)\n",
    "            total_time += retrieval_time\n",
    "            \n",
    "            # Generate answer based on context\n",
    "            answer, gen_time = self._generate_with_context(query, context)\n",
    "            total_time += gen_time\n",
    "            \n",
    "        elif task_type == 'analyze':\n",
    "            # May retrieve for analysis, but focus on reasoning\n",
    "            context, retrieval_time = self.retrieve_documents(query, k=2)\n",
    "            total_time += retrieval_time\n",
    "            \n",
    "            analyze_prompt = f\"\"\"Analyze the following information and provide insights:\n",
    "\n",
    "Context: {context if context else \"No specific context available.\"}\n",
    "\n",
    "Task: {sub_task['description']}\n",
    "Query: {query}\n",
    "\n",
    "Provide a thoughtful analysis:\"\"\"\n",
    "            \n",
    "            answer, gen_time = self._llm_invoke_timed(analyze_prompt, \"llm_analyze_subtask\")\n",
    "            total_time += gen_time\n",
    "            \n",
    "        else:  # synthesize or default\n",
    "            # Use general knowledge or previous context\n",
    "            answer, gen_time = self._generate_with_context(query, \"\")\n",
    "            total_time += gen_time\n",
    "        \n",
    "        print(f\"   ‚úÖ Completed in {format_time_ns(total_time)}\")\n",
    "        return answer, total_time\n",
    "\n",
    "    def retrieve_documents(self, query: str, k: int = 4) -> Tuple[str, int]:\n",
    "        \"\"\"Retrieve relevant documents from vector store\"\"\"\n",
    "        hits, elapsed = search_milvus(self.collection, self.embedder, query, k)\n",
    "        context = \"\\n\\n\".join(hits)\n",
    "        return context, elapsed\n",
    "\n",
    "    def _generate_with_context(self, query: str, context: str) -> Tuple[str, int]:\n",
    "        \"\"\"Generate answer with or without context\"\"\"\n",
    "        if context:\n",
    "            prompt = f\"\"\"Based on the following context, answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Answer the following question:\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        return self._llm_invoke_timed(prompt, \"llm_generate_answer\")\n",
    "\n",
    "    def synthesize_results(self, query: str, sub_results: List[Dict[str, Any]]) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        STEP 3: Synthesize all sub-task results into final answer\n",
    "        \"\"\"\n",
    "        print(f\"\\n   üîÑ Synthesizing {len(sub_results)} results...\")\n",
    "        \n",
    "        # Compile all sub-results\n",
    "        results_text = \"\"\n",
    "        for i, res in enumerate(sub_results, 1):\n",
    "            results_text += f\"\\n\\nSub-task {i} ({res['task_id']}):\\n\"\n",
    "            results_text += f\"Description: {res['description']}\\n\"\n",
    "            results_text += f\"Result: {res['answer'][:300]}...\\n\"\n",
    "        \n",
    "        synthesis_prompt = f\"\"\"You are synthesizing multiple sub-task results to answer the original query.\n",
    "\n",
    "Original Query: {query}\n",
    "\n",
    "Sub-task Results:\n",
    "{results_text}\n",
    "\n",
    "Instructions:\n",
    "1. Combine insights from all sub-tasks\n",
    "2. Create a coherent, comprehensive answer\n",
    "3. Highlight key findings\n",
    "4. Ensure logical flow\n",
    "\n",
    "Final Synthesized Answer:\"\"\"\n",
    "        \n",
    "        final_answer, elapsed = self._llm_invoke_timed(synthesis_prompt, \"llm_synthesize\")\n",
    "        print(f\"   ‚úÖ Synthesis completed in {format_time_ns(elapsed)}\")\n",
    "        \n",
    "        return final_answer, elapsed\n",
    "\n",
    "    def verify_solution(self, query: str, final_answer: str, sub_results: List[Dict]) -> Tuple[str, bool, int]:\n",
    "        \"\"\"\n",
    "        STEP 4: Verify the solution quality and completeness\n",
    "        \"\"\"\n",
    "        verify_prompt = f\"\"\"You are a quality verification agent. Evaluate the answer for completeness and accuracy.\n",
    "\n",
    "Original Query: {query}\n",
    "\n",
    "Final Answer:\n",
    "{final_answer}\n",
    "\n",
    "Number of sub-tasks completed: {len(sub_results)}\n",
    "\n",
    "Evaluate:\n",
    "1. Completeness Score (1-10): Does it fully address the query?\n",
    "2. Coherence Score (1-10): Is it well-organized and logical?\n",
    "3. Accuracy Score (1-10): Is the information accurate based on available evidence?\n",
    "4. Missing Elements: What's missing, if anything?\n",
    "5. Recommendation: ACCEPT or REPLAN (if significant gaps exist)\n",
    "\n",
    "Verification Report:\"\"\"\n",
    "        \n",
    "        verification, elapsed = self._llm_invoke_timed(verify_prompt, \"llm_verify\")\n",
    "        \n",
    "        # Determine if we need to replan\n",
    "        needs_replan = \"REPLAN\" in verification.upper()\n",
    "        \n",
    "        print(f\"   ‚úÖ Verification completed in {format_time_ns(elapsed)}\")\n",
    "        print(f\"   Status: {'üîÑ REPLAN NEEDED' if needs_replan else '‚úÖ ACCEPTED'}\")\n",
    "        \n",
    "        return verification, needs_replan, elapsed\n",
    "\n",
    "    def query(self, question: str, max_iterations: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main Plan-and-Solve query execution\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üöÄ PLAN-AND-SOLVE AGENTIC RAG\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        iteration = 0\n",
    "        final_answer = \"\"\n",
    "        verification_report = \"\"\n",
    "        all_iterations = []\n",
    "        \n",
    "        while iteration < max_iterations:\n",
    "            iter_start = time.time_ns()\n",
    "            iteration += 1\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"ITERATION {iteration}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # STEP 1: PLAN - Decompose query\n",
    "            print(\"\\nüéØ STEP 1: PLANNING\")\n",
    "            print(\"-\" * 70)\n",
    "            sub_tasks, plan_time = self.plan_decomposition(question)\n",
    "            \n",
    "            # STEP 2: SOLVE - Execute sub-tasks\n",
    "            print(f\"\\n‚öôÔ∏è  STEP 2: SOLVING ({len(sub_tasks)} sub-tasks)\")\n",
    "            print(\"-\" * 70)\n",
    "            sub_results = []\n",
    "            solve_start = time.time_ns()\n",
    "            \n",
    "            for sub_task in sub_tasks:\n",
    "                answer, task_time = self.solve_subtask(sub_task)\n",
    "                sub_results.append({\n",
    "                    'task_id': sub_task['task_id'],\n",
    "                    'task_type': sub_task['task_type'],\n",
    "                    'description': sub_task['description'],\n",
    "                    'query': sub_task['query'],\n",
    "                    'answer': answer,\n",
    "                    'time_ns': task_time\n",
    "                })\n",
    "            \n",
    "            solve_time = time.time_ns() - solve_start\n",
    "            \n",
    "            # STEP 3: SYNTHESIZE - Combine results\n",
    "            print(f\"\\nüîó STEP 3: SYNTHESIS\")\n",
    "            print(\"-\" * 70)\n",
    "            final_answer, synthesis_time = self.synthesize_results(question, sub_results)\n",
    "            \n",
    "            # STEP 4: VERIFY - Check quality\n",
    "            print(f\"\\n‚úîÔ∏è  STEP 4: VERIFICATION\")\n",
    "            print(\"-\" * 70)\n",
    "            verification_report, needs_replan, verify_time = self.verify_solution(\n",
    "                question, final_answer, sub_results\n",
    "            )\n",
    "            \n",
    "            iter_elapsed = time.time_ns() - iter_start\n",
    "            latency_report.add(\"plan_solve_iteration\", iter_elapsed)\n",
    "            \n",
    "            all_iterations.append({\n",
    "                'iteration': iteration,\n",
    "                'plan': sub_tasks,\n",
    "                'results': sub_results,\n",
    "                'final_answer': final_answer,\n",
    "                'verification': verification_report,\n",
    "                'time_ns': iter_elapsed\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n‚è±Ô∏è  Iteration {iteration} total time: {format_time_ns(iter_elapsed)}\")\n",
    "            \n",
    "            # Check if we should continue\n",
    "            if not needs_replan or iteration >= max_iterations:\n",
    "                print(f\"\\n‚úÖ Plan-and-Solve completed after {iteration} iteration(s)\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"\\nüîÑ Replanning needed, starting iteration {iteration + 1}...\")\n",
    "        \n",
    "        total_query_ns = time.time_ns() - overall_start\n",
    "        latency_report.add(\"plan_solve_query_total\", total_query_ns)\n",
    "        \n",
    "        # Display final answer\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üí¨ FINAL ANSWER:\")\n",
    "        print(\"=\"*70)\n",
    "        print(final_answer[:800])\n",
    "        if len(final_answer) > 800:\n",
    "            print(\"...\")\n",
    "        print(f\"\\n‚è±Ô∏è  Total query time: {format_time_ns(total_query_ns)}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"iterations\": iteration,\n",
    "            \"all_iterations\": all_iterations,\n",
    "            \"verification\": verification_report,\n",
    "            \"total_query_ns\": total_query_ns,\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# Vader Sentiment\n",
    "# ---------------------------\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores[\"compound\"]\n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "            percentage = round((compound + 1) * 50, 2)\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "            percentage = round((1 - abs(compound)) * 50, 2)\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "            percentage = round(50 + (compound * 50), 2)\n",
    "        return {\"label\": label, \"percentage\": percentage, \"compound\": compound, \"scores\": scores}\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str],\n",
    "                            target_ns: int = TARGET_NS, run_number: int = 1):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üî• SENTIMENT BENCHMARK RUN #{run_number}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üéØ TARGET: < {target_ns} ns per analysis\\n\")\n",
    "    individual_times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start_ns = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed_ns = time.time_ns() - start_ns\n",
    "        latency_report.add(\"vader_per_example\", elapsed_ns)\n",
    "        individual_times.append(elapsed_ns)\n",
    "        status = \"‚úÖ\" if elapsed_ns < target_ns else \"‚ùå\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed_ns):25s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    total_ns = sum(individual_times)\n",
    "    avg_ns = total_ns // len(individual_times)\n",
    "    min_ns = min(individual_times)\n",
    "    max_ns = max(individual_times)\n",
    "    under_target = sum(1 for t in individual_times if t < target_ns)\n",
    "    print(\"\\nüìä RUN #{run_number} STATISTICS:\")\n",
    "    print(f\"   Total:        {format_time_ns(total_ns)}\")\n",
    "    print(f\"   Average:      {format_time_ns(avg_ns)}\")\n",
    "    print(f\"   Min:          {format_time_ns(min_ns)}\")\n",
    "    print(f\"   Max:          {format_time_ns(max_ns)}\")\n",
    "    print(f\"   < {target_ns}ns: {under_target}/{len(individual_times)} texts\")\n",
    "    if avg_ns < target_ns:\n",
    "        print(\"   ‚úÖ TARGET MET!\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  TARGET MISSED\")\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üöÄ PLAN-AND-SOLVE AGENTIC RAG + FULL LATENCY INSTRUMENTATION\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    pipeline_start = time.time_ns()\n",
    "\n",
    "    # Phase 1: Load and prepare data\n",
    "    print(\"üìö PHASE 1: DATA PREPARATION\")\n",
    "    print(\"-\" * 70)\n",
    "    text, load_time = timed_call(load_pdf, PDF_PATH)\n",
    "    latency_report.add(\"pipeline_pdf_load\", load_time)\n",
    "\n",
    "    chunks, chunk_time = timed_call(chunk_text, text, 1000, 100)\n",
    "    latency_report.add(\"pipeline_chunking\", chunk_time)\n",
    "\n",
    "    embedder, embed_time = timed_call(load_embeddings, EMBED_MODEL)\n",
    "    latency_report.add(\"pipeline_embeddings_load\", embed_time)\n",
    "\n",
    "    milvus_collection, milvus_time = timed_call(init_milvus, MILVUS_HOST, MILVUS_PORT, COLLECTION_NAME, DIM)\n",
    "    latency_report.add(\"pipeline_milvus_init\", milvus_time)\n",
    "\n",
    "    insert_time_start = time.time_ns()\n",
    "    insert_chunks(milvus_collection, embedder, chunks)\n",
    "    insert_time = time.time_ns() - insert_time_start\n",
    "    latency_report.add(\"pipeline_insert_chunks\", insert_time)\n",
    "\n",
    "    # Phase 2: Initialize LLM\n",
    "    print(\"\\nüìö PHASE 2: LLM INITIALIZATION\")\n",
    "    print(\"-\" * 70)\n",
    "    llm_start = time.time_ns()\n",
    "    llm = ChatGroq(model_name=MODEL_NAME, groq_api_key=GROQ_API_KEY, temperature=0)\n",
    "    llm_time = time.time_ns() - llm_start\n",
    "    latency_report.add(\"llm_init\", llm_time)\n",
    "    print(f\"‚úÖ LLM initialized in {format_time_ns(llm_time)}\")\n",
    "\n",
    "    # Initialize Plan-and-Solve RAG\n",
    "    plan_solve_rag = PlanAndSolveRAG(llm, milvus_collection, embedder)\n",
    "    print(\"\\n‚úÖ Plan-and-Solve Agentic RAG system initialized!\")\n",
    "\n",
    "    # Phase 3: Run queries\n",
    "    print(\"\\nüìö PHASE 3: PLAN-AND-SOLVE RAG QUERIES\")\n",
    "    print(\"-\" * 70)\n",
    "    queries = [\n",
    "        \"What are the main themes and character relationships in this story?\",\n",
    "        \"Summarize the key events and emotional arcs in the document.\",\n",
    "        \"Compare the narrative structure and writing style used throughout.\"\n",
    "    ]\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        result = plan_solve_rag.query(q, max_iterations=2)\n",
    "        results.append(result)\n",
    "\n",
    "    # Phase 4: Sentiment benchmark\n",
    "    print(\"\\nüìö PHASE 4: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"-\" * 70)\n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    sa_init = 0\n",
    "    latency_report.add(\"vader_init\", sa_init)\n",
    "    print(f\"‚úÖ VADER INIT TIME: {format_time_ns(sa_init)}\\n\")\n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    for run in range(1, 3):\n",
    "        run_sentiment_benchmark(sa, examples, TARGET_NS, run)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency_report.add(\"pipeline_total\", pipeline_total)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìà PIPELINE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total pipeline time: {format_time_ns(pipeline_total)}\")\n",
    "    if results:\n",
    "        print(f\"Queries executed: {len(queries)}\")\n",
    "        print(f\"Average query time: {format_time_ns(sum(r['total_query_ns'] for r in results) // len(results))}\")\n",
    "    latency_report.pretty_print()\n",
    "\n",
    "    try:\n",
    "        connections.disconnect()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Interrupted by user\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Fatal error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
