{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ab5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "qdrant_feedback_rag.py\n",
    "RAG with Feedback Loops - Learn from user feedback to improve responses\n",
    "\n",
    "Feedback Loop Features:\n",
    "1. User Feedback Collection: Thumbs up/down, ratings, corrections\n",
    "2. Feedback Analysis: Extract patterns from negative feedback\n",
    "3. Response Refinement: Improve responses based on feedback\n",
    "4. Quality Tracking: Monitor improvement over time\n",
    "5. Adaptive Retrieval: Adjust search based on feedback history\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict, deque\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import pdfplumber\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION = \"feedback_rag_collection\"\n",
    "DIM = 384\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"‚ùå ERROR: Set GROQ_API_KEY environment variable!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =========================================================\n",
    "# LATENCY UTILITIES\n",
    "# =========================================================\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "def timer_ns(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"‚è±Ô∏è  {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    \n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    \n",
    "    def pretty_print(self):\n",
    "        s = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            s[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total\": format_time_ns(total),\n",
    "                \"avg\": format_time_ns(total // len(vals) if vals else 0),\n",
    "                \"min\": format_time_ns(min(vals) if vals else 0),\n",
    "                \"max\": format_time_ns(max(vals) if vals else 0)\n",
    "            }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items()):\n",
    "            print(f\"\\nüìä Component: {comp}\")\n",
    "            for k, v in stats.items():\n",
    "                print(f\"   {k.capitalize():10s} {v}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# =========================================================\n",
    "# FEEDBACK STRUCTURES\n",
    "# =========================================================\n",
    "@dataclass\n",
    "class UserFeedback:\n",
    "    \"\"\"User feedback on a response\"\"\"\n",
    "    query_id: int\n",
    "    question: str\n",
    "    answer: str\n",
    "    feedback_type: str  # positive, negative, correction, rating\n",
    "    rating: Optional[int]  # 1-5 if rating\n",
    "    comment: Optional[str]\n",
    "    correction: Optional[str]\n",
    "    timestamp: str\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class FeedbackPattern:\n",
    "    \"\"\"Identified pattern from feedback\"\"\"\n",
    "    pattern_type: str  # length, detail, accuracy, relevance\n",
    "    issue: str\n",
    "    frequency: int\n",
    "    examples: List[str]\n",
    "    first_seen: str\n",
    "    last_seen: str\n",
    "\n",
    "class FeedbackManager:\n",
    "    \"\"\"\n",
    "    Manages feedback collection and learning\n",
    "    \n",
    "    Features:\n",
    "    - Collect user feedback\n",
    "    - Analyze feedback patterns\n",
    "    - Adjust retrieval and generation\n",
    "    - Track quality metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feedback_history: List[UserFeedback] = []\n",
    "        self.feedback_patterns: Dict[str, FeedbackPattern] = {}\n",
    "        self.query_counter = 0\n",
    "        self.quality_metrics = {\n",
    "            'total_queries': 0,\n",
    "            'positive_feedback': 0,\n",
    "            'negative_feedback': 0,\n",
    "            'avg_rating': 0.0,\n",
    "            'improvement_rate': 0.0\n",
    "        }\n",
    "        \n",
    "        print(f\"üîÑ Feedback Manager initialized\")\n",
    "    \n",
    "    def collect_feedback(self, question: str, answer: str, \n",
    "                        feedback_type: str = \"positive\",\n",
    "                        rating: Optional[int] = None,\n",
    "                        comment: Optional[str] = None,\n",
    "                        correction: Optional[str] = None) -> UserFeedback:\n",
    "        \"\"\"Collect user feedback on a response\"\"\"\n",
    "        \n",
    "        start = time.time_ns()\n",
    "        \n",
    "        feedback = UserFeedback(\n",
    "            query_id=self.query_counter,\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            feedback_type=feedback_type,\n",
    "            rating=rating,\n",
    "            comment=comment,\n",
    "            correction=correction,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "        \n",
    "        self.feedback_history.append(feedback)\n",
    "        self.query_counter += 1\n",
    "        \n",
    "        # Update metrics\n",
    "        self.quality_metrics['total_queries'] += 1\n",
    "        if feedback_type == \"positive\":\n",
    "            self.quality_metrics['positive_feedback'] += 1\n",
    "        elif feedback_type == \"negative\":\n",
    "            self.quality_metrics['negative_feedback'] += 1\n",
    "        \n",
    "        if rating:\n",
    "            total_ratings = sum(f.rating for f in self.feedback_history if f.rating)\n",
    "            count_ratings = sum(1 for f in self.feedback_history if f.rating)\n",
    "            self.quality_metrics['avg_rating'] = total_ratings / count_ratings if count_ratings else 0\n",
    "        \n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"feedback_collect\", elapsed)\n",
    "        \n",
    "        print(f\"‚úÖ Feedback collected: {feedback_type}\" + \n",
    "              (f\" (rating: {rating}/5)\" if rating else \"\"))\n",
    "        \n",
    "        return feedback\n",
    "    \n",
    "    def analyze_feedback_patterns(self, llm) -> Dict[str, FeedbackPattern]:\n",
    "        \"\"\"Analyze feedback to identify patterns\"\"\"\n",
    "        \n",
    "        if len(self.feedback_history) < 2:\n",
    "            return {}\n",
    "        \n",
    "        start = time.time_ns()\n",
    "        \n",
    "        # Get recent negative feedback\n",
    "        recent_negative = [\n",
    "            f for f in self.feedback_history[-10:] \n",
    "            if f.feedback_type == \"negative\"\n",
    "        ]\n",
    "        \n",
    "        if not recent_negative:\n",
    "            return self.feedback_patterns\n",
    "        \n",
    "        # Analyze with LLM\n",
    "        feedback_text = \"\\n\\n\".join([\n",
    "            f\"Q: {f.question}\\nA: {f.answer[:200]}...\\nComment: {f.comment or 'None'}\"\n",
    "            for f in recent_negative\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"Analyze these negative feedback instances and identify patterns.\n",
    "\n",
    "Feedback Examples:\n",
    "{feedback_text}\n",
    "\n",
    "Identify common issues (e.g., too brief, lacks detail, off-topic, incorrect).\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"patterns\": [\n",
    "    {{\"type\": \"length|detail|accuracy|relevance\", \"issue\": \"description\", \"frequency\": 3}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Analysis:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                data = json.loads(json_match.group())\n",
    "                patterns = data.get('patterns', [])\n",
    "                \n",
    "                for p in patterns:\n",
    "                    pattern_key = f\"{p['type']}_{p['issue'][:20]}\"\n",
    "                    \n",
    "                    if pattern_key in self.feedback_patterns:\n",
    "                        self.feedback_patterns[pattern_key].frequency += 1\n",
    "                        self.feedback_patterns[pattern_key].last_seen = datetime.now().isoformat()\n",
    "                    else:\n",
    "                        self.feedback_patterns[pattern_key] = FeedbackPattern(\n",
    "                            pattern_type=p['type'],\n",
    "                            issue=p['issue'],\n",
    "                            frequency=p.get('frequency', 1),\n",
    "                            examples=[],\n",
    "                            first_seen=datetime.now().isoformat(),\n",
    "                            last_seen=datetime.now().isoformat()\n",
    "                        )\n",
    "                \n",
    "                print(f\"üîç Identified {len(patterns)} feedback patterns\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Pattern analysis failed: {e}\")\n",
    "        \n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"feedback_analyze\", elapsed)\n",
    "        \n",
    "        return self.feedback_patterns\n",
    "    \n",
    "    def get_adjustment_instructions(self) -> str:\n",
    "        \"\"\"Get instructions for adjusting response based on feedback\"\"\"\n",
    "        \n",
    "        if not self.feedback_patterns:\n",
    "            return \"\"\n",
    "        \n",
    "        # Get most frequent patterns\n",
    "        top_patterns = sorted(\n",
    "            self.feedback_patterns.values(),\n",
    "            key=lambda p: p.frequency,\n",
    "            reverse=True\n",
    "        )[:3]\n",
    "        \n",
    "        instructions = []\n",
    "        for p in top_patterns:\n",
    "            if p.pattern_type == \"length\":\n",
    "                instructions.append(f\"- Provide more detailed responses ({p.issue})\")\n",
    "            elif p.pattern_type == \"detail\":\n",
    "                instructions.append(f\"- Include more specific details ({p.issue})\")\n",
    "            elif p.pattern_type == \"accuracy\":\n",
    "                instructions.append(f\"- Ensure accuracy: {p.issue}\")\n",
    "            elif p.pattern_type == \"relevance\":\n",
    "                instructions.append(f\"- Stay focused on: {p.issue}\")\n",
    "        \n",
    "        if instructions:\n",
    "            return \"Based on user feedback:\\n\" + \"\\n\".join(instructions)\n",
    "        return \"\"\n",
    "    \n",
    "    def get_quality_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get quality metrics report\"\"\"\n",
    "        \n",
    "        total = self.quality_metrics['total_queries']\n",
    "        if total == 0:\n",
    "            return self.quality_metrics\n",
    "        \n",
    "        pos = self.quality_metrics['positive_feedback']\n",
    "        neg = self.quality_metrics['negative_feedback']\n",
    "        \n",
    "        satisfaction_rate = (pos / total) * 100 if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            **self.quality_metrics,\n",
    "            'satisfaction_rate': satisfaction_rate,\n",
    "            'total_patterns': len(self.feedback_patterns)\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# PDF/CHUNKING/EMBEDDINGS/QDRANT\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def load_pdf(path: str) -> str:\n",
    "    print(f\"üìÑ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for p in pdf.pages:\n",
    "            t = p.extract_text() or \"\"\n",
    "            text += t + \"\\n\"\n",
    "    print(f\"‚úÖ Loaded PDF: {len(text)} characters\")\n",
    "    return text\n",
    "\n",
    "@timer_ns\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    print(f\"‚úÇÔ∏è  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "@timer_ns\n",
    "def load_embeddings(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    print(f\"üî¢ Loading embeddings: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    print(f\"‚úÖ Embeddings loaded\")\n",
    "    return embedder\n",
    "\n",
    "@timer_ns\n",
    "def init_qdrant(collection_name: str = COLLECTION, dim: int = DIM) -> QdrantClient:\n",
    "    print(f\"üóÉÔ∏è  Initializing Qdrant\")\n",
    "    qdrant = QdrantClient(\":memory:\")\n",
    "    \n",
    "    if qdrant.collection_exists(collection_name):\n",
    "        qdrant.delete_collection(collection_name)\n",
    "    \n",
    "    qdrant.create_collection(\n",
    "        collection_name,\n",
    "        vectors_config=VectorParams(size=dim, distance=Distance.COSINE)\n",
    "    )\n",
    "    print(f\"‚úÖ Qdrant ready\")\n",
    "    return qdrant\n",
    "\n",
    "@timer_ns\n",
    "def insert_chunks(qdrant: QdrantClient, embedder: SentenceTransformer, \n",
    "                  chunks: List[str], collection_name: str = COLLECTION) -> None:\n",
    "    print(f\"‚¨ÜÔ∏è  Inserting {len(chunks)} chunks...\")\n",
    "    \n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    \n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=vectors[i].tolist(),\n",
    "            payload={\"text\": chunks[i], \"chunk_id\": i}\n",
    "        )\n",
    "        for i in range(len(chunks))\n",
    "    ]\n",
    "    \n",
    "    qdrant.upsert(collection_name=collection_name, points=points)\n",
    "    print(f\"‚úÖ Chunks inserted!\")\n",
    "\n",
    "def search_qdrant(qdrant: QdrantClient, embedder: SentenceTransformer, \n",
    "                  query: str, limit: int = 4, collection_name: str = COLLECTION) -> Tuple[List[str], int]:\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    \n",
    "    response = qdrant.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=qvec.tolist(),\n",
    "        limit=limit\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"qdrant_search\", elapsed)\n",
    "    \n",
    "    hits = [p.payload.get(\"text\", \"\") for p in response.points]\n",
    "    return hits, elapsed\n",
    "\n",
    "# =========================================================\n",
    "# VADER SENTIMENT\n",
    "# =========================================================\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        \n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "        \n",
    "        return {'label': label, 'compound': compound}\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str], \n",
    "                            target_ns: int = 200_000):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî• SENTIMENT BENCHMARK\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üéØ TARGET: < {target_ns} ns\\n\")\n",
    "    \n",
    "    times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed = time.time_ns() - start\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        status = \"‚úÖ\" if elapsed < target_ns else \"‚ùå\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed):20s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    \n",
    "    avg = sum(times) // len(times)\n",
    "    print(f\"\\nüìä Average: {format_time_ns(avg)}\")\n",
    "\n",
    "# =========================================================\n",
    "# RAG WITH FEEDBACK LOOPS\n",
    "# =========================================================\n",
    "class FeedbackRAG:\n",
    "    \"\"\"RAG system with feedback loop integration\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, qdrant: QdrantClient, embedder: SentenceTransformer, \n",
    "                 feedback_manager: FeedbackManager, collection_name: str = COLLECTION):\n",
    "        self.llm = llm\n",
    "        self.qdrant = qdrant\n",
    "        self.embedder = embedder\n",
    "        self.feedback_manager = feedback_manager\n",
    "        self.collection_name = collection_name\n",
    "    \n",
    "    def query(self, question: str, auto_feedback: bool = False,\n",
    "              feedback_type: str = \"positive\", rating: Optional[int] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query with feedback loop integration\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            auto_feedback: Simulate automatic feedback for demo\n",
    "            feedback_type: Type of feedback (positive/negative)\n",
    "            rating: Optional 1-5 rating\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîÑ FEEDBACK RAG QUERY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        \n",
    "        # Step 1: Get adjustment instructions from feedback\n",
    "        adjustments = self.feedback_manager.get_adjustment_instructions()\n",
    "        if adjustments:\n",
    "            print(\"üìä Applying feedback adjustments:\")\n",
    "            print(f\"   {adjustments[:100]}...\")\n",
    "        \n",
    "        # Step 2: Retrieve from Qdrant\n",
    "        print(\"\\nüìö Retrieving from documents...\")\n",
    "        hits, ret_time = search_qdrant(self.qdrant, self.embedder, question, 4, self.collection_name)\n",
    "        doc_context = \"\\n\\n\".join(hits)\n",
    "        print(f\"   ‚úì Retrieved: {len(hits)} docs ({format_time_ns(ret_time)})\")\n",
    "        \n",
    "        # Step 3: Generate answer with feedback adjustments\n",
    "        print(\"üí≠ Generating answer...\")\n",
    "        \n",
    "        prompt = f\"\"\"Answer the question based on the context.\n",
    "\n",
    "{adjustments}\n",
    "\n",
    "Context:\n",
    "{doc_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a comprehensive, accurate answer.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        start = time.time_ns()\n",
    "        response = self.llm.invoke(prompt)\n",
    "        gen_time = time.time_ns() - start\n",
    "        latency_report.add(\"llm_generate_with_feedback\", gen_time)\n",
    "        \n",
    "        answer = response.content if hasattr(response, 'content') else str(response)\n",
    "        \n",
    "        print(f\"\\nüí¨ ANSWER ({format_time_ns(gen_time)}):\")\n",
    "        print(answer[:400])\n",
    "        if len(answer) > 400:\n",
    "            print(\"...\")\n",
    "        \n",
    "        # Step 4: Collect feedback (automatic for demo, or manual)\n",
    "        if auto_feedback:\n",
    "            print(f\"\\nüìù Collecting feedback...\")\n",
    "            feedback = self.feedback_manager.collect_feedback(\n",
    "                question=question,\n",
    "                answer=answer,\n",
    "                feedback_type=feedback_type,\n",
    "                rating=rating\n",
    "            )\n",
    "            \n",
    "            # Analyze patterns periodically\n",
    "            if len(self.feedback_manager.feedback_history) % 3 == 0:\n",
    "                print(\"üîç Analyzing feedback patterns...\")\n",
    "                self.feedback_manager.analyze_feedback_patterns(self.llm)\n",
    "        \n",
    "        total = time.time_ns() - overall_start\n",
    "        latency_report.add(\"feedback_rag_total\", total)\n",
    "        \n",
    "        # Get quality metrics\n",
    "        quality = self.feedback_manager.get_quality_report()\n",
    "        \n",
    "        print(f\"\\nüìä Quality Metrics:\")\n",
    "        print(f\"   Total queries: {quality['total_queries']}\")\n",
    "        print(f\"   Satisfaction: {quality.get('satisfaction_rate', 0):.1f}%\")\n",
    "        print(f\"   Avg rating: {quality['avg_rating']:.2f}/5\")\n",
    "        print(f\"   Patterns identified: {quality['total_patterns']}\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  Total: {format_time_ns(total)}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'quality_metrics': quality,\n",
    "            'total_time': total\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üîÑ RAG WITH FEEDBACK LOOPS\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    pipeline_start = time.time_ns()\n",
    "    \n",
    "    # Phase 1: Data preparation\n",
    "    print(\"üìö PHASE 1: DATA PREPARATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    text = load_pdf(PDF_PATH)\n",
    "    chunks = chunk_text(text, 1000, 100)\n",
    "    embedder = load_embeddings()\n",
    "    qdrant = init_qdrant(COLLECTION, DIM)\n",
    "    insert_chunks(qdrant, embedder, chunks, COLLECTION)\n",
    "    \n",
    "    # Phase 2: Initialize LLM\n",
    "    print(f\"\\nüìö PHASE 2: LLM INITIALIZATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    llm_start = time.time_ns()\n",
    "    llm = ChatGroq(model_name=MODEL_NAME, groq_api_key=GROQ_API_KEY, temperature=0)\n",
    "    llm_time = time.time_ns() - llm_start\n",
    "    latency_report.add(\"llm_init\", llm_time)\n",
    "    print(f\"‚úÖ LLM initialized ({format_time_ns(llm_time)})\")\n",
    "    \n",
    "    # Initialize Feedback Manager\n",
    "    feedback_mgr = FeedbackManager()\n",
    "    \n",
    "    # Initialize Feedback RAG\n",
    "    fb_rag = FeedbackRAG(llm, qdrant, embedder, feedback_mgr, COLLECTION)\n",
    "    print(f\"\\n‚úÖ Feedback RAG system initialized!\")\n",
    "    \n",
    "    # Phase 3: Queries with feedback simulation\n",
    "    print(f\"\\nüìö PHASE 3: RAG WITH FEEDBACK LOOPS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Simulate conversation with feedback\n",
    "    interactions = [\n",
    "        {\"q\": \"What are the main themes?\", \"feedback\": \"positive\", \"rating\": 5},\n",
    "        {\"q\": \"Tell me about love theme\", \"feedback\": \"negative\", \"rating\": 2, \n",
    "         \"comment\": \"Too brief, need more detail\"},\n",
    "        {\"q\": \"Elaborate on mother-daughter relationship\", \"feedback\": \"positive\", \"rating\": 4},\n",
    "        {\"q\": \"What are the key events?\", \"feedback\": \"positive\", \"rating\": 5},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for i, interaction in enumerate(interactions, 1):\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(f\"INTERACTION {i}/{len(interactions)}\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        result = fb_rag.query(\n",
    "            interaction['q'],\n",
    "            auto_feedback=True,\n",
    "            feedback_type=interaction['feedback'],\n",
    "            rating=interaction.get('rating')\n",
    "        )\n",
    "        results.append(result)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Phase 4: Sentiment benchmark\n",
    "    print(f\"\\nüìö PHASE 4: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    \n",
    "    run_sentiment_benchmark(sa, examples, 200_000)\n",
    "    \n",
    "    # Final summary\n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency_report.add(\"pipeline_total\", pipeline_total)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìà PIPELINE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    final_quality = feedback_mgr.get_quality_report()\n",
    "    \n",
    "    print(f\"Total pipeline time: {format_time_ns(pipeline_total)}\")\n",
    "    print(f\"Queries executed: {len(interactions)}\")\n",
    "    print(f\"\\nüéØ FINAL QUALITY METRICS:\")\n",
    "    print(f\"   Total queries: {final_quality['total_queries']}\")\n",
    "    print(f\"   Positive feedback: {final_quality['positive_feedback']}\")\n",
    "    print(f\"   Negative feedback: {final_quality['negative_feedback']}\")\n",
    "    print(f\"   Satisfaction rate: {final_quality.get('satisfaction_rate', 0):.1f}%\")\n",
    "    print(f\"   Average rating: {final_quality['avg_rating']:.2f}/5\")\n",
    "    print(f\"   Patterns identified: {final_quality['total_patterns']}\")\n",
    "    \n",
    "    if feedback_mgr.feedback_patterns:\n",
    "        print(f\"\\nüîç Identified Patterns:\")\n",
    "        for i, (key, pattern) in enumerate(list(feedback_mgr.feedback_patterns.items())[:3], 1):\n",
    "            print(f\"   {i}. {pattern.pattern_type}: {pattern.issue[:50]}... ({pattern.frequency}x)\")\n",
    "    \n",
    "    latency_report.pretty_print()\n",
    "    print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Interrupted by user\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Fatal error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
