{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "weaviate_plan_and_solve_with_feedback.py\n",
    "\n",
    "Plan-and-Solve Agentic RAG built from your Feedback-RAG code style.\n",
    "Replaces the single-feedback RAG with a Planner -> Step Solver (with feedback loops) -> Aggregator.\n",
    "Keeps nanosecond latency instrumentation, VADER benchmark, and feedback assessments per step.\n",
    "\n",
    "Features:\n",
    "- Planner (LLM) breaks question into steps\n",
    "- StepSolver runs a mini-FeedbackRAG for each step (retrieve -> generate -> assess -> reformulate)\n",
    "- Per-step assessment and history retained\n",
    "- Aggregator synthesizes step answers and runs a final assessment\n",
    "- LatencyReport collects timings across components\n",
    "- Safe LLM fallback when GROQ not available\n",
    "- CLI for indexing, testing, sentiment benchmark, interactive mode\n",
    "\n",
    "Run examples:\n",
    "    python weaviate_plan_and_solve_with_feedback.py --pdf \"Data/ECHOES OF HER LOVE.pdf\" --question \"Summarize chapter 2 and list themes\"\n",
    "    python weaviate_plan_and_solve_with_feedback.py --skip_index --question \"Explain main idea\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import traceback\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party libs (ensure installed)\n",
    "import pdfplumber\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "try:\n",
    "    from langchain_groq import ChatGroq\n",
    "except Exception:\n",
    "    ChatGroq = None\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION_NAME = \"PlanSolve_Documents\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "MAX_PLAN_STEPS = 6\n",
    "MAX_STEP_FEEDBACK_LOOPS = 3\n",
    "RELEVANCE_THRESHOLD = 7\n",
    "COMPLETENESS_THRESHOLD = 7\n",
    "CONFIDENCE_THRESHOLD = 6\n",
    "TARGET_NS = 200_000\n",
    "\n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\", \"21ookhjbswyl5urlawqmxw.c0.asia-southeast1.gcp.weaviate.cloud\")\n",
    "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\", None)\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", None)\n",
    "\n",
    "# ==========================\n",
    "# LATENCY UTILITIES\n",
    "# ==========================\n",
    "\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "\n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "\n",
    "    def summary(self) -> Dict[str, Any]:\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // len(vals) if vals else 0,\n",
    "                \"min_ns\": min(vals) if vals else 0,\n",
    "                \"max_ns\": max(vals) if vals else 0,\n",
    "            }\n",
    "        return out\n",
    "\n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY (nanoseconds)\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items(), key=lambda p: p[0]):\n",
    "            print(f\"\\nüìä Component: {comp}\")\n",
    "            print(f\"   Count:     {stats['count']}\")\n",
    "            print(f\"   Total:     {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"   Average:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"   Min:       {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"   Max:       {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# ==========================\n",
    "# PDF, chunking, embeddings, weaviate\n",
    "# ==========================\n",
    "\n",
    "@staticmethod\n",
    "def _safe_open_pdf(path: str):\n",
    "    return pdfplumber.open(path)\n",
    "\n",
    "\n",
    "def load_pdf(path: str) -> str:\n",
    "    print(f\"üìÑ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, p in enumerate(pdf.pages):\n",
    "            start = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            text += t + \"\\n\"\n",
    "    print(f\"‚úÖ Loaded PDF: {len(text)} characters from {i+1} pages\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    print(\"‚úÇÔ∏è  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def load_embeddings(model_name: str = EMBED_MODEL) -> SentenceTransformer:\n",
    "    print(f\"üî¢ Loading embeddings model: {model_name}\")\n",
    "    start = time.time_ns()\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"embedder_load\", elapsed)\n",
    "    print(f\"‚úÖ Embeddings model loaded in {format_time_ns(elapsed)}\")\n",
    "    return embedder\n",
    "\n",
    "\n",
    "def init_weaviate(url: str, api_key: str, collection_name: str = COLLECTION_NAME) -> weaviate.WeaviateClient:\n",
    "    print(f\"üóÉÔ∏è  Initializing Weaviate connection to {url}\")\n",
    "    start = time.time_ns()\n",
    "    client = weaviate.connect_to_weaviate_cloud(cluster_url=url, auth_credentials=Auth.api_key(api_key))\n",
    "    latency_report.add(\"weaviate_connect\", time.time_ns() - start)\n",
    "\n",
    "    try:\n",
    "        if client.collections.exists(collection_name):\n",
    "            start = time.time_ns()\n",
    "            client.collections.delete(collection_name)\n",
    "            latency_report.add(\"weaviate_delete_collection\", time.time_ns() - start)\n",
    "            print(f\"üóëÔ∏è  Deleted existing collection '{collection_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Collection check/delete: {e}\")\n",
    "\n",
    "    try:\n",
    "        start = time.time_ns()\n",
    "        client.collections.create(\n",
    "            name=collection_name,\n",
    "            vectorizer_config=None,\n",
    "            properties=[\n",
    "                {\"name\": \"text\", \"dataType\": [\"text\"]},\n",
    "                {\"name\": \"chunk_id\", \"dataType\": [\"int\"]},\n",
    "                {\"name\": \"source\", \"dataType\": [\"text\"]},\n",
    "            ],\n",
    "        )\n",
    "        latency_report.add(\"weaviate_create_collection\", time.time_ns() - start)\n",
    "        print(f\"‚úÖ Collection '{collection_name}' created\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Collection creation: {e}\")\n",
    "\n",
    "    return client\n",
    "\n",
    "\n",
    "def insert_chunks(client: weaviate.WeaviateClient, embedder: SentenceTransformer, chunks: List[str], collection_name: str = COLLECTION_NAME) -> None:\n",
    "    print(f\"‚¨ÜÔ∏è  Inserting {len(chunks)} chunks into Weaviate...\")\n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    latency_report.add(\"embedding_encode_batch\", time.time_ns() - start)\n",
    "\n",
    "    collection = client.collections.get(collection_name)\n",
    "    start = time.time_ns()\n",
    "    with collection.batch.dynamic() as batch:\n",
    "        for i, (chunk, vector) in enumerate(zip(chunks, vectors)):\n",
    "            batch.add_object(properties={\"text\": chunk, \"chunk_id\": i, \"source\": f\"chunk_{i}\"}, vector=vector.tolist())\n",
    "    latency_report.add(\"weaviate_upsert\", time.time_ns() - start)\n",
    "    print(f\"‚úÖ All chunks inserted successfully!\")\n",
    "\n",
    "\n",
    "def search_weaviate(client: weaviate.WeaviateClient, embedder: SentenceTransformer, query: str, limit: int = 4, collection_name: str = COLLECTION_NAME) -> Tuple[List[str], int]:\n",
    "    # Encode query\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"query_embedding\", encode_time)\n",
    "\n",
    "    # Query Weaviate\n",
    "    start = time.time_ns()\n",
    "    collection = client.collections.get(collection_name)\n",
    "    response = collection.query.near_vector(near_vector=qvec.tolist(), limit=limit, return_metadata=MetadataQuery(distance=True))\n",
    "    search_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_search\", search_time)\n",
    "\n",
    "    hits = [obj.properties.get(\"text\", \"\") for obj in response.objects]\n",
    "    total_time = encode_time + search_time\n",
    "    return hits, total_time\n",
    "\n",
    "# ==========================\n",
    "# Feedback Assessment\n",
    "# ==========================\n",
    "class FeedbackAssessment:\n",
    "    def __init__(self, relevance: int, completeness: int, confidence: int, issues: List[str], suggestions: List[str]):\n",
    "        self.relevance = relevance\n",
    "        self.completeness = completeness\n",
    "        self.confidence = confidence\n",
    "        self.issues = issues\n",
    "        self.suggestions = suggestions\n",
    "\n",
    "    def is_satisfactory(self) -> bool:\n",
    "        return (self.relevance >= RELEVANCE_THRESHOLD and self.completeness >= COMPLETENESS_THRESHOLD and self.confidence >= CONFIDENCE_THRESHOLD)\n",
    "\n",
    "    def get_score_summary(self) -> str:\n",
    "        return f\"R:{self.relevance}/10, C:{self.completeness}/10, Conf:{self.confidence}/10\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FeedbackAssessment(relevance={self.relevance}, completeness={self.completeness}, confidence={self.confidence})\"\n",
    "\n",
    "# ==========================\n",
    "# LLM Wrapper (safe)\n",
    "# ==========================\n",
    "class LLMWrapper:\n",
    "    def __init__(self, groq_api_key: str = None, model: str = MODEL_NAME):\n",
    "        self.model = model\n",
    "        self.groq_api_key = groq_api_key or os.getenv(\"GROQ_API_KEY\")\n",
    "        self.client = None\n",
    "        if ChatGroq and self.groq_api_key:\n",
    "            try:\n",
    "                self.client = ChatGroq(api_key=self.groq_api_key, model=self.model)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    self.client = ChatGroq(api_key=self.groq_api_key)\n",
    "                except Exception:\n",
    "                    self.client = None\n",
    "\n",
    "    def invoke(self, prompt: str) -> Any:\n",
    "        start = time.time_ns()\n",
    "        if self.client is not None:\n",
    "            try:\n",
    "                if hasattr(self.client, 'invoke'):\n",
    "                    out = self.client.invoke(prompt)\n",
    "                elif hasattr(self.client, '__call__'):\n",
    "                    out = self.client(prompt)\n",
    "                elif hasattr(self.client, 'generate'):\n",
    "                    out = self.client.generate([prompt])\n",
    "                else:\n",
    "                    out = str(self.client)\n",
    "                elapsed = time.time_ns() - start\n",
    "                latency_report.add('llm_invoke', elapsed)\n",
    "                return out\n",
    "            except Exception as e:\n",
    "                elapsed = time.time_ns() - start\n",
    "                latency_report.add('llm_invoke_error', elapsed)\n",
    "                print(f\"‚ö†Ô∏è LLM client call failed: {e}\")\n",
    "                traceback.print_exc()\n",
    "        # fallback stub\n",
    "        class _Resp:\n",
    "            def __init__(self, content):\n",
    "                self.content = content\n",
    "            def __str__(self):\n",
    "                return self.content\n",
    "        preview = '\\n'.join(prompt.strip().split('\\n')[:6])\n",
    "        stub = f\"[STUB LLM RESPONSE]\\n\\nPrompt preview:\\n{preview}\\n\\nNote: fallback used.\"\n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add('llm_invoke_stub', elapsed)\n",
    "        return _Resp(stub)\n",
    "\n",
    "# ==========================\n",
    "# Planner\n",
    "# ==========================\n",
    "class Planner:\n",
    "    def __init__(self, llm: LLMWrapper, max_steps: int = MAX_PLAN_STEPS):\n",
    "        self.llm = llm\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def create_plan(self, question: str) -> Tuple[List[str], int]:\n",
    "        prompt = f\"\"\"You are a planner. Break down the task into up to {self.max_steps} clear, independent subtasks.\n",
    "Return a numbered list (1., 2., ...), each item one concise sentence.\n",
    "Task: {question}\n",
    "Plan:\"\"\"\n",
    "        start = time.time_ns()\n",
    "        resp = self.llm.invoke(prompt)\n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add('planner_invoke', elapsed)\n",
    "\n",
    "        text = resp.content if hasattr(resp, 'content') else str(resp)\n",
    "        steps = []\n",
    "        for line in text.split('\\n'):\n",
    "            s = line.strip()\n",
    "            if not s: continue\n",
    "            # naive parse: look for leading number\n",
    "            if s[0].isdigit():\n",
    "                # remove leading numbering\n",
    "                parts = s.split('.', 1)\n",
    "                if len(parts) > 1:\n",
    "                    steps.append(parts[1].strip())\n",
    "            else:\n",
    "                # also accept bullet-like responses\n",
    "                if len(steps) < self.max_steps:\n",
    "                    steps.append(s)\n",
    "            if len(steps) >= self.max_steps:\n",
    "                break\n",
    "        if not steps:\n",
    "            steps = [question]\n",
    "        print(f\"üß≠ Planner produced {len(steps)} step(s)\")\n",
    "        return steps, elapsed\n",
    "\n",
    "# ==========================\n",
    "# Step Solver (mini-feedback RAG per step)\n",
    "# ==========================\n",
    "class StepSolver:\n",
    "    def __init__(self, llm: LLMWrapper, client: weaviate.WeaviateClient, embedder: SentenceTransformer, collection_name: str = COLLECTION_NAME, max_loops: int = MAX_STEP_FEEDBACK_LOOPS):\n",
    "        self.llm = llm\n",
    "        self.client = client\n",
    "        self.embedder = embedder\n",
    "        self.collection_name = collection_name\n",
    "        self.max_loops = max_loops\n",
    "\n",
    "    def _llm_invoke_timed(self, prompt: str, label: str) -> Tuple[str, int]:\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label, elapsed)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label + '_error', elapsed)\n",
    "            print(f\"‚ùå LLM invoke for {label} failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return str(e), elapsed\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 4) -> Tuple[str, int]:\n",
    "        hits, elapsed = search_weaviate(self.client, self.embedder, query, k, self.collection_name)\n",
    "        context = \"\\n\\n\".join(hits)\n",
    "        return context, elapsed\n",
    "\n",
    "    def assess_answer(self, question: str, answer: str, context: str, loop_num: int) -> Tuple[FeedbackAssessment, str, int]:\n",
    "        prompt = f\"\"\"You are a strict quality evaluator. Assess this answer critically.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Context length: {len(context)}\n",
    "\n",
    "Evaluate on these criteria (1-10):\n",
    "RELEVANCE: \n",
    "COMPLETENESS: \n",
    "CONFIDENCE: \n",
    "\n",
    "ISSUES:\n",
    "- \n",
    "SUGGESTIONS:\n",
    "- \n",
    "\n",
    "Return the evaluation in the exact structure above (numbers and bulleted lists).\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "        feedback_text, elapsed = self._llm_invoke_timed(prompt, f\"llm_feedback_assessment_{loop_num}\")\n",
    "\n",
    "        # parse feedback\n",
    "        relevance = completeness = confidence = 5\n",
    "        issues = []\n",
    "        suggestions = []\n",
    "        lines = feedback_text.split('\\n')\n",
    "        current = None\n",
    "        for line in lines:\n",
    "            l = line.strip()\n",
    "            up = l.upper()\n",
    "            if up.startswith('RELEVANCE'):\n",
    "                nums = [int(s) for s in ''.join(ch if ch.isdigit() or ch==',' else ' ' for ch in l).split() if s.isdigit()]\n",
    "                if nums: relevance = max(1, min(10, nums[0]))\n",
    "            elif up.startswith('COMPLETENESS'):\n",
    "                nums = [int(s) for s in ''.join(ch if ch.isdigit() or ch==',' else ' ' for ch in l).split() if s.isdigit()]\n",
    "                if nums: completeness = max(1, min(10, nums[0]))\n",
    "            elif up.startswith('CONFIDENCE'):\n",
    "                nums = [int(s) for s in ''.join(ch if ch.isdigit() or ch==',' else ' ' for ch in l).split() if s.isdigit()]\n",
    "                if nums: confidence = max(1, min(10, nums[0]))\n",
    "            elif up.startswith('ISSUES'):\n",
    "                current = 'issues'\n",
    "            elif up.startswith('SUGGESTIONS'):\n",
    "                current = 'suggestions'\n",
    "            elif l.startswith('-') and current == 'issues':\n",
    "                issues.append(l[1:].strip())\n",
    "            elif l.startswith('-') and current == 'suggestions':\n",
    "                suggestions.append(l[1:].strip())\n",
    "\n",
    "        assessment = FeedbackAssessment(relevance, completeness, confidence, issues, suggestions)\n",
    "        return assessment, feedback_text, elapsed\n",
    "\n",
    "    def reformulate_query(self, original_query: str, feedback: FeedbackAssessment, loop_num: int) -> Tuple[str, int]:\n",
    "        issues_text = '\\n'.join(f'- {i}' for i in feedback.issues[:3]) if feedback.issues else '- lacks depth'\n",
    "        suggestions_text = '\\n'.join(f'- {s}' for s in feedback.suggestions[:3]) if feedback.suggestions else '- need specifics'\n",
    "        prompt = f\"\"\"Reformulate this search query to retrieve missing info.\n",
    "Original: {original_query}\n",
    "Issues:\n",
    "{issues_text}\n",
    "Needed:\n",
    "{suggestions_text}\n",
    "Return a short improved query under 120 chars.\n",
    "\n",
    "Reformulated:\"\"\"\n",
    "        reformulated, elapsed = self._llm_invoke_timed(prompt, f\"llm_query_reformulation_{loop_num}\")\n",
    "        reformulated = reformulated.strip().split('\\n')[0][:120]\n",
    "        if len(reformulated) < 8:\n",
    "            reformulated = original_query\n",
    "        return reformulated, elapsed\n",
    "\n",
    "    def generate_answer(self, question: str, context: str, previous_feedback: str = \"\") -> Tuple[str, int]:\n",
    "        if previous_feedback:\n",
    "            prompt = f\"\"\"Improve previous answer using feedback:\n",
    "Previous feedback: {previous_feedback}\n",
    "Question: {question}\n",
    "Context:\\n{context}\\n\n",
    "Improved Answer:\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Answer the question using the context.\n",
    "Question: {question}\n",
    "Context:\\n{context}\\n\n",
    "Answer:\"\"\"\n",
    "        ans, elapsed = self._llm_invoke_timed(prompt, 'llm_generate_answer')\n",
    "        return ans, elapsed\n",
    "\n",
    "    def solve_step(self, step: str) -> Dict[str, Any]:\n",
    "        print(f\"\\n‚û°Ô∏è Solving step: {step}\")\n",
    "        current_query = step\n",
    "        feedback_history = []\n",
    "        loop_results = []\n",
    "        best_answer = \"\"\n",
    "        best_assessment = None\n",
    "\n",
    "        for loop_num in range(1, self.max_loops + 1):\n",
    "            loop_start = time.time_ns()\n",
    "\n",
    "            # retrieve\n",
    "            context, retrieval_time = self.retrieve(current_query, k=4)\n",
    "            latency_report.add(f\"step_{loop_num}_retrieval\", retrieval_time)\n",
    "\n",
    "            # previous feedback summary\n",
    "            prev_summary = \"\"\n",
    "            if feedback_history:\n",
    "                prev = feedback_history[-1]['assessment']\n",
    "                prev_summary = '; '.join(prev.issues[:2]) if prev.issues else ''\n",
    "\n",
    "            # generate\n",
    "            answer, gen_time = self.generate_answer(step, context, prev_summary)\n",
    "            latency_report.add(f\"step_{loop_num}_generation\", gen_time)\n",
    "\n",
    "            # assess\n",
    "            assessment, feedback_text, assess_time = self.assess_answer(step, answer, context, loop_num)\n",
    "            latency_report.add(f\"step_{loop_num}_assessment\", assess_time)\n",
    "\n",
    "            loop_elapsed = time.time_ns() - loop_start\n",
    "            latency_report.add('step_feedback_loop', loop_elapsed)\n",
    "\n",
    "            loop_results.append({\n",
    "                'loop': loop_num,\n",
    "                'query': current_query,\n",
    "                'context_len': len(context),\n",
    "                'answer': answer,\n",
    "                'assessment': assessment,\n",
    "                'feedback_text': feedback_text,\n",
    "                'time_ns': loop_elapsed,\n",
    "            })\n",
    "            feedback_history.append({'assessment': assessment, 'text': feedback_text})\n",
    "\n",
    "            print(f\"      üìä Loop {loop_num} assessment: {assessment.get_score_summary()}\")\n",
    "\n",
    "            if assessment.is_satisfactory():\n",
    "                print(\"      ‚úÖ Step satisfied by assessment\")\n",
    "                best_answer = answer\n",
    "                best_assessment = assessment\n",
    "                break\n",
    "            else:\n",
    "                # keep best by simple heuristic (completeness+relevance)\n",
    "                score = assessment.relevance + assessment.completeness + assessment.confidence\n",
    "                if best_assessment is None or score > (best_assessment.relevance + best_assessment.completeness + best_assessment.confidence):\n",
    "                    best_assessment = assessment\n",
    "                    best_answer = answer\n",
    "\n",
    "                if loop_num < self.max_loops:\n",
    "                    current_query, reform_time = self.reformulate_query(step, assessment, loop_num)\n",
    "                    latency_report.add(f\"step_{loop_num}_reformulation\", reform_time)\n",
    "                    print(f\"      üîÑ Reformulated query for next loop: {current_query}\")\n",
    "                else:\n",
    "                    print(\"      ‚ö†Ô∏è Max loops reached for this step\")\n",
    "\n",
    "        return {\n",
    "            'step': step,\n",
    "            'best_answer': best_answer,\n",
    "            'best_assessment': best_assessment,\n",
    "            'loops': loop_results,\n",
    "        }\n",
    "\n",
    "# ==========================\n",
    "# Aggregator: synthesize and final assessment\n",
    "# ==========================\n",
    "class Aggregator:\n",
    "    def __init__(self, llm: LLMWrapper):\n",
    "        self.llm = llm\n",
    "\n",
    "    def synthesize(self, step_results: List[Dict[str, Any]], question: str) -> Tuple[str, FeedbackAssessment]:\n",
    "        # Simple concatenation + LLM synthesis for coherence\n",
    "        combined = []\n",
    "        for r in step_results:\n",
    "            combined.append(f\"Step: {r['step']}\\nAnswer: {r['best_answer']}\")\n",
    "        combined_text = \"\\n\\n\".join(combined)\n",
    "\n",
    "        prompt = f\"\"\"You are a synthesizer. Combine the step answers below into a single coherent final answer to the question.\n",
    "Question: {question}\n",
    "\n",
    "Step answers:\n",
    "{combined_text}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "        start = time.time_ns()\n",
    "        resp = self.llm.invoke(prompt)\n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add('aggregator_invoke', elapsed)\n",
    "        final_answer = resp.content if hasattr(resp, 'content') else str(resp)\n",
    "\n",
    "        # Final assessment using the same assess prompt\n",
    "        assessor = StepSolver(self.llm, None, None)  # we only use assess_answer; pass Nones\n",
    "        # Create a small context hint\n",
    "        assessment, _, _ = assessor.assess_answer(question, final_answer, combined_text, loop_num=0)\n",
    "        return final_answer, assessment\n",
    "\n",
    "# ==========================\n",
    "# VADER Sentiment Analyzer (benchmark)\n",
    "# ==========================\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        if compound >= 0.05:\n",
    "            label = 'POSITIVE'\n",
    "            percentage = round((compound + 1) * 50, 2)\n",
    "        elif compound <= -0.05:\n",
    "            label = 'NEGATIVE'\n",
    "            percentage = round((1 - abs(compound)) * 50, 2)\n",
    "        else:\n",
    "            label = 'NEUTRAL'\n",
    "            percentage = round(50 + (compound * 50), 2)\n",
    "        return {'label': label, 'percentage': percentage, 'compound': compound, 'scores': scores}\n",
    "\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str], target_ns: int = TARGET_NS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üî• VADER SENTIMENT BENCHMARK\")\n",
    "    print(f\"Target: < {target_ns} ns per analysis\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start = time.time_ns()\n",
    "        res = sa.analyze(text)\n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add('vader_analysis', elapsed)\n",
    "        times.append(elapsed)\n",
    "        status = '‚úÖ' if elapsed < target_ns else '‚ùå'\n",
    "        print(f\"[{i}] {format_time_ns(elapsed):25s} {status} | {res['label']:8s} | '{text[:50]}...'\")\n",
    "    total = sum(times)\n",
    "    print(f\"\\nAvg: {format_time_ns(total//len(times))} | Total: {format_time_ns(total)}\")\n",
    "\n",
    "# ==========================\n",
    "# Orchestrator: Plan-and-Solve with feedback\n",
    "# ==========================\n",
    "class PlanAndSolveRAG:\n",
    "    def __init__(self, llm: LLMWrapper, client: weaviate.WeaviateClient, embedder: SentenceTransformer):\n",
    "        self.llm = llm\n",
    "        self.client = client\n",
    "        self.embedder = embedder\n",
    "        self.planner = Planner(llm)\n",
    "        self.solver = StepSolver(llm, client, embedder)\n",
    "        self.aggregator = Aggregator(llm)\n",
    "\n",
    "    def run(self, question: str) -> Dict[str, Any]:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"üîÅ PLAN-AND-SOLVE RAG (with feedback loops)\")\n",
    "        print(f\"Question: {question}\")\n",
    "        plan, plan_time = self.planner.create_plan(question)\n",
    "\n",
    "        step_results = []\n",
    "        for step in plan:\n",
    "            res = self.solver.solve_step(step)\n",
    "            step_results.append(res)\n",
    "\n",
    "        final_answer, final_assessment = self.aggregator.synthesize(step_results, question)\n",
    "\n",
    "        print(\"\\nüí¨ FINAL ANSWER (synthesized):\\n\")\n",
    "        print(final_answer[:1000])\n",
    "        print(\"\\nüìä FINAL ASSESSMENT:\")\n",
    "        print(f\"   Relevance: {final_assessment.relevance}/10\")\n",
    "        print(f\"   Completeness: {final_assessment.completeness}/10\")\n",
    "        print(f\"   Confidence: {final_assessment.confidence}/10\")\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'plan': plan,\n",
    "            'step_results': step_results,\n",
    "            'final_answer': final_answer,\n",
    "            'final_assessment': final_assessment,\n",
    "        }\n",
    "\n",
    "# ==========================\n",
    "# CLI / main\n",
    "# ==========================\n",
    "\n",
    "def main(argv=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--pdf', type=str, default=PDF_PATH)\n",
    "    parser.add_argument('--question', type=str, default=None)\n",
    "    parser.add_argument('--skip_index', action='store_true')\n",
    "    parser.add_argument('--run_sentiment', action='store_true')\n",
    "    args = parser.parse_args(argv)\n",
    "\n",
    "    # Load embedder\n",
    "    embedder = load_embeddings()\n",
    "\n",
    "    # Init weaviate\n",
    "    if not WEAVIATE_API_KEY:\n",
    "        print(\"‚ö†Ô∏è  Set WEAVIATE_API_KEY environment variable.\")\n",
    "        return\n",
    "    client = init_weaviate(WEAVIATE_URL, WEAVIATE_API_KEY)\n",
    "\n",
    "    # Index PDF unless skipped\n",
    "    if not args.skip_index:\n",
    "        print(\"Indexing PDF into Weaviate...\")\n",
    "        text = load_pdf(args.pdf)\n",
    "        chunks = chunk_text(text)\n",
    "        insert_chunks(client, embedder, chunks)\n",
    "\n",
    "    # LLM wrapper\n",
    "    llm = LLMWrapper(groq_api_key=GROQ_API_KEY)\n",
    "\n",
    "    rag = PlanAndSolveRAG(llm, client, embedder)\n",
    "\n",
    "    # Optional VADER benchmark\n",
    "    if args.run_sentiment:\n",
    "        sa = VaderSentimentAnalyzer()\n",
    "        examples = [\n",
    "            \"I absolutely loved the story, it moved me to tears.\",\n",
    "            \"This is the worst experience I've ever had.\",\n",
    "            \"It was okay, not great but not terrible either.\",\n",
    "        ]\n",
    "        run_sentiment_benchmark(sa, examples)\n",
    "\n",
    "    if args.question:\n",
    "        out = rag.run(args.question)\n",
    "        latency_report.pretty_print()\n",
    "    else:\n",
    "        print(\"Interactive mode. Type a question (or 'exit'):\")\n",
    "        while True:\n",
    "            try:\n",
    "                q = input('> ').strip()\n",
    "            except (EOFError, KeyboardInterrupt):\n",
    "                print('\\nExiting.')\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            if q.lower() in ('exit', 'quit'):\n",
    "                break\n",
    "            try:\n",
    "                rag.run(q)\n",
    "                latency_report.pretty_print()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TESTING BLOCK (VADER + VAD) ‚Äî EXACT STYLE LIKE YOUR OLD CODE\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# This matches your previous format exactly:\n",
    "#\n",
    "# # Testing\n",
    "# start = time.time()\n",
    "# sa = VaderSentimentAnalyzer()\n",
    "# init_time = (time.time() - start) * 1000\n",
    "# print(f\"‚è±Ô∏è Init time: {init_time:.3f} ms\n",
    "\")\n",
    "#\n",
    "# examples = [...]\n",
    "# for text in examples:\n",
    "#     ... measure latency in ns\n",
    "#\n",
    "# Added VAD in SAME STYLE.\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def run_sentiment_and_vad_testing():\n",
    "    \"\"\"\n",
    "    Full testing block for VADER sentiment + VAD, written EXACTLY in your old style.\n",
    "    \"\"\"\n",
    "    print(\"\n",
    "================ TESTING (VADER + VAD) ================\n",
    "\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 1) VADER TESTING ‚Äî SAME FORMAT AS YOUR OLD CODE\n",
    "    # --------------------------------------------\n",
    "    start = time.time()\n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    init_ms = (time.time() - start) * 1000\n",
    "    print(f\"‚è±Ô∏è VADER Init time: {init_ms:.3f} ms\n",
    "\")\n",
    "\n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is terrible.\",\n",
    "        \"It was okay, not great.\",\n",
    "        \"Absolutely amazing experience!\"\n",
    "    ]\n",
    "\n",
    "    for text in examples:\n",
    "        t0 = time.perf_counter_ns()\n",
    "        score = sa.analyze(text)\n",
    "        t1 = time.perf_counter_ns()\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Latency: {t1 - t0} ns / {(t1 - t0)/1e6:.3f} ms\n",
    "\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 2) VAD TESTING ‚Äî SAME PATTERN AS VADER\n",
    "    # --------------------------------------------\n",
    "    try:\n",
    "        import webrtcvad\n",
    "        vad = webrtcvad.Vad(3)\n",
    "        print(\"üé§ VAD Test: Using WebRTC VAD (mode=3)\")\n",
    "    except Exception:\n",
    "        print(\"‚ö†Ô∏è VAD TEST SKIPPED ‚Äî install webrtcvad: pip install webrtcvad\")\n",
    "        return\n",
    "\n",
    "    # Create synthetic 1 second silence (16-bit PCM)\n",
    "    sample_rate = 16000\n",
    "    frame_ms = 30\n",
    "    frame_len = int(sample_rate * frame_ms / 1000) * 2  # bytes\n",
    "   print(\"\n",
    "============== TESTING COMPLETE ==============\n",
    "\") (ADDED)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- VADER Sentiment Testing (ns) ---\n",
    "def run_vader_ns_test():\n",
    "import time\n",
    "sa = VaderSentimentAnalyzer()\n",
    "\n",
    "\n",
    "print(\"\n",
    "üß™ VADER Sentiment Testing (ns)\")\n",
    "examples = [\n",
    "\"I love this product!\",\n",
    "\"This is terrible\",\n",
    "\"The movie was okay, not great\",\n",
    "\"An absolutely wonderful experience\"\n",
    "]\n",
    "\n",
    "\n",
    "for text in examples:\n",
    "start = time.perf_counter_ns()\n",
    "res = sa.analyze(text)\n",
    "end = time.perf_counter_ns()\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Score: {res}\")\n",
    "print(f\"Latency: {end - start} ns / {(end-start)/1e6:.3f} ms\n",
    "\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Simple VAD (Voice Activity Detection) using WebRTC ---\n",
    "# Optional: user may not have webrtcvad installed\n",
    "try:\n",
    "import webrtcvad\n",
    "except ImportError:\n",
    "webrtcvad = None\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
