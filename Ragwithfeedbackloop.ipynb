{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "rag_feedback_loops_safe.py\n",
    "\n",
    "RAG with Feedback Loops + full nanosecond latency instrumentation.\n",
    "\n",
    "USAGE:\n",
    "  1) Set environment variables (example):\n",
    "     - Windows PowerShell:\n",
    "         setx GROQ_API_KEY \"gsk_....\"\n",
    "         setx PINECONE_API_KEY \"pcsk_....\"\n",
    "     - macOS / Linux (bash/zsh):\n",
    "         export GROQ_API_KEY=\"gsk_....\"\n",
    "         export PINECONE_API_KEY=\"pcsk_....\"\n",
    "\n",
    "  2) Install dependencies (examples):\n",
    "       pip install pdfplumber langchain-text-splitters langchain-huggingface langchain-groq langchain-pinecone pinecone-client vaderSentiment\n",
    "\n",
    "  3) Run:\n",
    "       python rag_feedback_loops_safe.py\n",
    "\n",
    "NOTES:\n",
    " - This script intentionally loads keys from environment variables for safety.\n",
    " - It instruments every major step using time.time_ns() and prints a latency summary.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Third-party imports (these must be installed)\n",
    "import pdfplumber\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"   # change as needed\n",
    "INDEX_NAME = \"new2\"                       # change as needed if your Pinecone index differs\n",
    "DIM = 384\n",
    "MODEL_NAME = os.getenv(\"GROQ_MODEL_NAME\", \"llama-3.1-8b-instant\")\n",
    "QUALITY_THRESHOLD_DEFAULT = 85.0\n",
    "MAX_CYCLES_DEFAULT = 5\n",
    "\n",
    "# Load secrets from environment (safe)\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    print(\"ERROR: PINECONE_API_KEY not found in environment variables.\")\n",
    "    print(\"Set it before running. Example (PowerShell):\")\n",
    "    print(\"  setx PINECONE_API_KEY \\\"pcsk_...\\\"\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"ERROR: GROQ_API_KEY not found in environment variables.\")\n",
    "    print(\"Set it before running. Example (PowerShell):\")\n",
    "    print(\"  setx GROQ_API_KEY \\\"gsk_...\\\"\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities: timing & reporting\n",
    "# ---------------------------\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    def summary(self) -> Dict[str, Dict[str, Any]]:\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            count = len(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": count,\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // count if count else 0,\n",
    "                \"min_ns\": min(vals) if vals else 0,\n",
    "                \"max_ns\": max(vals) if vals else 0,\n",
    "            }\n",
    "        return out\n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items(), key=lambda x: x[0]):\n",
    "            print(f\"\\nComponent: {comp}\")\n",
    "            print(f\"  Count: {stats['count']}\")\n",
    "            print(f\"  Total: {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"  Avg:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"  Min:   {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"  Max:   {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# ---------------------------\n",
    "# Data classes for feedback\n",
    "# ---------------------------\n",
    "@dataclass\n",
    "class QualityScores:\n",
    "    relevance: int = 0\n",
    "    completeness: int = 0\n",
    "    accuracy: int = 0\n",
    "    coherence: int = 0\n",
    "    specificity: int = 0\n",
    "    @property\n",
    "    def overall(self) -> float:\n",
    "        return (self.relevance + self.completeness + self.accuracy + self.coherence + self.specificity) * 2\n",
    "    def __repr__(self):\n",
    "        return f\"QualityScores(overall={self.overall:.1f}%, rel={self.relevance}, comp={self.completeness}, acc={self.accuracy}, coh={self.coherence}, spec={self.specificity})\"\n",
    "\n",
    "@dataclass\n",
    "class Feedback:\n",
    "    cycle: int\n",
    "    quality_scores: QualityScores\n",
    "    strengths: List[str] = field(default_factory=list)\n",
    "    weaknesses: List[str] = field(default_factory=list)\n",
    "    improvement_suggestions: List[str] = field(default_factory=list)\n",
    "    missing_aspects: List[str] = field(default_factory=list)\n",
    "    retrieval_suggestions: List[str] = field(default_factory=list)\n",
    "    raw_feedback: str = \"\"\n",
    "    timestamp: int = 0\n",
    "    def __repr__(self):\n",
    "        return f\"Feedback(cycle={self.cycle}, overall={self.quality_scores.overall:.1f}%)\"\n",
    "\n",
    "@dataclass\n",
    "class FeedbackCycle:\n",
    "    cycle_number: int\n",
    "    query_refinement: str\n",
    "    retrieved_docs: List = field(default_factory=list)\n",
    "    retrieved_content: str = \"\"\n",
    "    generated_answer: str = \"\"\n",
    "    feedback: Optional[Feedback] = None\n",
    "    elapsed_ns: int = 0\n",
    "    improvements_from_previous: List[str] = field(default_factory=list)\n",
    "    def __repr__(self):\n",
    "        score = self.feedback.quality_scores.overall if self.feedback else 0\n",
    "        return f\"FeedbackCycle({self.cycle_number}, score={score:.1f}%)\"\n",
    "\n",
    "@dataclass\n",
    "class FeedbackHistory:\n",
    "    original_query: str\n",
    "    cycles: List[FeedbackCycle] = field(default_factory=list)\n",
    "    final_answer: str = \"\"\n",
    "    total_elapsed_ns: int = 0\n",
    "    convergence_reached: bool = False\n",
    "    learned_patterns: Dict[str, Any] = field(default_factory=dict)\n",
    "    def add_cycle(self, cycle: FeedbackCycle):\n",
    "        self.cycles.append(cycle)\n",
    "    def get_score_progression(self) -> List[float]:\n",
    "        return [c.feedback.quality_scores.overall for c in self.cycles if c.feedback]\n",
    "    def get_best_cycle(self) -> Optional[FeedbackCycle]:\n",
    "        if not self.cycles:\n",
    "            return None\n",
    "        return max(self.cycles, key=lambda c: c.feedback.quality_scores.overall if c.feedback else 0)\n",
    "    def analyze_convergence(self) -> Dict[str, Any]:\n",
    "        scores = self.get_score_progression()\n",
    "        if len(scores) < 2:\n",
    "            return {\"converged\": False, \"trend\": \"insufficient_data\"}\n",
    "        improvements = [scores[i] - scores[i-1] for i in range(1, len(scores))]\n",
    "        avg_improvement = sum(improvements) / len(improvements)\n",
    "        last_improvement = improvements[-1] if improvements else 0\n",
    "        converged = abs(last_improvement) < 5  # less than 5% change\n",
    "        return {\n",
    "            \"converged\": converged,\n",
    "            \"trend\": \"improving\" if avg_improvement > 0 else \"declining\",\n",
    "            \"avg_improvement\": avg_improvement,\n",
    "            \"last_improvement\": last_improvement,\n",
    "            \"final_score\": scores[-1],\n",
    "            \"score_range\": (min(scores), max(scores))\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# Core RAG with feedback loops\n",
    "# ---------------------------\n",
    "class FeedbackLoopRAG:\n",
    "    def __init__(self, vectorstore, llm, quality_threshold: float = QUALITY_THRESHOLD_DEFAULT, max_cycles: int = MAX_CYCLES_DEFAULT):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.quality_threshold = quality_threshold\n",
    "        self.max_cycles = max_cycles\n",
    "        self.retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "    def _llm_invoke(self, prompt: str, label: str) -> str:\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(f\"llm_{label}\", elapsed)\n",
    "            return response.content if hasattr(response, \"content\") else str(response)\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(f\"llm_{label}_error\", elapsed)\n",
    "            print(f\"LLM error ({label}): {e}\")\n",
    "            traceback.print_exc()\n",
    "            return str(e)\n",
    "\n",
    "    def retrieve_with_refinement(self, query: str, feedback: Optional[Feedback] = None, cycle: int = 1) -> Tuple[List, str]:\n",
    "        print(f\"    üîç Retrieving documents (Cycle {cycle})...\")\n",
    "        refined_query = query\n",
    "        if feedback and feedback.retrieval_suggestions:\n",
    "            refinement_prompt = f\"\"\"Refine this search query based on feedback about what's missing:\n",
    "\n",
    "Original Query: {query}\n",
    "\n",
    "Feedback on what to search for:\n",
    "{chr(10).join(f\"- {s}\" for s in feedback.retrieval_suggestions)}\n",
    "\n",
    "Create an improved search query that addresses these gaps:\n",
    "\n",
    "Refined Query:\"\"\"\n",
    "            refined_query = self._llm_invoke(refinement_prompt, f\"query_refinement_cycle_{cycle}\").strip().split(\"\\n\")[0]\n",
    "            print(f\"    üéØ Refined query: '{refined_query}'\")\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            docs = self.retriever.invoke(refined_query)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(f\"retrieval_cycle_{cycle}\", elapsed)\n",
    "            # build text content robustly\n",
    "            parts = []\n",
    "            for i, d in enumerate(docs):\n",
    "                content = getattr(d, \"page_content\", None) or getattr(d, \"content\", None) or str(d)\n",
    "                parts.append(f\"[Document {i+1}]\\n{content}\")\n",
    "            full_content = \"\\n\\n\".join(parts)\n",
    "            print(f\"    ‚úÖ Retrieved {len(docs)} docs in {format_time_ns(elapsed)}\")\n",
    "            return docs, full_content\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(f\"retrieval_cycle_{cycle}_error\", elapsed)\n",
    "            print(f\"    ‚ùå Retrieval failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return [], \"\"\n",
    "\n",
    "    def generate_answer(self, query: str, context: str, previous_feedback: Optional[Feedback], cycle: int = 1) -> str:\n",
    "        print(f\"    üí° Generating answer (Cycle {cycle})...\")\n",
    "        feedback_context = \"\"\n",
    "        if previous_feedback and cycle > 1:\n",
    "            feedback_context = f\"\"\"\n",
    "PREVIOUS ATTEMPT FEEDBACK:\n",
    "Strengths: {', '.join(previous_feedback.strengths)}\n",
    "Weaknesses: {', '.join(previous_feedback.weaknesses)}\n",
    "\n",
    "IMPROVEMENTS NEEDED:\n",
    "{chr(10).join(f\"- {s}\" for s in previous_feedback.improvement_suggestions)}\n",
    "\n",
    "MISSING ASPECTS:\n",
    "{chr(10).join(f\"- {a}\" for a in previous_feedback.missing_aspects)}\n",
    "\n",
    "ADDRESS THESE ISSUES in your answer.\"\"\"\n",
    "        prompt = f\"\"\"Answer the question using the provided context.{feedback_context}\n",
    "\n",
    "Context:\n",
    "{context[:3000]}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Provide a comprehensive, well-structured answer:\n",
    "\n",
    "Answer:\"\"\"\n",
    "        answer = self._llm_invoke(prompt, f\"generation_cycle_{cycle}\")\n",
    "        print(f\"    ‚úÖ Answer generated ({len(answer)} chars)\")\n",
    "        return answer\n",
    "\n",
    "    def evaluate_answer(self, query: str, answer: str, context: str, cycle: int) -> Feedback:\n",
    "        print(f\"    üìä Evaluating answer quality (Cycle {cycle})...\")\n",
    "        eval_prompt = f\"\"\"You are a critical evaluator. Evaluate this answer comprehensively.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Context Available: {len(context)} characters\n",
    "\n",
    "Provide evaluation in this EXACT format:\n",
    "\n",
    "SCORES (rate each 0-10):\n",
    "Relevance: [0-10]\n",
    "Completeness: [0-10]\n",
    "Accuracy: [0-10]\n",
    "Coherence: [0-10]\n",
    "Specificity: [0-10]\n",
    "\n",
    "STRENGTHS:\n",
    "- [strength 1]\n",
    "- [strength 2]\n",
    "\n",
    "WEAKNESSES:\n",
    "- [weakness 1]\n",
    "- [weakness 2]\n",
    "\n",
    "IMPROVEMENT_SUGGESTIONS:\n",
    "- [specific suggestion 1]\n",
    "- [specific suggestion 2]\n",
    "\n",
    "MISSING_ASPECTS:\n",
    "- [missing aspect 1]\n",
    "- [missing aspect 2]\n",
    "\n",
    "RETRIEVAL_SUGGESTIONS:\n",
    "- [what to search for 1]\n",
    "- [what to search for 2]\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "        eval_text = self._llm_invoke(eval_prompt, f\"evaluation_cycle_{cycle}\")\n",
    "        # parse evaluation\n",
    "        scores = QualityScores()\n",
    "        strengths = []\n",
    "        weaknesses = []\n",
    "        improvements = []\n",
    "        missing = []\n",
    "        retrieval_suggestions = []\n",
    "        current = None\n",
    "        for line in eval_text.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"Relevance:\"):\n",
    "                scores.relevance = self._extract_score(line)\n",
    "            elif line.startswith(\"Completeness:\"):\n",
    "                scores.completeness = self._extract_score(line)\n",
    "            elif line.startswith(\"Accuracy:\"):\n",
    "                scores.accuracy = self._extract_score(line)\n",
    "            elif line.startswith(\"Coherence:\"):\n",
    "                scores.coherence = self._extract_score(line)\n",
    "            elif line.startswith(\"Specificity:\"):\n",
    "                scores.specificity = self._extract_score(line)\n",
    "            elif line.startswith(\"STRENGTHS:\"):\n",
    "                current = \"strengths\"\n",
    "            elif line.startswith(\"WEAKNESSES:\"):\n",
    "                current = \"weaknesses\"\n",
    "            elif line.startswith(\"IMPROVEMENT_SUGGESTIONS:\"):\n",
    "                current = \"improvements\"\n",
    "            elif line.startswith(\"MISSING_ASPECTS:\"):\n",
    "                current = \"missing\"\n",
    "            elif line.startswith(\"RETRIEVAL_SUGGESTIONS:\"):\n",
    "                current = \"retrieval\"\n",
    "            elif line.startswith(\"-\") and current:\n",
    "                item = line[1:].strip()\n",
    "                if current == \"strengths\":\n",
    "                    strengths.append(item)\n",
    "                elif current == \"weaknesses\":\n",
    "                    weaknesses.append(item)\n",
    "                elif current == \"improvements\":\n",
    "                    improvements.append(item)\n",
    "                elif current == \"missing\":\n",
    "                    missing.append(item)\n",
    "                elif current == \"retrieval\":\n",
    "                    retrieval_suggestions.append(item)\n",
    "        feedback = Feedback(\n",
    "            cycle=cycle,\n",
    "            quality_scores=scores,\n",
    "            strengths=strengths,\n",
    "            weaknesses=weaknesses,\n",
    "            improvement_suggestions=improvements,\n",
    "            missing_aspects=missing,\n",
    "            retrieval_suggestions=retrieval_suggestions,\n",
    "            raw_feedback=eval_text,\n",
    "            timestamp=time.time_ns()\n",
    "        )\n",
    "        print(f\"    üìà Quality Score: {scores.overall:.1f}% -- Relevance {scores.relevance}/10, Completeness {scores.completeness}/10\")\n",
    "        return feedback\n",
    "\n",
    "    def _extract_score(self, line: str) -> int:\n",
    "        nums = re.findall(r\"\\d+\", line)\n",
    "        if nums:\n",
    "            s = int(nums[0])\n",
    "            return max(0, min(10, s))\n",
    "        return 0\n",
    "\n",
    "    def execute_feedback_cycle(self, query: str, history: FeedbackHistory, previous_feedback: Optional[Feedback] = None) -> FeedbackCycle:\n",
    "        cycle_num = len(history.cycles) + 1\n",
    "        print(f\"\\n  {'='*58}\")\n",
    "        print(f\"  üîÑ FEEDBACK CYCLE {cycle_num}\")\n",
    "        print(f\"  {'='*58}\")\n",
    "        start = time.time_ns()\n",
    "        cycle = FeedbackCycle(cycle_number=cycle_num, query_refinement=query)\n",
    "        docs, content = self.retrieve_with_refinement(query, previous_feedback, cycle_num)\n",
    "        cycle.retrieved_docs = docs\n",
    "        cycle.retrieved_content = content\n",
    "        if not content:\n",
    "            print(\"    ‚ö†Ô∏è No content retrieved; will generate based on query alone.\")\n",
    "        answer = self.generate_answer(query, content, previous_feedback, cycle_num)\n",
    "        cycle.generated_answer = answer\n",
    "        feedback = self.evaluate_answer(query, answer, content, cycle_num)\n",
    "        cycle.feedback = feedback\n",
    "        # compute improvements vs previous\n",
    "        if previous_feedback:\n",
    "            prev_score = previous_feedback.quality_scores.overall\n",
    "            curr_score = feedback.quality_scores.overall\n",
    "            improvements = []\n",
    "            if curr_score > prev_score:\n",
    "                improvements.append(f\"Overall +{curr_score - prev_score:.1f}%\")\n",
    "            prev_weak = set(previous_feedback.weaknesses)\n",
    "            curr_weak = set(feedback.weaknesses)\n",
    "            addressed = prev_weak - curr_weak\n",
    "            if addressed:\n",
    "                improvements.append(f\"Addressed {len(addressed)} weaknesses\")\n",
    "            cycle.improvements_from_previous = improvements\n",
    "            if improvements:\n",
    "                print(f\"    ‚ú® Improvements: {', '.join(improvements)}\")\n",
    "        cycle.elapsed_ns = time.time_ns() - start\n",
    "        latency_report.add(\"feedback_cycle_total\", cycle.elapsed_ns)\n",
    "        print(f\"  ‚úÖ Cycle {cycle_num} completed in {format_time_ns(cycle.elapsed_ns)}\")\n",
    "        return cycle\n",
    "\n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üîÅ RAG WITH FEEDBACK LOOPS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"‚ùì Question: {question}\")\n",
    "        print(f\"üéØ Quality threshold: {self.quality_threshold}%\")\n",
    "        print(f\"üìä Max cycles: {self.max_cycles}\\n\")\n",
    "        start_overall = time.time_ns()\n",
    "        history = FeedbackHistory(original_query=question)\n",
    "        previous_feedback = None\n",
    "        for i in range(1, self.max_cycles + 1):\n",
    "            cycle = self.execute_feedback_cycle(question, history, previous_feedback)\n",
    "            history.add_cycle(cycle)\n",
    "            score = cycle.feedback.quality_scores.overall\n",
    "            if score >= self.quality_threshold:\n",
    "                print(f\"\\n  üéâ Quality threshold reached: {score:.1f}%\")\n",
    "                history.convergence_reached = True\n",
    "                break\n",
    "            if i >= self.max_cycles:\n",
    "                print(f\"\\n  ‚ö†Ô∏è Max cycles reached ({self.max_cycles}). Final quality: {score:.1f}%\")\n",
    "                break\n",
    "            previous_feedback = cycle.feedback\n",
    "            print(f\"\\n  üîÑ Preparing next cycle (score {score:.1f}% < {self.quality_threshold}%)\")\n",
    "        best = history.get_best_cycle()\n",
    "        history.final_answer = best.generated_answer if best else \"\"\n",
    "        history.total_elapsed_ns = time.time_ns() - start_overall\n",
    "        latency_report.add(\"feedback_query_total\", history.total_elapsed_ns)\n",
    "        conv = history.analyze_convergence()\n",
    "        history.learned_patterns = conv\n",
    "        self._print_summary(history, conv)\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": history.final_answer,\n",
    "            \"history\": history,\n",
    "            \"num_cycles\": len(history.cycles),\n",
    "            \"converged\": history.convergence_reached,\n",
    "            \"final_quality\": best.feedback.quality_scores.overall if best else 0,\n",
    "            \"score_progression\": history.get_score_progression(),\n",
    "            \"total_elapsed_ns\": history.total_elapsed_ns\n",
    "        }\n",
    "\n",
    "    def _print_summary(self, history: FeedbackHistory, conv: Dict[str, Any]):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FEEDBACK LOOP SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Cycles executed: {len(history.cycles)}\")\n",
    "        print(f\"Converged: {history.convergence_reached}\")\n",
    "        print(f\"Total time: {format_time_ns(history.total_elapsed_ns)}\")\n",
    "        print(\"\\nQuality progression:\")\n",
    "        for idx, c in enumerate(history.cycles, 1):\n",
    "            score = c.feedback.quality_scores.overall\n",
    "            marker = \"‚úÖ\" if score >= self.quality_threshold else \"üìä\"\n",
    "            print(f\"  Cycle {idx}: {score:.1f}% {marker}\")\n",
    "        print(\"\\nConvergence analysis:\")\n",
    "        print(f\"  Trend: {conv.get('trend')}\")\n",
    "        if conv.get(\"avg_improvement\") is not None:\n",
    "            print(f\"  Avg improvement: {conv['avg_improvement']:.1f}%\")\n",
    "        if conv.get(\"final_score\") is not None:\n",
    "            print(f\"  Final score: {conv['final_score']:.1f}%\")\n",
    "        if conv.get(\"score_range\"):\n",
    "            lo, hi = conv[\"score_range\"]\n",
    "            print(f\"  Score range: {lo:.1f}% - {hi:.1f}%\")\n",
    "        best = history.get_best_cycle()\n",
    "        if best:\n",
    "            print(f\"\\nBest cycle: {best.cycle_number} with score {best.feedback.quality_scores.overall:.1f}%\")\n",
    "            if best.feedback.strengths:\n",
    "                print(f\"  Strength examples: {best.feedback.strengths[:3]}\")\n",
    "\n",
    "# ---------------------------\n",
    "# PDF processing, chunking, vectorstore init\n",
    "# ---------------------------\n",
    "def load_and_process_pdf(path: str) -> str:\n",
    "    print(f\"üìÑ Loading PDF: {path} ...\")\n",
    "    start = time.time_ns()\n",
    "    text_parts = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for p in pdf.pages:\n",
    "            t = p.extract_text() or \"\"\n",
    "            text_parts.append(t)\n",
    "            latency_report.add(\"pdf_page_extract\", 0)  # placeholder if per-page is desired\n",
    "    text = \"\\n\".join(text_parts)\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"pdf_load\", elapsed)\n",
    "    print(f\"‚úÖ Loaded {len(text)} characters in {format_time_ns(elapsed)}\")\n",
    "    return text\n",
    "\n",
    "def chunk_text_for_indexing(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    print(\"üî™ Chunking text...\")\n",
    "    start = time.time_ns()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"chunking\", elapsed)\n",
    "    print(f\"üìÑ Created {len(chunks)} chunks in {format_time_ns(elapsed)}\")\n",
    "    return chunks\n",
    "\n",
    "def init_vectorstore(chunks: List[str], api_key: str, index_name: str = INDEX_NAME) -> PineconeVectorStore:\n",
    "    print(\"üîß Initializing Pinecone vectorstore...\")\n",
    "    start = time.time_ns()\n",
    "    pc = Pinecone(api_key=api_key)\n",
    "    existing = [idx.name for idx in pc.list_indexes()]\n",
    "    print(f\"  Found indexes: {existing}\")\n",
    "    if index_name not in existing:\n",
    "        print(f\"  Index '{index_name}' not found. Creating it now...\")\n",
    "        t0 = time.time_ns()\n",
    "        pc.create_index(name=index_name, dimension=DIM, metric=\"cosine\", spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "        latency_report.add(\"pinecone_create_index\", time.time_ns() - t0)\n",
    "        # small sleep to allow index creation to propagate\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        print(f\"  Using existing index '{index_name}'\")\n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    t1 = time.time_ns()\n",
    "    vectorstore = PineconeVectorStore.from_texts(texts=chunks, embedding=embed_model, index_name=index_name, namespace=\"feedback_loop\", metadatas=[{\"chunk_id\": i} for i in range(len(chunks))])\n",
    "    latency_report.add(\"pinecone_upsert\", time.time_ns() - t1)\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"vectorstore_init\", elapsed)\n",
    "    print(f\"‚úÖ Vectorstore initialized in {format_time_ns(elapsed)}\")\n",
    "    return vectorstore\n",
    "\n",
    "# ---------------------------\n",
    "# Sentiment analyzer (optional benchmarking)\n",
    "# ---------------------------\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        s = self.analyzer.polarity_scores(text)\n",
    "        c = s[\"compound\"]\n",
    "        if c >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "        elif c <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "        return {\"label\": label, \"scores\": s, \"compound\": c}\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üöÄ RAG WITH FEEDBACK LOOPS (SAFE MODE)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Phase 0: load PDF & build vectorstore\n",
    "    try:\n",
    "        text = load_and_process_pdf(PDF_PATH)\n",
    "        chunks = chunk_text_for_indexing(text)\n",
    "        vectorstore = init_vectorstore(chunks, PINECONE_API_KEY, INDEX_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"Setup error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Init LLM (ChatGroq)\n",
    "    print(\"\\nü§ñ Initializing LLM (ChatGroq)...\")\n",
    "    t0 = time.time_ns()\n",
    "    try:\n",
    "        llm = ChatGroq(model_name=MODEL_NAME, temperature=0, groq_api_key=GROQ_API_KEY)\n",
    "    except Exception as e:\n",
    "        print(f\"LLM init error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    latency_report.add(\"llm_init\", time.time_ns() - t0)\n",
    "    print(f\"‚úÖ LLM initialized in {format_time_ns(latency_report.summary().get('llm_init', {}).get('total_ns', 0))}\")\n",
    "\n",
    "    # Create FeedbackLoopRAG\n",
    "    feedback_rag = FeedbackLoopRAG(vectorstore, llm, quality_threshold=80.0, max_cycles=4)\n",
    "    print(\"\\n‚úÖ Feedback Loop RAG ready\\n\")\n",
    "\n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Describe the main character's journey.\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        try:\n",
    "            res = feedback_rag.query(q)\n",
    "            results.append(res)\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"FINAL ANSWER (truncated):\")\n",
    "            print(res[\"answer\"][:800] + (\"...\" if len(res[\"answer\"]) > 800 else \"\"))\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during query '{q}': {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # Print final latency and statistics\n",
    "    latency_report.pretty_print()\n",
    "\n",
    "    # Feedback loop aggregated stats\n",
    "    total_cycles = sum(r[\"num_cycles\"] for r in results)\n",
    "    converged = sum(1 for r in results if r[\"converged\"])\n",
    "    avg_quality = sum(r[\"final_quality\"] for r in results) / len(results) if results else 0\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEEDBACK LOOP AGGREGATE STATS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total queries: {len(results)}\")\n",
    "    print(f\"Total cycles: {total_cycles}\")\n",
    "    print(f\"Average cycles per query: {total_cycles / len(results):.1f}\" if results else \"N/A\")\n",
    "    print(f\"Converged queries: {converged}/{len(results)}\")\n",
    "    print(f\"Average final quality: {avg_quality:.1f}%\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
