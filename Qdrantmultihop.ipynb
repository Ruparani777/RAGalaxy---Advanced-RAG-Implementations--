{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a93d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "qdrant_multihop_rag_fast.py\n",
    "\n",
    "Optimized rewrite of your Multi-Hop RAG pipeline with:\n",
    "- optional ultra-fast in-memory NumPy search (\"mem\" mode)\n",
    "- controlled Qdrant fallback (\"qdrant\" mode)\n",
    "- caching of embeddings to .npy to avoid recompute\n",
    "- reduced logging in hot paths\n",
    "- batched encoding and reduced Python overhead\n",
    "- preserved multi-hop/coT flow and latency instrumentation\n",
    "\n",
    "Notes:\n",
    "- This script still cannot make LLM or embedding calls complete in 3,000 ns.\n",
    "- For true microsecond latency you'd need tiny on-device models and compiled code (C/C++, Rust) running on specialized hardware.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Optional imports for Qdrant compatibility\n",
    "try:\n",
    "    from qdrant_client import QdrantClient\n",
    "    from qdrant_client.http.models import VectorParams, Distance, PointStruct\n",
    "    QDRANT_AVAILABLE = True\n",
    "except Exception:\n",
    "    QDRANT_AVAILABLE = False\n",
    "\n",
    "# Minimal LLM shim (keeps your llm.invoke(prompt).content interface)\n",
    "# Replace ChatGroq with your real client in production.\n",
    "class DummyLLM:\n",
    "    def __init__(self, latency_ms: float = 20.0):\n",
    "        self.latency_ms = latency_ms\n",
    "\n",
    "    def invoke(self, prompt: str):\n",
    "        # emulate response object with .content\n",
    "        time.sleep(self.latency_ms / 1000.0)\n",
    "        class R:\n",
    "            def __init__(self, c): self.content = c\n",
    "        # simple heuristic response — replace with real LLM client\n",
    "        return R(\"COG: quick thought\\nDECISION: RETRIEVE\" if \"Decide\" in prompt else\n",
    "                 \"COG: short\\nANSWER: dummy answer\\nHOPS: []\")\n",
    "\n",
    "# ---------------------------\n",
    "# Latency utilities\n",
    "# ---------------------------\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    def add(self, k: str, ns: int):\n",
    "        self.store[k].append(ns)\n",
    "    def summary(self):\n",
    "        out = {}\n",
    "        for k, vals in self.store.items():\n",
    "            s = sum(vals)\n",
    "            out[k] = {\"count\": len(vals), \"total_ns\": s,\n",
    "                      \"avg_ns\": s // len(vals) if vals else 0,\n",
    "                      \"min_ns\": min(vals) if vals else 0,\n",
    "                      \"max_ns\": max(vals) if vals else 0}\n",
    "        return out\n",
    "    def pretty_print(self):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LATENCY SUMMARY\")\n",
    "        for k, v in sorted(self.summary().items()):\n",
    "            print(f\"{k:25s} count={v['count']:3d} avg={format_time_ns(v['avg_ns'])} total={format_time_ns(v['total_ns'])}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "latency = LatencyReport()\n",
    "\n",
    "# ---------------------------\n",
    "# Fast text chunker (lightweight)\n",
    "# ---------------------------\n",
    "def chunk_text_fast(text: str, chunk_size: int=1000, overlap: int=100) -> List[str]:\n",
    "    # Very cheap splitter: slide window on whitespace boundaries\n",
    "    if not text:\n",
    "        return []\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(words)\n",
    "    while i < n:\n",
    "        j = i + chunk_size\n",
    "        chunk = \" \".join(words[i: min(j, n)])\n",
    "        chunks.append(chunk)\n",
    "        i = j - overlap\n",
    "        if i <= 0:\n",
    "            i = j\n",
    "    return chunks\n",
    "\n",
    "# ---------------------------\n",
    "# Embedding utils with caching\n",
    "# ---------------------------\n",
    "def load_or_compute_embeddings(embedder: SentenceTransformer, chunks: List[str], cache_path: str):\n",
    "    \"\"\"\n",
    "    Compute embeddings once and cache to .npy. Returns numpy array shape (N, D).\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_path):\n",
    "        start = time.time_ns()\n",
    "        mat = np.load(cache_path)\n",
    "        latency.add(\"emb_cache_load\", time.time_ns() - start)\n",
    "        return mat\n",
    "    # batch encode for memory efficiency\n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False, batch_size=128)\n",
    "    encode_ns = time.time_ns() - start\n",
    "    latency.add(\"embed_encode_full\", encode_ns)\n",
    "    mat = np.asarray(vectors, dtype=np.float32)\n",
    "    # persist\n",
    "    np.save(cache_path, mat)\n",
    "    latency.add(\"emb_cache_save\", 0)\n",
    "    return mat\n",
    "\n",
    "# ---------------------------\n",
    "# In-memory search (very fast)\n",
    "# ---------------------------\n",
    "class MemANN:\n",
    "    def __init__(self, vectors: np.ndarray):\n",
    "        # normalize once for cosine similarity\n",
    "        self.v = vectors.astype(np.float32)\n",
    "        norms = np.linalg.norm(self.v, axis=1, keepdims=True)\n",
    "        # avoid division by zero\n",
    "        norms[norms==0] = 1.0\n",
    "        self.vn = self.v / norms\n",
    "\n",
    "    def query(self, qvec: np.ndarray, top_k: int = 4) -> Tuple[List[int], List[float], int]:\n",
    "        # qvec shape (D,)\n",
    "        t0 = time.time_ns()\n",
    "        qn = qvec.astype(np.float32)\n",
    "        qn = qn / (np.linalg.norm(qn) + 1e-12)\n",
    "        # compute dot product fast\n",
    "        sims = self.vn.dot(qn)\n",
    "        if top_k >= sims.shape[0]:\n",
    "            idx = np.argsort(-sims)\n",
    "        else:\n",
    "            # partial selection is faster for large N\n",
    "            idx = np.argpartition(-sims, top_k)[:top_k]\n",
    "            idx = idx[np.argsort(-sims[idx])]\n",
    "        top_scores = sims[idx].tolist()\n",
    "        elapsed = time.time_ns() - t0\n",
    "        latency.add(\"memann_query\", elapsed)\n",
    "        return idx.tolist(), top_scores, elapsed\n",
    "\n",
    "# ---------------------------\n",
    "# Qdrant helper (kept minimal)\n",
    "# ---------------------------\n",
    "def init_qdrant_minimal(collection: str, dim: int, in_memory: bool = True) -> QdrantClient:\n",
    "    if not QDRANT_AVAILABLE:\n",
    "        raise RuntimeError(\"Qdrant client not installed\")\n",
    "    q = QdrantClient(\":memory:\") if in_memory else QdrantClient()\n",
    "    if q.collection_exists(collection):\n",
    "        q.delete_collection(collection)\n",
    "    q.create_collection(collection_name=collection, vectors_config=VectorParams(size=dim, distance=Distance.COSINE))\n",
    "    return q\n",
    "\n",
    "# ---------------------------\n",
    "# Search wrapper supporting mem/qdrant\n",
    "# ---------------------------\n",
    "class FastSearcher:\n",
    "    def __init__(self, mode: str, vectors: np.ndarray = None, qdrant_client: QdrantClient = None, chunks: List[str] = None):\n",
    "        self.mode = mode\n",
    "        self.vectors = vectors  # numpy matrix\n",
    "        self.qdrant = qdrant_client\n",
    "        self.chunks = chunks or []\n",
    "        if mode == \"mem\":\n",
    "            assert vectors is not None\n",
    "            self.ann = MemANN(vectors)\n",
    "        elif mode == \"qdrant\":\n",
    "            assert qdrant_client is not None\n",
    "\n",
    "    def search(self, embedder: SentenceTransformer, query: str, top_k: int = 4) -> Tuple[List[str], int]:\n",
    "        # embed query (inline)\n",
    "        t0 = time.time_ns()\n",
    "        qvec = embedder.encode([query], show_progress_bar=False)[0]\n",
    "        embed_ns = time.time_ns() - t0\n",
    "        latency.add(\"query_encode\", embed_ns)\n",
    "        if self.mode == \"mem\":\n",
    "            idxs, scores, q_ns = self.ann.query(np.asarray(qvec, dtype=np.float32), top_k=top_k)\n",
    "            hits = [self.chunks[i] for i in idxs]\n",
    "            return hits, q_ns + embed_ns\n",
    "        else:\n",
    "            # minimal qdrant query using client.query_points\n",
    "            t1 = time.time_ns()\n",
    "            resp = self.qdrant.query_points(collection_name=\"rag_collection\", query=np.asarray(qvec).tolist(), limit=top_k)\n",
    "            q_ns = time.time_ns() - t1\n",
    "            hits = [p.payload.get(\"text\", \"\") for p in resp.points]\n",
    "            latency.add(\"qdrant_query\", q_ns)\n",
    "            return hits, embed_ns + q_ns\n",
    "\n",
    "# ---------------------------\n",
    "# MultiHopRAG (unchanged structure, micro-optimized)\n",
    "# ---------------------------\n",
    "class MultiHopRAG:\n",
    "    def __init__(self, llm, searcher: FastSearcher, embedder: SentenceTransformer):\n",
    "        self.llm = llm\n",
    "        self.searcher = searcher\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def decide_and_chain(self, question: str) -> Tuple[bool, str, int]:\n",
    "        prompt = f\"You are an assistant that reasons step-by-step (chain-of-thought).\\nDecide whether to retrieve document context to answer the question. Show brief chain-of-thought and then the decision.\\nQuestion: {question}\\nProvide:\\nCOG:\\nDECISION:\\n\"\n",
    "        t0 = time.time_ns()\n",
    "        r = self.llm.invoke(prompt)\n",
    "        t_ns = time.time_ns() - t0\n",
    "        latency.add(\"llm_decide\", t_ns)\n",
    "        text = r.content if hasattr(r, \"content\") else str(r)\n",
    "        return ('RETRIEVE' in text.upper()), text, t_ns\n",
    "\n",
    "    def generate_and_hop(self, question: str, context: str = \"\") -> Tuple[str, List[str], int]:\n",
    "        if context:\n",
    "            prompt = f\"Context:\\n{context}\\nQuestion: {question}\\nProvide COG, ANSWER, HOPS as JSON array.\"\n",
    "        else:\n",
    "            prompt = f\"Question: {question}\\nProvide COG, ANSWER, HOPS as JSON array.\"\n",
    "        t0 = time.time_ns()\n",
    "        r = self.llm.invoke(prompt)\n",
    "        t_ns = time.time_ns() - t0\n",
    "        latency.add(\"llm_generate\", t_ns)\n",
    "        text = r.content if hasattr(r, \"content\") else str(r)\n",
    "\n",
    "        # Lightweight parse: attempt JSON extraction for HOPS\n",
    "        cog = \"\"\n",
    "        answer = \"\"\n",
    "        hops = []\n",
    "        lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "        for ln in lines:\n",
    "            if ln.upper().startswith(\"COG:\"):\n",
    "                cog = ln.split(\":\",1)[1].strip()\n",
    "            elif ln.upper().startswith(\"ANSWER:\"):\n",
    "                answer = ln.split(\":\",1)[1].strip()\n",
    "            elif ln.upper().startswith(\"HOPS:\"):\n",
    "                # try JSON parse\n",
    "                try:\n",
    "                    hops = json.loads(ln.split(\":\",1)[1].strip())\n",
    "                except Exception:\n",
    "                    # crude split fallback\n",
    "                    part = ln.split(\":\",1)[1].strip()\n",
    "                    part = part.strip(\"[]\")\n",
    "                    if part:\n",
    "                        hops = [p.strip().strip('\"').strip(\"'\") for p in part.split(\",\") if p.strip()]\n",
    "        return answer, hops, t_ns\n",
    "\n",
    "    def self_critique(self, question: str, answer: str, context: str = \"\") -> Tuple[str, bool, int]:\n",
    "        prompt = f\"Given question and answer indicate NEED_MORE or ENOUGH. Question: {question}\\nAnswer: {answer}\\nContext: {'provided' if context else 'none'}\"\n",
    "        t0 = time.time_ns()\n",
    "        r = self.llm.invoke(prompt)\n",
    "        t_ns = time.time_ns() - t0\n",
    "        latency.add(\"llm_critique\", t_ns)\n",
    "        text = r.content if hasattr(r, \"content\") else str(r)\n",
    "        return text, ('NEED_MORE' in text.upper()), t_ns\n",
    "\n",
    "    def query(self, question: str, max_hops: int = 3, top_k: int = 4) -> Dict[str, Any]:\n",
    "        t0 = time.time_ns()\n",
    "        needs_retrieval, dec_text, dec_ns = self.decide_and_chain(question)\n",
    "\n",
    "        combined_context = []\n",
    "        retrieved_snippets = []\n",
    "\n",
    "        if needs_retrieval:\n",
    "            hits, rt_ns = self.searcher.search(self.embedder, question, top_k=top_k)\n",
    "            combined_context.append(\"\\n\\n\".join(hits))\n",
    "            retrieved_snippets.extend(hits)\n",
    "\n",
    "        answer, hops, gen_ns = self.generate_and_hop(question, \"\\n\\n\".join(combined_context))\n",
    "        hops_done = 0\n",
    "        per_hop = []\n",
    "\n",
    "        while hops and hops_done < max_hops:\n",
    "            hop_start = time.time_ns()\n",
    "            next_hops = []\n",
    "            for hq in hops:\n",
    "                hits, rt_ns = self.searcher.search(self.embedder, hq, top_k=top_k)\n",
    "                if hits:\n",
    "                    combined_context.append(\"\\n\\n\".join(hits))\n",
    "                    retrieved_snippets.extend(hits)\n",
    "            merged_ctx = \"\\n\\n\".join(combined_context)\n",
    "            answer2, next_hops, synth_ns = self.generate_and_hop(f\"Refine: {question}\", merged_ctx)\n",
    "            if answer2.strip():\n",
    "                answer = answer2\n",
    "            hop_elapsed = time.time_ns() - hop_start\n",
    "            per_hop.append(hop_elapsed)\n",
    "            latency.add(\"multihop_hop_total\", hop_elapsed)\n",
    "            hops = next_hops\n",
    "            hops_done += 1\n",
    "\n",
    "        critique_text, need_more, crt_ns = self.self_critique(question, answer, \"\\n\\n\".join(combined_context))\n",
    "        total_ns = time.time_ns() - t0\n",
    "        latency.add(\"multihop_total\", total_ns)\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"hops_done\": hops_done,\n",
    "            \"retrieved_count\": len(retrieved_snippets),\n",
    "            \"retrieved\": retrieved_snippets,\n",
    "            \"critique\": critique_text,\n",
    "            \"per_hop_ns\": per_hop,\n",
    "            \"total_ns\": total_ns\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# Main optimized pipeline\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # CONFIG - tweak these for experimentation\n",
    "    PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "    CACHE_EMB_NPY = \"embeddings_cache.npy\"\n",
    "    MODE = os.getenv(\"FAST_MODE\", \"mem\")   # \"mem\" or \"qdrant\"\n",
    "    MODEL_EMBED = os.getenv(\"EMBED_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    DIM = 384\n",
    "\n",
    "    print(\"FAST RAG START (mode=%s)\" % MODE)\n",
    "    pipeline_start = time.time_ns()\n",
    "\n",
    "    # PHASE 1: Load text quickly or assume pre-supplied text\n",
    "    # If the PDF is large, user should pre-extract text and provide path to .txt to avoid repeated PDF parsing.\n",
    "    text = \"\"\n",
    "    if os.path.exists(PDF_PATH):\n",
    "        # lazy fast read: attempt to read text file first (if user provided pre-extracted)\n",
    "        if PDF_PATH.lower().endswith(\".txt\"):\n",
    "            with open(PDF_PATH, \"r\", encoding=\"utf8\") as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            # Attempt fast PDF extraction via pdfplumber only once (keeps compatibility)\n",
    "            try:\n",
    "                import pdfplumber\n",
    "                t0 = time.time_ns()\n",
    "                with pdfplumber.open(PDF_PATH) as pdf:\n",
    "                    pages = pdf.pages\n",
    "                    parts = []\n",
    "                    for p in pages:\n",
    "                        parts.append(p.extract_text() or \"\")\n",
    "                text = \"\\n\".join(parts)\n",
    "                latency.add(\"pdf_load\", time.time_ns() - t0)\n",
    "            except Exception:\n",
    "                text = \"\"\n",
    "    else:\n",
    "        # no PDF; assume user will provide text via other means\n",
    "        text = \"\"\n",
    "\n",
    "    # Chunk\n",
    "    chunks = chunk_text_fast(text, chunk_size=200, overlap=20) if text else [\"(empty document)\"]\n",
    "    # Embedding model load (once)\n",
    "    t0 = time.time_ns()\n",
    "    embedder = SentenceTransformer(MODEL_EMBED)\n",
    "    latency.add(\"embedder_load\", time.time_ns() - t0)\n",
    "\n",
    "    # Compute / load embeddings\n",
    "    vectors = load_or_compute_embeddings(embedder, chunks, CACHE_EMB_NPY)\n",
    "\n",
    "    # Init searcher\n",
    "    if MODE == \"mem\":\n",
    "        searcher = FastSearcher(mode=\"mem\", vectors=vectors, chunks=chunks)\n",
    "    else:\n",
    "        if not QDRANT_AVAILABLE:\n",
    "            raise RuntimeError(\"Qdrant not available; install qdrant-client or switch mode=mem\")\n",
    "        qdrant = init_qdrant_minimal(\"rag_collection\", vectors.shape[1], in_memory=True)\n",
    "        # create minimal point structs and upsert in batch (fast)\n",
    "        ids = list(range(vectors.shape[0]))\n",
    "        points = [PointStruct(id=i, vector=vectors[i].tolist(), payload={\"text\": chunks[i]}) for i in ids]\n",
    "        t0 = time.time_ns()\n",
    "        qdrant.upsert(collection_name=\"rag_collection\", points=points)\n",
    "        latency.add(\"qdrant_upsert_all\", time.time_ns() - t0)\n",
    "        searcher = FastSearcher(mode=\"qdrant\", qdrant_client=qdrant, chunks=chunks)\n",
    "\n",
    "    # LLM: replace DummyLLM with real ChatGroq (or other) that returns .content quickly\n",
    "    llm = DummyLLM(latency_ms=10.0)  # emulate a 10ms LLM call for benching\n",
    "    rag = MultiHopRAG(llm, searcher, embedder)\n",
    "\n",
    "    # Run a few queries\n",
    "    queries = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Summarize the key events in the document.\",\n",
    "        \"What is the capital of France?\"\n",
    "    ]\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        r = rag.query(q, max_hops=2, top_k=4)\n",
    "        results.append(r)\n",
    "        print(f\"Q: {q[:60]} -> total {format_time_ns(r['total_ns'])}\")\n",
    "\n",
    "    # Final summary\n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency.add(\"pipeline_total\", pipeline_total)\n",
    "    print(f\"Pipeline total: {format_time_ns(pipeline_total)}\")\n",
    "    latency.pretty_print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
