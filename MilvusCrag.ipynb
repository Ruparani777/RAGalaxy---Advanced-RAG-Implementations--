{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    utility,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION_NAME = \"CorrectiveRAG_Documents\"\n",
    "DIM = 384\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000\n",
    "RELEVANCE_THRESHOLD = 0.5  # Threshold for document relevance\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"‚ùå ERROR: Set GROQ_API_KEY environment variable!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------------------\n",
    "# LATENCY UTILITIES\n",
    "# ---------------------------\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "def timer_ns(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"‚è±Ô∏è  {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    \n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    \n",
    "    def summary(self) -> Dict:\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // len(vals) if vals else 0,\n",
    "                \"min_ns\": min(vals) if vals else 0,\n",
    "                \"max_ns\": max(vals) if vals else 0\n",
    "            }\n",
    "        return out\n",
    "    \n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY (nanoseconds)\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items(), key=lambda p: p[0]):\n",
    "            print(f\"\\nüìä Component: {comp}\")\n",
    "            print(f\"   Count:     {stats['count']}\")\n",
    "            print(f\"   Total:     {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"   Average:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"   Min:       {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"   Max:       {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# ---------------------------\n",
    "# PDF Loader\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def load_pdf(path: str) -> str:\n",
    "    print(f\"üìÑ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, p in enumerate(pdf.pages):\n",
    "            start_ns = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start_ns\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            text += t + \"\\n\"\n",
    "    print(f\"‚úÖ Loaded PDF: {len(text)} characters from {len(pdf.pages)} pages\")\n",
    "    return text\n",
    "\n",
    "# ---------------------------\n",
    "# Chunker\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    print(\"‚úÇÔ∏è  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# ---------------------------\n",
    "# Embeddings loader\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def load_embeddings(model_name: str = EMBED_MODEL) -> SentenceTransformer:\n",
    "    print(f\"üî¢ Loading embeddings model: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    print(\"‚úÖ Embeddings model loaded\")\n",
    "    return embedder\n",
    "\n",
    "# ---------------------------\n",
    "# Milvus init\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def init_milvus(host: str, port: str, collection_name: str = COLLECTION_NAME, dim: int = DIM) -> Collection:\n",
    "    print(f\"üóÉÔ∏è  Initializing Milvus connection to {host}:{port}\")\n",
    "    connections.connect(host=host, port=port)\n",
    "    \n",
    "    try:\n",
    "        if utility.has_collection(collection_name):\n",
    "            utility.drop_collection(collection_name)\n",
    "            print(f\"üóëÔ∏è  Deleted existing collection '{collection_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Collection check/delete: {e}\")\n",
    "    \n",
    "    chunk_id_field = FieldSchema(name=\"chunk_id\", dtype=DataType.INT64, is_primary=True, auto_id=True)\n",
    "    text_field = FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535)\n",
    "    source_field = FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=1024)\n",
    "    embedding_field = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    \n",
    "    schema = CollectionSchema(fields=[chunk_id_field, text_field, source_field, embedding_field],\n",
    "                              description=\"Corrective RAG document chunks\")\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "    \n",
    "    index_params = {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 1024}}\n",
    "    try:\n",
    "        collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  create_index: {e}\")\n",
    "    \n",
    "    try:\n",
    "        collection.load()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  load: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Milvus collection '{collection_name}' ready\")\n",
    "    return collection\n",
    "\n",
    "# ---------------------------\n",
    "# Insert chunks\n",
    "# ---------------------------\n",
    "@timer_ns\n",
    "def insert_chunks(collection: Collection, embedder: SentenceTransformer, chunks: List[str]) -> None:\n",
    "    print(f\"‚¨ÜÔ∏è  Inserting {len(chunks)} chunks into Milvus...\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_encode_batch\", encode_time)\n",
    "    print(f\"   ‚úÖ Encoded in {format_time_ns(encode_time)}\")\n",
    "    \n",
    "    texts = chunks\n",
    "    sources = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "    embeddings = [v.tolist() if hasattr(v, \"tolist\") else list(v) for v in vectors]\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    collection.insert([texts, sources, embeddings])\n",
    "    insert_time = time.time_ns() - start\n",
    "    latency_report.add(\"milvus_insert\", insert_time)\n",
    "    print(f\"   ‚úÖ Inserted {len(chunks)} vectors in {format_time_ns(insert_time)}\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    collection.flush()\n",
    "    flush_time = time.time_ns() - start\n",
    "    latency_report.add(\"milvus_flush\", flush_time)\n",
    "    print(f\"   ‚úÖ Flushed collection ({format_time_ns(flush_time)})\")\n",
    "\n",
    "# ---------------------------\n",
    "# Search with scores\n",
    "# ---------------------------\n",
    "def search_milvus_with_scores(collection: Collection, embedder: SentenceTransformer, query: str, limit: int = 5) -> Tuple[List[Dict], int]:\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"query_embedding\", encode_time)\n",
    "    \n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    start = time.time_ns()\n",
    "    try:\n",
    "        results = collection.search(\n",
    "            data=[qvec.tolist()], \n",
    "            anns_field=\"embedding\", \n",
    "            param=search_params, \n",
    "            limit=limit,\n",
    "            output_fields=[\"text\", \"source\", \"chunk_id\"]\n",
    "        )\n",
    "        search_time = time.time_ns() - start\n",
    "        latency_report.add(\"milvus_search\", search_time)\n",
    "        \n",
    "        hits = []\n",
    "        for hit in results[0]:\n",
    "            try:\n",
    "                ent = getattr(hit, \"entity\", None) or getattr(hit, \"_fields\", None) or {}\n",
    "                distance = getattr(hit, \"distance\", 1.0)\n",
    "                \n",
    "                if isinstance(ent, dict):\n",
    "                    txt = ent.get(\"text\", \"\")\n",
    "                else:\n",
    "                    txt = str(hit)\n",
    "                \n",
    "                hits.append({\n",
    "                    \"text\": txt,\n",
    "                    \"distance\": distance,\n",
    "                    \"source\": ent.get(\"source\", \"\") if isinstance(ent, dict) else \"\"\n",
    "                })\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        search_time = time.time_ns() - start\n",
    "        latency_report.add(\"milvus_search_error\", search_time)\n",
    "        print(f\"‚ö†Ô∏è Milvus search failed: {e}\")\n",
    "        hits = []\n",
    "    \n",
    "    total_time = encode_time + search_time\n",
    "    return hits, total_time\n",
    "\n",
    "# ---------------------------\n",
    "# Corrective RAG System\n",
    "# ---------------------------\n",
    "class CorrectiveRAG:\n",
    "    def __init__(self, llm, collection: Collection, embedder: SentenceTransformer, \n",
    "                 relevance_threshold: float = RELEVANCE_THRESHOLD):\n",
    "        self.llm = llm\n",
    "        self.collection = collection\n",
    "        self.embedder = embedder\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        \n",
    "    def _llm_invoke_timed(self, prompt: str, label: str) -> Tuple[str, int]:\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label, elapsed)\n",
    "            content = response.content if hasattr(response, \"content\") else str(response)\n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label + \"_error\", elapsed)\n",
    "            print(f\"LLM invoke for {label} failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return str(e), elapsed\n",
    "    \n",
    "    def retrieve_documents(self, query: str, k: int = 5) -> Tuple[List[Dict], int]:\n",
    "        \"\"\"Initial document retrieval with relevance scores\"\"\"\n",
    "        hits, elapsed = search_milvus_with_scores(self.collection, self.embedder, query, k)\n",
    "        print(f\"   ‚úÖ Retrieved {len(hits)} documents in {format_time_ns(elapsed)}\")\n",
    "        return hits, elapsed\n",
    "    \n",
    "    def evaluate_relevance(self, query: str, documents: List[Dict]) -> Tuple[Dict[str, List[Dict]], int]:\n",
    "        \"\"\"\n",
    "        Evaluate relevance of retrieved documents using LLM\n",
    "        Returns: {\n",
    "            'correct': [highly relevant docs],\n",
    "            'ambiguous': [partially relevant docs],\n",
    "            'incorrect': [irrelevant docs]\n",
    "        }\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return {'correct': [], 'ambiguous': [], 'incorrect': []}, 0\n",
    "        \n",
    "        # Build evaluation prompt\n",
    "        doc_texts = \"\\n\\n\".join([\n",
    "            f\"Document {i+1}:\\n{doc['text'][:300]}...\"\n",
    "            for i, doc in enumerate(documents)\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"You are a document relevance evaluator. Evaluate how relevant each document is to the given query.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Documents:\n",
    "{doc_texts}\n",
    "\n",
    "For each document, classify it as:\n",
    "- CORRECT: Highly relevant and directly answers the query\n",
    "- AMBIGUOUS: Partially relevant or contains some useful information\n",
    "- INCORRECT: Not relevant or off-topic\n",
    "\n",
    "Format your response as:\n",
    "Document 1: [CORRECT/AMBIGUOUS/INCORRECT] - [brief reason]\n",
    "Document 2: [CORRECT/AMBIGUOUS/INCORRECT] - [brief reason]\n",
    "etc.\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "        \n",
    "        evaluation, elapsed = self._llm_invoke_timed(prompt, \"llm_relevance_evaluation\")\n",
    "        \n",
    "        # Parse evaluation results\n",
    "        categorized = {\n",
    "            'correct': [],\n",
    "            'ambiguous': [],\n",
    "            'incorrect': []\n",
    "        }\n",
    "        \n",
    "        lines = evaluation.strip().split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            if i >= len(documents):\n",
    "                break\n",
    "            \n",
    "            line_upper = line.upper()\n",
    "            doc = documents[i]\n",
    "            \n",
    "            if 'CORRECT' in line_upper and 'INCORRECT' not in line_upper:\n",
    "                categorized['correct'].append(doc)\n",
    "            elif 'AMBIGUOUS' in line_upper:\n",
    "                categorized['ambiguous'].append(doc)\n",
    "            elif 'INCORRECT' in line_upper:\n",
    "                categorized['incorrect'].append(doc)\n",
    "            else:\n",
    "                # Default to ambiguous if unclear\n",
    "                categorized['ambiguous'].append(doc)\n",
    "        \n",
    "        print(f\"   üìä Relevance evaluation: {len(categorized['correct'])} correct, \"\n",
    "              f\"{len(categorized['ambiguous'])} ambiguous, {len(categorized['incorrect'])} incorrect\")\n",
    "        \n",
    "        return categorized, elapsed\n",
    "    \n",
    "    def apply_knowledge_refinement(self, query: str, correct_docs: List[Dict], \n",
    "                                   ambiguous_docs: List[Dict]) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Apply knowledge refinement on ambiguous documents\n",
    "        Extract and refine relevant information from partially relevant documents\n",
    "        \"\"\"\n",
    "        if not ambiguous_docs:\n",
    "            return \"\", 0\n",
    "        \n",
    "        ambiguous_texts = \"\\n\\n\".join([\n",
    "            f\"Document {i+1}:\\n{doc['text']}\"\n",
    "            for i, doc in enumerate(ambiguous_docs)\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"You are refining partially relevant information. Extract ONLY the information that is relevant to the query.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Partially Relevant Documents:\n",
    "{ambiguous_texts}\n",
    "\n",
    "Extract and summarize only the relevant parts that help answer the query. Ignore irrelevant information.\n",
    "\n",
    "Refined Information:\"\"\"\n",
    "        \n",
    "        refined_info, elapsed = self._llm_invoke_timed(prompt, \"llm_knowledge_refinement\")\n",
    "        print(f\"   üîß Knowledge refined in {format_time_ns(elapsed)}\")\n",
    "        \n",
    "        return refined_info, elapsed\n",
    "    \n",
    "    def generate_search_queries(self, original_query: str, categorized: Dict) -> Tuple[List[str], int]:\n",
    "        \"\"\"\n",
    "        Generate alternative search queries for web search when retrieved docs are insufficient\n",
    "        \"\"\"\n",
    "        if len(categorized['correct']) >= 2:\n",
    "            return [], 0  # Sufficient correct documents found\n",
    "        \n",
    "        prompt = f\"\"\"The retrieved documents are insufficient to answer this query. Generate 2-3 alternative search queries that might find better information.\n",
    "\n",
    "Original Query: {original_query}\n",
    "\n",
    "Current situation:\n",
    "- Correct documents: {len(categorized['correct'])}\n",
    "- Ambiguous documents: {len(categorized['ambiguous'])}\n",
    "- Incorrect documents: {len(categorized['incorrect'])}\n",
    "\n",
    "Generate alternative search queries (one per line):\"\"\"\n",
    "        \n",
    "        queries_text, elapsed = self._llm_invoke_timed(prompt, \"llm_generate_search_queries\")\n",
    "        \n",
    "        # Parse queries\n",
    "        search_queries = []\n",
    "        for line in queries_text.strip().split('\\n'):\n",
    "            clean = line.strip().lstrip('0123456789.-‚Ä¢) ').strip()\n",
    "            if clean and len(clean) > 10:\n",
    "                search_queries.append(clean)\n",
    "        \n",
    "        if search_queries:\n",
    "            print(f\"   üîç Generated {len(search_queries)} alternative search queries\")\n",
    "            for i, sq in enumerate(search_queries, 1):\n",
    "                print(f\"      {i}. {sq}\")\n",
    "        \n",
    "        return search_queries, elapsed\n",
    "    \n",
    "    def fallback_web_search(self, search_queries: List[str]) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Simulate web search fallback (in real implementation, this would call a web search API)\n",
    "        \"\"\"\n",
    "        if not search_queries:\n",
    "            return \"\", 0\n",
    "        \n",
    "        start = time.time_ns()\n",
    "        \n",
    "        # Simulate web search - in production, this would use actual web search\n",
    "        print(f\"   üåê [SIMULATED] Web search for: {search_queries[0][:50]}...\")\n",
    "        web_results = \"\"\"[Simulated Web Search Results]\n",
    "        \n",
    "In a production environment, this would contain real web search results from external sources.\n",
    "The system would fetch additional information to supplement the document collection when\n",
    "the retrieved documents are deemed insufficient to answer the query accurately.\"\"\"\n",
    "        \n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"web_search_fallback\", elapsed)\n",
    "        \n",
    "        return web_results, elapsed\n",
    "    \n",
    "    def generate_corrected_answer(self, query: str, correct_docs: List[Dict], \n",
    "                                  refined_info: str, web_info: str) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Generate final answer using corrected and refined information\n",
    "        \"\"\"\n",
    "        # Build context\n",
    "        context_parts = []\n",
    "        \n",
    "        if correct_docs:\n",
    "            correct_context = \"\\n\\n\".join([\n",
    "                f\"Relevant Document {i+1}:\\n{doc['text']}\"\n",
    "                for i, doc in enumerate(correct_docs)\n",
    "            ])\n",
    "            context_parts.append(f\"Highly Relevant Information:\\n{correct_context}\")\n",
    "        \n",
    "        if refined_info:\n",
    "            context_parts.append(f\"\\nRefined Information:\\n{refined_info}\")\n",
    "        \n",
    "        if web_info:\n",
    "            context_parts.append(f\"\\nAdditional Information:\\n{web_info}\")\n",
    "        \n",
    "        full_context = \"\\n\\n\".join(context_parts) if context_parts else \"No relevant information found.\"\n",
    "        \n",
    "        prompt = f\"\"\"Answer the following query using the provided corrected and refined information.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "{full_context}\n",
    "\n",
    "Provide a comprehensive, accurate answer based on the information above.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        answer, elapsed = self._llm_invoke_timed(prompt, \"llm_generate_corrected_answer\")\n",
    "        print(f\"   ‚úÖ Corrected answer generated in {format_time_ns(elapsed)}\")\n",
    "        \n",
    "        return answer, elapsed\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process query using Corrective RAG (CRAG)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üîß CORRECTIVE RAG (CRAG) QUERY PROCESSING\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        \n",
    "        # Step 1: Initial Retrieval\n",
    "        print(\"üì• Step 1: Initial Document Retrieval\")\n",
    "        print(\"-\" * 70)\n",
    "        documents, retrieval_time = self.retrieve_documents(question, k=5)\n",
    "        \n",
    "        # Step 2: Relevance Evaluation\n",
    "        print(\"\\n‚öñÔ∏è  Step 2: Relevance Evaluation\")\n",
    "        print(\"-\" * 70)\n",
    "        categorized, eval_time = self.evaluate_relevance(question, documents)\n",
    "        \n",
    "        # Step 3: Knowledge Refinement (for ambiguous docs)\n",
    "        print(\"\\nüîß Step 3: Knowledge Refinement\")\n",
    "        print(\"-\" * 70)\n",
    "        refined_info = \"\"\n",
    "        refine_time = 0\n",
    "        if categorized['ambiguous']:\n",
    "            refined_info, refine_time = self.apply_knowledge_refinement(\n",
    "                question, \n",
    "                categorized['correct'], \n",
    "                categorized['ambiguous']\n",
    "            )\n",
    "        else:\n",
    "            print(\"   ‚ÑπÔ∏è  No ambiguous documents to refine\")\n",
    "        \n",
    "        # Step 4: Web Search Fallback (if needed)\n",
    "        print(\"\\nüåê Step 4: Web Search Fallback\")\n",
    "        print(\"-\" * 70)\n",
    "        web_info = \"\"\n",
    "        web_time = 0\n",
    "        search_queries = []\n",
    "        \n",
    "        if len(categorized['correct']) < 2:\n",
    "            print(\"   ‚ö†Ô∏è  Insufficient relevant documents, generating search queries...\")\n",
    "            search_queries, sq_time = self.generate_search_queries(question, categorized)\n",
    "            \n",
    "            if search_queries:\n",
    "                web_info, web_time = self.fallback_web_search(search_queries)\n",
    "            else:\n",
    "                print(\"   ‚ÑπÔ∏è  No additional search needed\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Sufficient relevant documents found, skipping web search\")\n",
    "        \n",
    "        # Step 5: Generate Corrected Answer\n",
    "        print(\"\\nüí° Step 5: Generate Corrected Answer\")\n",
    "        print(\"-\" * 70)\n",
    "        answer, gen_time = self.generate_corrected_answer(\n",
    "            question,\n",
    "            categorized['correct'],\n",
    "            refined_info,\n",
    "            web_info\n",
    "        )\n",
    "        \n",
    "        total_query_ns = time.time_ns() - overall_start\n",
    "        latency_report.add(\"crag_query_total\", total_query_ns)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üí¨ FINAL CORRECTED ANSWER:\")\n",
    "        print(\"=\"*70)\n",
    "        print(answer[:800])\n",
    "        if len(answer) > 800:\n",
    "            print(\"...\")\n",
    "        \n",
    "        print(f\"\\nüìä CRAG Statistics:\")\n",
    "        print(f\"   Total documents retrieved: {len(documents)}\")\n",
    "        print(f\"   Correct documents: {len(categorized['correct'])}\")\n",
    "        print(f\"   Ambiguous documents: {len(categorized['ambiguous'])}\")\n",
    "        print(f\"   Incorrect documents: {len(categorized['incorrect'])}\")\n",
    "        print(f\"   Knowledge refined: {'Yes' if refined_info else 'No'}\")\n",
    "        print(f\"   Web search used: {'Yes' if web_info else 'No'}\")\n",
    "        print(f\"   Total query time: {format_time_ns(total_query_ns)}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"categorized_docs\": categorized,\n",
    "            \"refined_info\": refined_info,\n",
    "            \"web_search_used\": bool(web_info),\n",
    "            \"search_queries\": search_queries,\n",
    "            \"total_query_ns\": total_query_ns,\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# Vader Sentiment\n",
    "# ---------------------------\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores[\"compound\"]\n",
    "        \n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "            percentage = round((compound + 1) * 50, 2)\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "            percentage = round((1 - abs(compound)) * 50, 2)\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "            percentage = round(50 + (compound * 50), 2)\n",
    "        \n",
    "        return {\"label\": label, \"percentage\": percentage, \"compound\": compound, \"scores\": scores}\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str],\n",
    "                            target_ns: int = TARGET_NS, run_number: int = 1):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üî• SENTIMENT BENCHMARK RUN #{run_number}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üéØ TARGET: < {target_ns} ns per analysis\\n\")\n",
    "    \n",
    "    individual_times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start_ns = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed_ns = time.time_ns() - start_ns\n",
    "        latency_report.add(\"vader_per_example\", elapsed_ns)\n",
    "        individual_times.append(elapsed_ns)\n",
    "        \n",
    "        status = \"‚úÖ\" if elapsed_ns < target_ns else \"‚ùå\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed_ns):25s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    \n",
    "    total_ns = sum(individual_times)\n",
    "    avg_ns = total_ns // len(individual_times)\n",
    "    min_ns = min(individual_times)\n",
    "    max_ns = max(individual_times)\n",
    "    under_target = sum(1 for t in individual_times if t < target_ns)\n",
    "    \n",
    "    print(\"\\nüìä RUN #{run_number} STATISTICS:\")\n",
    "    print(f\"   Total:        {format_time_ns(total_ns)}\")\n",
    "    print(f\"   Average:      {format_time_ns(avg_ns)}\")\n",
    "    print(f\"   Min:          {format_time_ns(min_ns)}\")\n",
    "    print(f\"   Max:          {format_time_ns(max_ns)}\")\n",
    "    print(f\"   < {target_ns}ns: {under_target}/{len(individual_times)} texts\")\n",
    "    \n",
    "    if avg_ns < target_ns:\n",
    "        print(\"   ‚úÖ TARGET MET!\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  TARGET MISSED\")\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üîß CORRECTIVE RAG (CRAG) + FULL LATENCY INSTRUMENTATION\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    pipeline_start = time.time_ns()\n",
    "    \n",
    "    # Phase 1: Load and prepare data\n",
    "    print(\"üìö PHASE 1: DATA PREPARATION\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    text, load_time = timed_call(load_pdf, PDF_PATH)\n",
    "    latency_report.add(\"pipeline_pdf_load\", load_time)\n",
    "    \n",
    "    chunks, chunk_time = timed_call(chunk_text, text, 1000, 100)\n",
    "    latency_report.add(\"pipeline_chunking\", chunk_time)\n",
    "    \n",
    "    embedder, embed_time = timed_call(load_embeddings, EMBED_MODEL)\n",
    "    latency_report.add(\"pipeline_embeddings_load\", embed_time)\n",
    "    \n",
    "    milvus_collection, milvus_time = timed_call(init_milvus, MILVUS_HOST, MILVUS_PORT, COLLECTION_NAME, DIM)\n",
    "    latency_report.add(\"pipeline_milvus_init\", milvus_time)\n",
    "    \n",
    "    insert_time_start = time.time_ns()\n",
    "    insert_chunks(milvus_collection, embedder, chunks)\n",
    "    insert_time = time.time_ns() - insert_time_start\n",
    "    latency_report.add(\"pipeline_insert_chunks\", insert_time)\n",
    "    \n",
    "    # Phase 2: Initialize LLM\n",
    "    print(\"\\nüìö PHASE 2: LLM INITIALIZATION\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    llm_start = time.time_ns()\n",
    "    llm = ChatGroq(model_name=MODEL_NAME, groq_api_key=GROQ_API_KEY, temperature=0)\n",
    "    llm_time = time.time_ns() - llm_start\n",
    "    latency_report.add(\"llm_init\", llm_time)\n",
    "    print(f\"‚úÖ LLM initialized in {format_time_ns(llm_time)}\")\n",
    "    \n",
    "    # Initialize Corrective RAG\n",
    "    crag = CorrectiveRAG(llm, milvus_collection, embedder, relevance_threshold=RELEVANCE_THRESHOLD)\n",
    "    print(f\"\\n‚úÖ Corrective RAG system initialized!\")\n",
    "    \n",
    "    # Phase 3: Run CRAG queries\n",
    "    print(\"\\nüìö PHASE 3: CORRECTIVE RAG QUERIES\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    queries = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Tell me about the character development.\",\n",
    "        \"What is quantum computing?\",  # Likely to trigger web search fallback\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for q in queries:\n",
    "        result = crag.query(q)\n",
    "        results.append(result)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Phase 4: Sentiment benchmark\n",
    "    print(\"\\nüìö PHASE 4: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    sa_init = 0\n",
    "    latency_report.add(\"vader_init\", sa_init)\n",
    "    print(f\"‚úÖ VADER INIT TIME: {format_time_ns(sa_init)}\\n\")\n",
    "    \n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    \n",
    "    for run in range(1, 3):\n",
    "        run_sentiment_benchmark(sa, examples, TARGET_NS, run)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency_report.add(\"pipeline_total\", pipeline_total)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìà PIPELINE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total pipeline time: {format_time_ns(pipeline_total)}\")\n",
    "    if results:\n",
    "        print(f\"Queries executed: {len(queries)}\")\n",
    "        print(f\"Average query time: {format_time_ns(sum(r['total_query_ns'] for r in results) // len(results))}\")\n",
    "        correct_total = sum(len(r['categorized_docs']['correct']) for r in results)\n",
    "        ambiguous_total = sum(len(r['categorized_docs']['ambiguous']) for r in results)\n",
    "        incorrect_total = sum(len(r['categorized_docs']['incorrect']) for r in results)\n",
    "        print(f\"Total correct documents used: {correct_total}\")\n",
    "        print(f\"Total ambiguous documents refined: {ambiguous_total}\")\n",
    "        print(f\"Total incorrect documents filtered: {incorrect_total}\")\n",
    "        web_searches = sum(1 for r in results if r['web_search_used'])\n",
    "        print(f\"Web searches triggered: {web_searches}/{len(results)}\")\n",
    "    \n",
    "    latency_report.pretty_print()\n",
    "    \n",
    "    try:\n",
    "        connections.disconnect()\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Interrupted by user\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Fatal error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
