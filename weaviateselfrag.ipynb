{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "weaviate_selfrag_full.py\n",
    "Self-RAG with Weaviate and comprehensive nanosecond latency instrumentation.\n",
    "\n",
    "Features:\n",
    "- Weaviate vector database integration\n",
    "- Full pipeline timing (PDF load, chunking, embeddings, vectorstore)\n",
    "- Per-component latency tracking\n",
    "- Query-level performance metrics\n",
    "- Detailed latency reports\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import pdfplumber\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION_NAME = \"SelfRAG_Documents\"\n",
    "DIM = 384  # MiniLM embedding dimension\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000\n",
    "\n",
    "# Weaviate credentials\n",
    "WEAVIATE_URL = \"21ookhjbswyl5urlawqmxw.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
    "WEAVIATE_API_KEY = \"NTVWQ1dZVDI1bkptcndrZF9JRTFySVg3TEFBc1R5V0luUEtHaU9MajB6am5VQkc3aG5yVkgwWkFQVDc0PV92MjAw\"\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"‚ùå ERROR: Set GROQ_API_KEY environment variable!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =========================================================\n",
    "# LATENCY UTILITIES\n",
    "# =========================================================\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    \"\"\"Return human-readable representation of nanoseconds.\"\"\"\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    \"\"\"Call fn(*args, **kwargs) and return (result, elapsed_ns).\"\"\"\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "def timer_ns(func):\n",
    "    \"\"\"Decorator that prints elapsed ns and stores last_elapsed_ns on wrapper.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"‚è±Ô∏è  {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "class LatencyReport:\n",
    "    \"\"\"Aggregates and reports latency metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    \n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    \n",
    "    def summary(self) -> Dict:\n",
    "        out = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            out[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total_ns\": total,\n",
    "                \"avg_ns\": total // len(vals) if vals else 0,\n",
    "                \"min_ns\": min(vals) if vals else 0,\n",
    "                \"max_ns\": max(vals) if vals else 0\n",
    "            }\n",
    "        return out\n",
    "    \n",
    "    def pretty_print(self):\n",
    "        s = self.summary()\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY (nanoseconds)\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items(), key=lambda p: p[0]):\n",
    "            print(f\"\\nüìä Component: {comp}\")\n",
    "            print(f\"   Count:     {stats['count']}\")\n",
    "            print(f\"   Total:     {format_time_ns(stats['total_ns'])}\")\n",
    "            print(f\"   Average:   {format_time_ns(stats['avg_ns'])}\")\n",
    "            print(f\"   Min:       {format_time_ns(stats['min_ns'])}\")\n",
    "            print(f\"   Max:       {format_time_ns(stats['max_ns'])}\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# =========================================================\n",
    "# PDF LOAD WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def load_pdf(path: str) -> str:\n",
    "    \"\"\"Load PDF with per-page timing\"\"\"\n",
    "    print(f\"üìÑ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    \n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for i, p in enumerate(pdf.pages):\n",
    "            start_ns = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start_ns\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            text += t + \"\\n\"\n",
    "    \n",
    "    print(f\"‚úÖ Loaded PDF: {len(text)} characters from {len(pdf.pages)} pages\")\n",
    "    return text\n",
    "\n",
    "# =========================================================\n",
    "# CHUNK TEXT WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    \"\"\"Chunk text with timing\"\"\"\n",
    "    print(f\"‚úÇÔ∏è  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# =========================================================\n",
    "# LOAD EMBEDDINGS WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def load_embeddings(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    \"\"\"Load embedding model with timing\"\"\"\n",
    "    print(f\"üî¢ Loading embeddings model: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    print(f\"‚úÖ Embeddings model loaded\")\n",
    "    return embedder\n",
    "\n",
    "# =========================================================\n",
    "# INIT WEAVIATE WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def init_weaviate(url: str, api_key: str, collection_name: str = COLLECTION_NAME) -> weaviate.WeaviateClient:\n",
    "    \"\"\"Initialize Weaviate client and collection with timing\"\"\"\n",
    "    print(f\"üóÉÔ∏è  Initializing Weaviate connection to {url}\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    \n",
    "    # Connect to Weaviate Cloud\n",
    "    client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=url,\n",
    "        auth_credentials=Auth.api_key(api_key)\n",
    "    )\n",
    "    \n",
    "    connect_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_connect\", connect_time)\n",
    "    print(f\"‚úÖ Connected to Weaviate ({format_time_ns(connect_time)})\")\n",
    "    \n",
    "    # Delete collection if exists\n",
    "    try:\n",
    "        if client.collections.exists(collection_name):\n",
    "            start = time.time_ns()\n",
    "            client.collections.delete(collection_name)\n",
    "            delete_time = time.time_ns() - start\n",
    "            latency_report.add(\"weaviate_delete_collection\", delete_time)\n",
    "            print(f\"üóëÔ∏è  Deleted existing collection '{collection_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Collection check/delete: {e}\")\n",
    "    \n",
    "    # Create collection\n",
    "    start = time.time_ns()\n",
    "    try:\n",
    "        client.collections.create(\n",
    "            name=collection_name,\n",
    "            vectorizer_config=None,  # We'll provide vectors manually\n",
    "            properties=[\n",
    "                {\"name\": \"text\", \"dataType\": [\"text\"]},\n",
    "                {\"name\": \"chunk_id\", \"dataType\": [\"int\"]},\n",
    "                {\"name\": \"source\", \"dataType\": [\"text\"]}\n",
    "            ]\n",
    "        )\n",
    "        create_time = time.time_ns() - start\n",
    "        latency_report.add(\"weaviate_create_collection\", create_time)\n",
    "        print(f\"‚úÖ Collection '{collection_name}' created ({format_time_ns(create_time)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Collection creation: {e}\")\n",
    "    \n",
    "    return client\n",
    "\n",
    "# =========================================================\n",
    "# INSERT CHUNKS WITH TIMING\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def insert_chunks(client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                  chunks: List[str], collection_name: str = COLLECTION_NAME) -> None:\n",
    "    \"\"\"Insert chunks into Weaviate with detailed timing\"\"\"\n",
    "    print(f\"‚¨ÜÔ∏è  Inserting {len(chunks)} chunks into Weaviate...\")\n",
    "    \n",
    "    # Encode chunks (batch embedding)\n",
    "    print(f\"   üî¢ Encoding {len(chunks)} chunks...\")\n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_encode_batch\", encode_time)\n",
    "    print(f\"   ‚úÖ Encoded in {format_time_ns(encode_time)}\")\n",
    "    \n",
    "    # Get collection\n",
    "    collection = client.collections.get(collection_name)\n",
    "    \n",
    "    # Insert objects with vectors\n",
    "    print(f\"   üíæ Upserting to Weaviate...\")\n",
    "    start = time.time_ns()\n",
    "    \n",
    "    with collection.batch.dynamic() as batch:\n",
    "        for i, (chunk, vector) in enumerate(zip(chunks, vectors)):\n",
    "            batch.add_object(\n",
    "                properties={\n",
    "                    \"text\": chunk,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"source\": f\"chunk_{i}\"\n",
    "                },\n",
    "                vector=vector.tolist()\n",
    "            )\n",
    "    \n",
    "    upsert_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_upsert\", upsert_time)\n",
    "    print(f\"   ‚úÖ Upserted in {format_time_ns(upsert_time)}\")\n",
    "    \n",
    "    print(f\"‚úÖ All chunks inserted successfully!\")\n",
    "\n",
    "# =========================================================\n",
    "# SEARCH WEAVIATE WITH TIMING\n",
    "# =========================================================\n",
    "def search_weaviate(client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                    query: str, limit: int = 4, collection_name: str = COLLECTION_NAME) -> Tuple[List[str], int]:\n",
    "    \"\"\"Search Weaviate with timing\"\"\"\n",
    "    \n",
    "    # Encode query\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"query_embedding\", encode_time)\n",
    "    \n",
    "    # Query Weaviate\n",
    "    start = time.time_ns()\n",
    "    collection = client.collections.get(collection_name)\n",
    "    \n",
    "    response = collection.query.near_vector(\n",
    "        near_vector=qvec.tolist(),\n",
    "        limit=limit,\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "    \n",
    "    search_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_search\", search_time)\n",
    "    \n",
    "    # Extract texts\n",
    "    hits = [obj.properties.get(\"text\", \"\") for obj in response.objects]\n",
    "    \n",
    "    total_time = encode_time + search_time\n",
    "    \n",
    "    return hits, total_time\n",
    "\n",
    "# =========================================================\n",
    "# SELF-RAG WITH TIMING\n",
    "# =========================================================\n",
    "class SelfRAG:\n",
    "    \"\"\"Self-RAG system with comprehensive timing\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                 collection_name: str = COLLECTION_NAME):\n",
    "        self.llm = llm\n",
    "        self.client = client\n",
    "        self.embedder = embedder\n",
    "        self.collection_name = collection_name\n",
    "    \n",
    "    def _llm_invoke_timed(self, prompt: str, label: str) -> Tuple[str, int]:\n",
    "        \"\"\"Invoke LLM with timing\"\"\"\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label, elapsed)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label + \"_error\", elapsed)\n",
    "            print(f\"LLM invoke for {label} failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return str(e), elapsed\n",
    "    \n",
    "    def retrieve_decision(self, query: str) -> Tuple[bool, str, int]:\n",
    "        \"\"\"Decide if retrieval is needed\"\"\"\n",
    "        prompt = f\"\"\"You are a helpful assistant. Decide if you need to retrieve information from a document to answer this question.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Think step by step:\n",
    "1. Can you answer this from general knowledge?\n",
    "2. Does it require specific document information?\n",
    "\n",
    "Answer with ONLY 'RETRIEVE' or 'NO_RETRIEVE' and a brief reason.\n",
    "\n",
    "Decision:\"\"\"\n",
    "        \n",
    "        decision_text, elapsed = self._llm_invoke_timed(prompt, \"llm_retrieve_decision\")\n",
    "        needs_retrieval = 'RETRIEVE' in decision_text.upper() and 'NO_RETRIEVE' not in decision_text.upper()\n",
    "        \n",
    "        print(f\"ü§î Retrieval Decision: {'RETRIEVE' if needs_retrieval else 'NO_RETRIEVE'}\")\n",
    "        print(f\"   Reasoning: {decision_text.strip()[:100]}...\")\n",
    "        print(f\"   Time: {format_time_ns(elapsed)}\")\n",
    "        \n",
    "        return needs_retrieval, decision_text, elapsed\n",
    "    \n",
    "    def retrieve_documents(self, query: str, k: int = 4) -> Tuple[str, int]:\n",
    "        \"\"\"Retrieve documents from Weaviate\"\"\"\n",
    "        print(f\"   üîç Retrieving documents...\")\n",
    "        \n",
    "        hits, elapsed = search_weaviate(self.client, self.embedder, query, k, self.collection_name)\n",
    "        context = \"\\n\\n\".join(hits)\n",
    "        \n",
    "        print(f\"   ‚úÖ Retrieved {len(hits)} documents in {format_time_ns(elapsed)}\")\n",
    "        \n",
    "        return context, elapsed\n",
    "    \n",
    "    def generate_answer(self, query: str, context: str = \"\") -> Tuple[str, int]:\n",
    "        \"\"\"Generate answer with timing\"\"\"\n",
    "        if context:\n",
    "            prompt = f\"\"\"Answer the question based on the following context:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Provide a detailed answer based on the context above.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Answer the following question based on your general knowledge:\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        print(f\"   üí° Generating answer...\")\n",
    "        answer, elapsed = self._llm_invoke_timed(prompt, \"llm_generate_answer\")\n",
    "        print(f\"   ‚úÖ Answer generated in {format_time_ns(elapsed)}\")\n",
    "        \n",
    "        return answer, elapsed\n",
    "    \n",
    "    def self_critique(self, query: str, answer: str, context: str = \"\") -> Tuple[str, bool, int]:\n",
    "        \"\"\"Self-critique the answer\"\"\"\n",
    "        critique_prompt = f\"\"\"You are a critical evaluator. Evaluate the following answer.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Context Available: {'Yes' if context else 'No'}\n",
    "\n",
    "Rate the answer on a scale of 1-10 and provide:\n",
    "1. Relevance Score (1-10)\n",
    "2. Completeness Score (1-10)\n",
    "3. Accuracy Assessment\n",
    "4. Should we retrieve more information? (YES/NO)\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "        \n",
    "        print(f\"   üîç Self-critiquing answer...\")\n",
    "        critique_text, elapsed = self._llm_invoke_timed(critique_prompt, \"llm_self_critique\")\n",
    "        \n",
    "        needs_more = 'YES' in critique_text.upper() and 'RETRIEVE' in critique_text.upper()\n",
    "        \n",
    "        print(f\"   ‚úÖ Critique completed in {format_time_ns(elapsed)}\")\n",
    "        \n",
    "        return critique_text, needs_more, elapsed\n",
    "    \n",
    "    def query(self, question: str, max_iterations: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"Process query with Self-RAG pipeline\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üöÄ SELF-RAG QUERY PROCESSING\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚ùì Question: {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        \n",
    "        iteration = 0\n",
    "        context = \"\"\n",
    "        answer = \"\"\n",
    "        per_iteration_times = []\n",
    "        \n",
    "        while iteration < max_iterations:\n",
    "            iter_start = time.time_ns()\n",
    "            iteration += 1\n",
    "            print(f\"\\n--- Iteration {iteration} ---\")\n",
    "            \n",
    "            # Step 1: Decide if retrieval is needed\n",
    "            if iteration == 1:\n",
    "                needs_retrieval, decision_reason, decision_time = self.retrieve_decision(question)\n",
    "            else:\n",
    "                needs_retrieval = True  # Force retrieval when refining\n",
    "            \n",
    "            # Step 2: Retrieve if needed\n",
    "            if needs_retrieval:\n",
    "                context, retrieval_time = self.retrieve_documents(question)\n",
    "                print(f\"   üìù Context length: {len(context)} characters\")\n",
    "            \n",
    "            # Step 3: Generate answer\n",
    "            answer, gen_time = self.generate_answer(question, context)\n",
    "            print(f\"   üìÑ Answer length: {len(answer)} characters\")\n",
    "            \n",
    "            # Step 4: Self-critique\n",
    "            critique_text, needs_more, critique_time = self.self_critique(question, answer, context)\n",
    "            \n",
    "            iter_elapsed = time.time_ns() - iter_start\n",
    "            per_iteration_times.append(iter_elapsed)\n",
    "            latency_report.add(\"selfrag_iteration\", iter_elapsed)\n",
    "            \n",
    "            print(f\"\\n‚è±Ô∏è  Iteration {iteration} total time: {format_time_ns(iter_elapsed)}\")\n",
    "            \n",
    "            # Step 5: Decide loop break\n",
    "            if not needs_more or iteration >= max_iterations:\n",
    "                print(f\"\\n‚úÖ Self-RAG completed after {iteration} iteration(s)\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"\\nüîÑ Refinement needed, starting iteration {iteration + 1}...\")\n",
    "        \n",
    "        total_query_ns = time.time_ns() - overall_start\n",
    "        latency_report.add(\"selfrag_query_total\", total_query_ns)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üí¨ FINAL ANSWER:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(answer[:800])\n",
    "        if len(answer) > 800:\n",
    "            print(\"...\")\n",
    "        print(f\"\\n‚è±Ô∏è  Total query time: {format_time_ns(total_query_ns)}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'context': context,\n",
    "            'iterations': iteration,\n",
    "            'critique': critique_text,\n",
    "            'per_iteration_times': per_iteration_times,\n",
    "            'total_query_ns': total_query_ns\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# VADER SENTIMENT BENCHMARK\n",
    "# =========================================================\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        \n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "            percentage = round((compound + 1) * 50, 2)\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "            percentage = round((1 - abs(compound)) * 50, 2)\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "            percentage = round(50 + (compound * 50), 2)\n",
    "        \n",
    "        return {\n",
    "            'label': label,\n",
    "            'percentage': percentage,\n",
    "            'compound': compound,\n",
    "            'scores': scores\n",
    "        }\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str],\n",
    "                            target_ns: int = TARGET_NS, run_number: int = 1):\n",
    "    \"\"\"Run sentiment analysis benchmark\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî• SENTIMENT BENCHMARK RUN #{run_number}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üéØ TARGET: < {target_ns} ns per analysis\\n\")\n",
    "    \n",
    "    individual_times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start_ns = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed_ns = time.time_ns() - start_ns\n",
    "        latency_report.add(\"vader_per_example\", elapsed_ns)\n",
    "        individual_times.append(elapsed_ns)\n",
    "        \n",
    "        status = \"‚úÖ\" if elapsed_ns < target_ns else \"‚ùå\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed_ns):25s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    \n",
    "    total_ns = sum(individual_times)\n",
    "    avg_ns = total_ns // len(individual_times)\n",
    "    min_ns = min(individual_times)\n",
    "    max_ns = max(individual_times)\n",
    "    under_target = sum(1 for t in individual_times if t < target_ns)\n",
    "    \n",
    "    print(f\"\\nüìä RUN #{run_number} STATISTICS:\")\n",
    "    print(f\"   Total:        {format_time_ns(total_ns)}\")\n",
    "    print(f\"   Average:      {format_time_ns(avg_ns)}\")\n",
    "    print(f\"   Min:          {format_time_ns(min_ns)}\")\n",
    "    print(f\"   Max:          {format_time_ns(max_ns)}\")\n",
    "    print(f\"   < {target_ns}ns: {under_target}/{len(individual_times)} texts\")\n",
    "    \n",
    "    if avg_ns < target_ns:\n",
    "        print(f\"   ‚úÖ TARGET MET!\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  TARGET MISSED\")\n",
    "\n",
    "# =========================================================\n",
    "# MAIN PROGRAM\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üöÄ SELF-RAG WEAVIATE + FULL LATENCY INSTRUMENTATION\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    pipeline_start = time.time_ns()\n",
    "    \n",
    "    # Phase 1: Load and prepare data\n",
    "    print(\"üìö PHASE 1: DATA PREPARATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    text, load_time = timed_call(load_pdf, PDF_PATH)\n",
    "    latency_report.add(\"pipeline_pdf_load\", load_time)\n",
    "    \n",
    "    chunks, chunk_time = timed_call(chunk_text, text, 1000, 100)\n",
    "    latency_report.add(\"pipeline_chunking\", chunk_time)\n",
    "    \n",
    "    embedder, embed_time = timed_call(load_embeddings)\n",
    "    latency_report.add(\"pipeline_embeddings_load\", embed_time)\n",
    "    \n",
    "    weaviate_client, weaviate_time = timed_call(init_weaviate, WEAVIATE_URL, WEAVIATE_API_KEY, COLLECTION_NAME)\n",
    "    latency_report.add(\"pipeline_weaviate_init\", weaviate_time)\n",
    "    \n",
    "    insert_time_start = time.time_ns()\n",
    "    insert_chunks(weaviate_client, embedder, chunks, COLLECTION_NAME)\n",
    "    insert_time = time.time_ns() - insert_time_start\n",
    "    latency_report.add(\"pipeline_insert_chunks\", insert_time)\n",
    "    \n",
    "    # Phase 2: Initialize LLM\n",
    "    print(f\"\\nüìö PHASE 2: LLM INITIALIZATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    llm_start = time.time_ns()\n",
    "    llm = ChatGroq(\n",
    "        model_name=MODEL_NAME,\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        temperature=0\n",
    "    )\n",
    "    llm_time = time.time_ns() - llm_start\n",
    "    latency_report.add(\"llm_init\", llm_time)\n",
    "    print(f\"‚úÖ LLM initialized in {format_time_ns(llm_time)}\")\n",
    "    \n",
    "    # Initialize Self-RAG\n",
    "    self_rag = SelfRAG(llm, weaviate_client, embedder, COLLECTION_NAME)\n",
    "    print(f\"\\n‚úÖ Self-RAG system initialized!\")\n",
    "    \n",
    "    # Phase 3: Run queries\n",
    "    print(f\"\\nüìö PHASE 3: SELF-RAG QUERIES\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    queries = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Summarize the key events in the document.\",\n",
    "        \"What is the capital of France?\"  # General knowledge test\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for q in queries:\n",
    "        result = self_rag.query(q, max_iterations=2)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Phase 4: Sentiment benchmark\n",
    "    print(f\"\\nüìö PHASE 4: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    sa_start = time.time_ns()\n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    sa_init = time.time_ns() - sa_start\n",
    "    latency_report.add(\"vader_init\", sa_init)\n",
    "    print(f\"‚úÖ VADER INIT TIME: {format_time_ns(sa_init)}\\n\")\n",
    "    \n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    \n",
    "    for run in range(1, 3):\n",
    "        run_sentiment_benchmark(sa, examples, TARGET_NS, run)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Final summary\n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency_report.add(\"pipeline_total\", pipeline_total)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìà PIPELINE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total pipeline time: {format_time_ns(pipeline_total)}\")\n",
    "    print(f\"Queries executed: {len(queries)}\")\n",
    "    print(f\"Average query time: {format_time_ns(sum(r['total_query_ns'] for r in results) // len(results))}\")\n",
    "    \n",
    "    # Detailed latency report\n",
    "    latency_report.pretty_print()\n",
    "    \n",
    "    # Cleanup\n",
    "    weaviate_client.close()\n",
    "    \n",
    "    print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Interrupted by user\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Fatal error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
