{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d285f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "plan_solve_rag.py - Plan-and-Solve RAG\n",
    "Creates a comprehensive plan first, then executes it systematically\n",
    "\n",
    "Plan-and-Solve RAG Process:\n",
    "1. PLAN: Break down query into logical sub-tasks\n",
    "2. SOLVE: Execute each sub-task in sequence\n",
    "3. AGGREGATE: Combine all results\n",
    "4. ANSWER: Generate comprehensive final answer\n",
    "\n",
    "Advantages:\n",
    "- More structured than ReAct\n",
    "- Better handling of complex queries\n",
    "- Predictable execution flow\n",
    "- Easier debugging\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pdfplumber\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "INDEX_NAME = \"new2\"\n",
    "DIM = 384\n",
    "PINECONE_API_KEY = \"pcsk_6nSvRh_ALcjY7qdJPA83yRRpUJSFzRnNp3Y5sRNZFwXRY2RDfynWRZJjGMDhc1eKFfArWL\"\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "\n",
    "# ---------------------------\n",
    "# UTILITIES\n",
    "# ---------------------------\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s\"\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    \n",
    "    def add(self, component, ns):\n",
    "        self.store[component].append(ns)\n",
    "    \n",
    "    def pretty_print(self):\n",
    "        s = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            s[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total\": format_time_ns(total),\n",
    "                \"avg\": format_time_ns(total // len(vals) if vals else 0),\n",
    "                \"min\": format_time_ns(min(vals) if vals else 0),\n",
    "                \"max\": format_time_ns(max(vals) if vals else 0)\n",
    "            }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items()):\n",
    "            print(f\"\\nüìä Component: {comp}\")\n",
    "            for k, v in stats.items():\n",
    "                print(f\"   {k.capitalize():10s} {v}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# ---------------------------\n",
    "# PDF/EMBEDDINGS/PINECONE\n",
    "# ---------------------------\n",
    "def load_pdf(path):\n",
    "    start = time.time_ns()\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        page_texts = []\n",
    "        for p in pdf.pages:\n",
    "            t = p.extract_text() or \"\"\n",
    "            page_texts.append(t)\n",
    "        text = \"\\n\".join(page_texts)\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"pipeline_pdf_load\", elapsed)\n",
    "    print(f\"üìÑ Loaded PDF: {len(text)} chars ({format_time_ns(elapsed)})\")\n",
    "    return text\n",
    "\n",
    "def chunk_text(text):\n",
    "    start = time.time_ns()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = splitter.split_text(text)\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"pipeline_chunking\", elapsed)\n",
    "    print(f\"üìÑ Created {len(chunks)} chunks ({format_time_ns(elapsed)})\")\n",
    "    return chunks\n",
    "\n",
    "def get_embeddings_model():\n",
    "    start = time.time_ns()\n",
    "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"pipeline_embeddings_load\", elapsed)\n",
    "    print(f\"üß† Embeddings loaded ({format_time_ns(elapsed)})\")\n",
    "    return emb\n",
    "\n",
    "def init_pinecone(index_name):\n",
    "    start = time.time_ns()\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing = [idx.name for idx in pc.list_indexes()]\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"pinecone_init\", elapsed)\n",
    "    \n",
    "    if index_name not in existing:\n",
    "        print(f\"‚ùå ERROR: Index '{index_name}' does not exist!\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"‚úÖ Connected to index '{index_name}' ({format_time_ns(elapsed)})\")\n",
    "    return pc\n",
    "\n",
    "def create_vectorstore(embed, chunks, index_name):\n",
    "    start = time.time_ns()\n",
    "    vs = PineconeVectorStore.from_existing_index(\n",
    "        index_name=index_name,\n",
    "        embedding=embed\n",
    "    )\n",
    "    \n",
    "    print(f\"üì§ Uploading {len(chunks)} chunks...\")\n",
    "    insert_start = time.time_ns()\n",
    "    vs.add_texts(\n",
    "        texts=chunks,\n",
    "        metadatas=[{\"chunk_id\": i, \"source\": \"plan_solve\"} for i in range(len(chunks))]\n",
    "    )\n",
    "    insert_elapsed = time.time_ns() - insert_start\n",
    "    latency_report.add(\"pipeline_insert_chunks\", insert_elapsed)\n",
    "    \n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"pipeline_vectorstore_create\", elapsed)\n",
    "    print(f\"‚úÖ Vector store ready ({format_time_ns(elapsed)})\")\n",
    "    return vs\n",
    "\n",
    "# ---------------------------\n",
    "# VADER SENTIMENT\n",
    "# ---------------------------\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        if compound >= 0.05:\n",
    "            return {'label': 'POSITIVE', 'compound': compound}\n",
    "        elif compound <= -0.05:\n",
    "            return {'label': 'NEGATIVE', 'compound': compound}\n",
    "        return {'label': 'NEUTRAL', 'compound': compound}\n",
    "\n",
    "def run_sentiment_benchmark(run_num, sa, examples):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî• SENTIMENT BENCHMARK RUN #{run_num}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üéØ TARGET: < {TARGET_NS} ns per analysis\\n\")\n",
    "    \n",
    "    times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed = time.time_ns() - start\n",
    "        times.append(elapsed)\n",
    "        latency_report.add(\"vader_per_example\", elapsed)\n",
    "        \n",
    "        status = \"‚úÖ\" if elapsed < TARGET_NS else \"‚ùå\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed):20s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    \n",
    "    total = sum(times)\n",
    "    avg = total // len(times)\n",
    "    \n",
    "    print(f\"\\nüìä RUN #{run_num} STATISTICS:\")\n",
    "    print(f\"   Total:        {format_time_ns(total)}\")\n",
    "    print(f\"   Average:      {format_time_ns(avg)}\")\n",
    "    print(f\"   Min:          {format_time_ns(min(times))}\")\n",
    "    print(f\"   Max:          {format_time_ns(max(times))}\")\n",
    "    print(f\"   < {TARGET_NS}ns: {sum(1 for t in times if t < TARGET_NS)}/{len(times)} texts\")\n",
    "    \n",
    "    if avg < TARGET_NS:\n",
    "        print(f\"   ‚úÖ TARGET MET\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  TARGET MISSED\")\n",
    "    \n",
    "    return avg\n",
    "\n",
    "# ---------------------------\n",
    "# PLAN-AND-SOLVE RAG\n",
    "# ---------------------------\n",
    "@dataclass\n",
    "class SubTask:\n",
    "    \"\"\"Single sub-task in the plan\"\"\"\n",
    "    id: int\n",
    "    description: str\n",
    "    type: str  # retrieve, analyze, compute, summarize\n",
    "    params: Dict[str, Any]\n",
    "    result: str = \"\"\n",
    "    success: bool = False\n",
    "    elapsed_ns: int = 0\n",
    "\n",
    "class PlanAndSolveRAG:\n",
    "    \"\"\"\n",
    "    Plan-and-Solve RAG Agent\n",
    "    \n",
    "    1. PLAN: Decompose query into logical sub-tasks\n",
    "    2. SOLVE: Execute each sub-task sequentially\n",
    "    3. AGGREGATE: Combine all results\n",
    "    4. ANSWER: Generate comprehensive response\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    def _llm_invoke(self, prompt: str, label: str) -> Tuple[str, int]:\n",
    "        \"\"\"Timed LLM invocation\"\"\"\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label, elapsed)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label + \"_error\", elapsed)\n",
    "            print(f\"‚ö†Ô∏è  LLM error: {e}\")\n",
    "            return str(e), elapsed\n",
    "    \n",
    "    def _create_plan(self, query: str) -> Tuple[List[SubTask], int]:\n",
    "        \"\"\"\n",
    "        Phase 1: PLAN\n",
    "        Decompose query into logical sub-tasks\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You are a planning assistant. Break down this query into logical sub-tasks.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Available Task Types:\n",
    "- retrieve: Search documents for information\n",
    "- analyze: Analyze or process retrieved information\n",
    "- compute: Perform calculations\n",
    "- summarize: Create summaries\n",
    "\n",
    "Create a step-by-step plan. Return as JSON:\n",
    "{{\n",
    "  \"plan\": [\n",
    "    {{\n",
    "      \"id\": 1,\n",
    "      \"description\": \"What to do\",\n",
    "      \"type\": \"retrieve|analyze|compute|summarize\",\n",
    "      \"params\": {{\"query\": \"search query\" or other params}}\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Keep the plan simple and focused (2-5 steps).\n",
    "\n",
    "Plan:\"\"\"\n",
    "        \n",
    "        plan_text, elapsed = self._llm_invoke(prompt, \"llm_planning\")\n",
    "        \n",
    "        # Parse plan\n",
    "        subtasks = []\n",
    "        try:\n",
    "            json_match = re.search(r'\\{.*\\}', plan_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                data = json.loads(json_match.group())\n",
    "                plan_items = data.get('plan', [])\n",
    "                \n",
    "                for item in plan_items:\n",
    "                    subtasks.append(SubTask(\n",
    "                        id=item.get('id', len(subtasks) + 1),\n",
    "                        description=item.get('description', ''),\n",
    "                        type=item.get('type', 'retrieve'),\n",
    "                        params=item.get('params', {})\n",
    "                    ))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Plan parsing error: {e}\")\n",
    "            # Fallback: create simple retrieve task\n",
    "            subtasks.append(SubTask(\n",
    "                id=1,\n",
    "                description=\"Retrieve relevant information from documents\",\n",
    "                type=\"retrieve\",\n",
    "                params={\"query\": query}\n",
    "            ))\n",
    "        \n",
    "        return subtasks, elapsed\n",
    "    \n",
    "    def _execute_retrieve(self, params: Dict) -> Tuple[str, int]:\n",
    "        \"\"\"Execute retrieval sub-task\"\"\"\n",
    "        query = params.get('query', '')\n",
    "        \n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            docs = self.retriever.invoke(query)\n",
    "            content = \"\\n\\n\".join([\n",
    "                getattr(doc, \"page_content\", str(doc)) for doc in docs\n",
    "            ])\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(\"subtask_retrieve\", elapsed)\n",
    "            \n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            print(f\"‚ö†Ô∏è  Retrieval error: {e}\")\n",
    "            return f\"Error: {str(e)}\", elapsed\n",
    "    \n",
    "    def _execute_analyze(self, params: Dict, context: str) -> Tuple[str, int]:\n",
    "        \"\"\"Execute analysis sub-task\"\"\"\n",
    "        instruction = params.get('instruction', 'Analyze the information')\n",
    "        \n",
    "        prompt = f\"\"\"Analyze the following information according to the instruction.\n",
    "\n",
    "Context:\n",
    "{context[:2000]}\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "Analysis:\"\"\"\n",
    "        \n",
    "        result, elapsed = self._llm_invoke(prompt, \"subtask_analyze\")\n",
    "        latency_report.add(\"subtask_analyze_total\", elapsed)\n",
    "        return result, elapsed\n",
    "    \n",
    "    def _execute_compute(self, params: Dict) -> Tuple[str, int]:\n",
    "        \"\"\"Execute computation sub-task\"\"\"\n",
    "        expression = params.get('expression', '')\n",
    "        \n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            # Safe eval\n",
    "            allowed = {'abs': abs, 'round': round, 'min': min, 'max': max, 'sum': sum, 'len': len}\n",
    "            result = eval(expression, {\"__builtins__\": {}}, allowed)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(\"subtask_compute\", elapsed)\n",
    "            return str(result), elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            return f\"Computation error: {str(e)}\", elapsed\n",
    "    \n",
    "    def _execute_summarize(self, params: Dict, context: str) -> Tuple[str, int]:\n",
    "        \"\"\"Execute summarization sub-task\"\"\"\n",
    "        max_words = params.get('max_words', 150)\n",
    "        \n",
    "        prompt = f\"\"\"Summarize the following in {max_words} words or less.\n",
    "\n",
    "Content:\n",
    "{context[:3000]}\n",
    "\n",
    "Summary:\"\"\"\n",
    "        \n",
    "        result, elapsed = self._llm_invoke(prompt, \"subtask_summarize\")\n",
    "        latency_report.add(\"subtask_summarize_total\", elapsed)\n",
    "        return result, elapsed\n",
    "    \n",
    "    def _execute_subtask(self, subtask: SubTask, accumulated_context: str) -> SubTask:\n",
    "        \"\"\"\n",
    "        Phase 2: SOLVE\n",
    "        Execute a single sub-task\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n   üîπ Executing Sub-task {subtask.id}\")\n",
    "        print(f\"      Type: {subtask.type}\")\n",
    "        print(f\"      Description: {subtask.description}\")\n",
    "        \n",
    "        start = time.time_ns()\n",
    "        \n",
    "        if subtask.type == \"retrieve\":\n",
    "            result, exec_elapsed = self._execute_retrieve(subtask.params)\n",
    "            subtask.result = result\n",
    "            subtask.success = bool(result and \"Error\" not in result)\n",
    "        \n",
    "        elif subtask.type == \"analyze\":\n",
    "            result, exec_elapsed = self._execute_analyze(subtask.params, accumulated_context)\n",
    "            subtask.result = result\n",
    "            subtask.success = bool(result)\n",
    "        \n",
    "        elif subtask.type == \"compute\":\n",
    "            result, exec_elapsed = self._execute_compute(subtask.params)\n",
    "            subtask.result = result\n",
    "            subtask.success = \"error\" not in result.lower()\n",
    "        \n",
    "        elif subtask.type == \"summarize\":\n",
    "            result, exec_elapsed = self._execute_summarize(subtask.params, accumulated_context)\n",
    "            subtask.result = result\n",
    "            subtask.success = bool(result)\n",
    "        \n",
    "        else:\n",
    "            result = f\"Unknown task type: {subtask.type}\"\n",
    "            exec_elapsed = 0\n",
    "            subtask.success = False\n",
    "        \n",
    "        subtask.elapsed_ns = time.time_ns() - start\n",
    "        latency_report.add(f\"subtask_{subtask.type}_total\", subtask.elapsed_ns)\n",
    "        \n",
    "        status = \"‚úÖ\" if subtask.success else \"‚ùå\"\n",
    "        result_preview = subtask.result[:80].replace('\\n', ' ')\n",
    "        print(f\"      {status} Result: {result_preview}...\")\n",
    "        print(f\"      ‚è±Ô∏è  {format_time_ns(subtask.elapsed_ns)}\")\n",
    "        \n",
    "        return subtask\n",
    "    \n",
    "    def _aggregate_results(self, subtasks: List[SubTask]) -> str:\n",
    "        \"\"\"\n",
    "        Phase 3: AGGREGATE\n",
    "        Combine all sub-task results\n",
    "        \"\"\"\n",
    "        \n",
    "        aggregated = []\n",
    "        for st in subtasks:\n",
    "            if st.success:\n",
    "                aggregated.append(f\"[Task {st.id}: {st.description}]\")\n",
    "                aggregated.append(st.result)\n",
    "                aggregated.append(\"\")\n",
    "        \n",
    "        return \"\\n\".join(aggregated)\n",
    "    \n",
    "    def _generate_final_answer(self, query: str, aggregated_results: str, plan: List[SubTask]) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Phase 4: ANSWER\n",
    "        Generate comprehensive final answer\n",
    "        \"\"\"\n",
    "        \n",
    "        plan_summary = \"\\n\".join([\n",
    "            f\"{st.id}. {st.description} ({'‚úÖ' if st.success else '‚ùå'})\"\n",
    "            for st in plan\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"Generate a comprehensive answer to the query using the results from the execution plan.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Execution Plan:\n",
    "{plan_summary}\n",
    "\n",
    "Results:\n",
    "{aggregated_results[:3000]}\n",
    "\n",
    "Provide a clear, detailed answer that synthesizes all the information above.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        answer, elapsed = self._llm_invoke(prompt, \"llm_final_answer\")\n",
    "        return answer, elapsed\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute Plan-and-Solve RAG pipeline\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üìã PLAN-AND-SOLVE RAG\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚ùì {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        \n",
    "        # PHASE 1: PLAN\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        print(\"PHASE 1: PLANNING\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        plan, plan_time = self._create_plan(question)\n",
    "        \n",
    "        print(f\"\\nüìã Created plan with {len(plan)} sub-tasks ({format_time_ns(plan_time)}):\")\n",
    "        for st in plan:\n",
    "            print(f\"   {st.id}. [{st.type.upper()}] {st.description}\")\n",
    "        \n",
    "        # PHASE 2: SOLVE\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"PHASE 2: SOLVING\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        accumulated_context = \"\"\n",
    "        \n",
    "        for subtask in plan:\n",
    "            subtask = self._execute_subtask(subtask, accumulated_context)\n",
    "            \n",
    "            # Accumulate successful results for next tasks\n",
    "            if subtask.success:\n",
    "                accumulated_context += f\"\\n\\n{subtask.result}\"\n",
    "        \n",
    "        successful_tasks = sum(1 for st in plan if st.success)\n",
    "        print(f\"\\n   ‚úÖ Completed {successful_tasks}/{len(plan)} tasks\")\n",
    "        \n",
    "        # PHASE 3: AGGREGATE\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"PHASE 3: AGGREGATING\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        aggregated = self._aggregate_results(plan)\n",
    "        print(f\"   ‚úÖ Aggregated {len(aggregated)} chars of results\")\n",
    "        \n",
    "        # PHASE 4: ANSWER\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(\"PHASE 4: FINAL ANSWER\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        answer, answer_time = self._generate_final_answer(question, aggregated, plan)\n",
    "        \n",
    "        print(f\"\\nüí¨ ANSWER ({format_time_ns(answer_time)}):\")\n",
    "        print(f\"{answer}\\n\")\n",
    "        \n",
    "        total_time = time.time_ns() - overall_start\n",
    "        latency_report.add(\"plan_solve_query_total\", total_time)\n",
    "        \n",
    "        print(f\"‚è±Ô∏è  Total: {format_time_ns(total_time)}\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'plan': [{'id': st.id, 'description': st.description, 'type': st.type, 'success': st.success} for st in plan],\n",
    "            'successful_tasks': successful_tasks,\n",
    "            'total_tasks': len(plan),\n",
    "            'total_time': total_time\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "def main():\n",
    "    pipeline_start = time.time_ns()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üìã PLAN-AND-SOLVE RAG PIPELINE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Setup\n",
    "    text = load_pdf(PDF_PATH)\n",
    "    chunks = chunk_text(text)\n",
    "    embed = get_embeddings_model()\n",
    "    pc = init_pinecone(INDEX_NAME)\n",
    "    vs = create_vectorstore(embed, chunks, INDEX_NAME)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Initializing LLM...\")\n",
    "    llm_start = time.time_ns()\n",
    "    llm = ChatGroq(model_name=MODEL_NAME, temperature=0, groq_api_key=GROQ_API_KEY)\n",
    "    llm_elapsed = time.time_ns() - llm_start\n",
    "    latency_report.add(\"llm_init\", llm_elapsed)\n",
    "    print(f\"   ‚úÖ LLM ready ({format_time_ns(llm_elapsed)})\")\n",
    "    \n",
    "    # Initialize Plan-and-Solve RAG\n",
    "    ps_rag = PlanAndSolveRAG(vs, llm)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 1: PLAN-AND-SOLVE RAG QUERIES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    queries = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Summarize the key events and analyze the emotional tone\",\n",
    "        \"What is the capital of France?\",\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        print(f\"\\n{'‚ïê'*70}\")\n",
    "        print(f\"QUERY {i}/{len(queries)}\")\n",
    "        print(f\"{'‚ïê'*70}\")\n",
    "        result = ps_rag.query(q)\n",
    "        results.append(result)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # VADER Benchmark\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 2: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    vader_start = time.time_ns()\n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    vader_init = time.time_ns() - vader_start\n",
    "    latency_report.add(\"vader_init\", vader_init)\n",
    "    \n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    \n",
    "    run_sentiment_benchmark(1, sa, examples)\n",
    "    \n",
    "    # Final Summary\n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency_report.add(\"pipeline_total\", pipeline_total)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìà PIPELINE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_tasks = sum(r['total_tasks'] for r in results)\n",
    "    successful_tasks = sum(r['successful_tasks'] for r in results)\n",
    "    avg_time = sum(r['total_time'] for r in results) // len(results)\n",
    "    \n",
    "    print(f\"Total pipeline time: {format_time_ns(pipeline_total)}\")\n",
    "    print(f\"Queries executed: {len(results)}\")\n",
    "    print(f\"Average query time: {format_time_ns(avg_time)}\")\n",
    "    print(f\"Total sub-tasks: {total_tasks}\")\n",
    "    print(f\"Successful tasks: {successful_tasks}/{total_tasks} ({100*successful_tasks//total_tasks}%)\")\n",
    "    \n",
    "    print(\"\\nüß† Plan-and-Solve Statistics:\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  Query {i}: {r['total_tasks']} tasks, {format_time_ns(r['total_time'])}\")\n",
    "    \n",
    "    latency_report.pretty_print()\n",
    "    print(\"‚úÖ PIPELINE COMPLETE\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
