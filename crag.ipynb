{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf652317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "crag_full.py - Corrective RAG (CRAG) - FIXED VERSION\n",
    "Self-correcting RAG that evaluates retrieved documents and takes corrective actions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pdfplumber\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG - UPDATED WITH NEW INDEX\n",
    "# ---------------------------\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "INDEX_NAME = \"new2\"  # CHANGED: Use existing index\n",
    "DIM = 384\n",
    "PINECONE_API_KEY = \"pcsk_6nSvRh_ALcjY7qdJPA83yRRpUJSFzRnNp3Y5sRNZFwXRY2RDfynWRZJjGMDhc1eKFfArWL\"\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "\n",
    "# ---------------------------\n",
    "# UTILITIES\n",
    "# ---------------------------\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s\"\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    def add(self, component, ns):\n",
    "        self.store[component].append(ns)\n",
    "    def pretty_print(self):\n",
    "        s = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            s[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total\": format_time_ns(total),\n",
    "                \"avg\": format_time_ns(total // len(vals)),\n",
    "                \"min\": format_time_ns(min(vals)),\n",
    "                \"max\": format_time_ns(max(vals))\n",
    "            }\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items()):\n",
    "            print(f\"\\n{comp}:\")\n",
    "            for k, v in stats.items():\n",
    "                print(f\"  {k}: {v}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# ---------------------------\n",
    "# PDF/EMBEDDINGS/PINECONE - UPDATED\n",
    "# ---------------------------\n",
    "def load_pdf(path):\n",
    "    start = time.time_ns()\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        text = \"\\n\".join([p.extract_text() or \"\" for p in pdf.pages])\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"pdf_load\", elapsed)\n",
    "    print(f\"üìÑ Loaded PDF: {len(text)} chars ({format_time_ns(elapsed)})\")\n",
    "    return text\n",
    "\n",
    "def chunk_text(text):\n",
    "    start = time.time_ns()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = splitter.split_text(text)\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"chunking\", elapsed)\n",
    "    print(f\"üìÑ Created {len(chunks)} chunks ({format_time_ns(elapsed)})\")\n",
    "    return chunks\n",
    "\n",
    "def get_embeddings_model():\n",
    "    start = time.time_ns()\n",
    "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_init\", elapsed)\n",
    "    print(f\"üß† Embeddings loaded ({format_time_ns(elapsed)})\")\n",
    "    return emb\n",
    "\n",
    "def init_pinecone(index_name):\n",
    "    \"\"\"Connect to existing Pinecone index - NO CREATION\"\"\"\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing = [idx.name for idx in pc.list_indexes()]\n",
    "    \n",
    "    if index_name not in existing:\n",
    "        print(f\"‚ùå ERROR: Index '{index_name}' does not exist!\")\n",
    "        print(f\"Available indexes: {existing}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"‚úÖ Connected to existing index '{index_name}'\")\n",
    "    return pc\n",
    "\n",
    "def create_vectorstore(embed, chunks, index_name):\n",
    "    \"\"\"Use existing index and add documents\"\"\"\n",
    "    start = time.time_ns()\n",
    "    \n",
    "    # Connect to existing index\n",
    "    vs = PineconeVectorStore.from_existing_index(\n",
    "        index_name=index_name,\n",
    "        embedding=embed\n",
    "    )\n",
    "    \n",
    "    # Add documents with namespace to avoid conflicts\n",
    "    print(f\"üì§ Uploading {len(chunks)} chunks to '{index_name}'...\")\n",
    "    vs.add_texts(\n",
    "        texts=chunks,\n",
    "        metadatas=[{\"chunk_id\": i, \"source\": \"crag\"} for i in range(len(chunks))]\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"vectorstore_create\", elapsed)\n",
    "    print(f\"‚úÖ Vector store ready: {len(chunks)} chunks ({format_time_ns(elapsed)})\")\n",
    "    return vs\n",
    "\n",
    "# ---------------------------\n",
    "# VADER SENTIMENT\n",
    "# ---------------------------\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    def analyze(self, text):\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        if compound >= 0.05:\n",
    "            return {'label': 'POSITIVE', 'compound': compound}\n",
    "        elif compound <= -0.05:\n",
    "            return {'label': 'NEGATIVE', 'compound': compound}\n",
    "        return {'label': 'NEUTRAL', 'compound': compound}\n",
    "\n",
    "def run_sentiment_benchmark(run_num, sa, examples):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî• VADER RUN #{run_num}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed = time.time_ns() - start\n",
    "        times.append(elapsed)\n",
    "        latency_report.add(\"vader_inference\", elapsed)\n",
    "        status = \"‚úÖ\" if elapsed < TARGET_NS else \"‚ùå\"\n",
    "        print(f\"[{i}] {format_time_ns(elapsed):15s} {status} {result['label']:8s} \\\"{text}\\\"\")\n",
    "    \n",
    "    avg = sum(times) // len(times)\n",
    "    print(f\"üìä Average: {format_time_ns(avg)}\")\n",
    "    return avg\n",
    "\n",
    "# ---------------------------\n",
    "# CORRECTIVE RAG (CRAG)\n",
    "# ---------------------------\n",
    "class CorrectiveRAG:\n",
    "    \"\"\"\n",
    "    CRAG: Self-correcting RAG with document evaluation\n",
    "    \n",
    "    Steps:\n",
    "    1. Initial Retrieval\n",
    "    2. Relevance Evaluation (CORRECT/INCORRECT/AMBIGUOUS)\n",
    "    3. Corrective Action\n",
    "    4. Generate answer with corrected context\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm, max_corrections=2):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.max_corrections = max_corrections\n",
    "        self.retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    def _llm_invoke(self, prompt, label):\n",
    "        \"\"\"Timed LLM invocation\"\"\"\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label, elapsed)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            return content, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(label + \"_error\", elapsed)\n",
    "            print(f\"LLM error: {e}\")\n",
    "            return str(e), elapsed\n",
    "    \n",
    "    def _retrieve_documents(self, query: str) -> Tuple[List, int]:\n",
    "        \"\"\"Retrieve documents from vector store\"\"\"\n",
    "        start = time.time_ns()\n",
    "        try:\n",
    "            docs = self.retriever.invoke(query)\n",
    "            elapsed = time.time_ns() - start\n",
    "            latency_report.add(\"crag_retrieve\", elapsed)\n",
    "            return docs, elapsed\n",
    "        except Exception as e:\n",
    "            elapsed = time.time_ns() - start\n",
    "            print(f\"Retrieval error: {e}\")\n",
    "            return [], elapsed\n",
    "    \n",
    "    def _evaluate_relevance(self, query: str, docs: List) -> Tuple[str, float, int]:\n",
    "        \"\"\"\n",
    "        Evaluate if retrieved documents are relevant\n",
    "        Returns: (evaluation, confidence, elapsed_ns)\n",
    "        \"\"\"\n",
    "        if not docs:\n",
    "            return \"INCORRECT\", 0.0, 0\n",
    "        \n",
    "        doc_texts = []\n",
    "        for doc in docs[:3]:\n",
    "            text = getattr(doc, \"page_content\", str(doc))\n",
    "            doc_texts.append(text[:300])\n",
    "        \n",
    "        combined_docs = \"\\n\\n\".join([f\"Doc {i+1}: {t}\" for i, t in enumerate(doc_texts)])\n",
    "        \n",
    "        prompt = f\"\"\"Evaluate if these documents are relevant to answer the question.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Documents:\n",
    "{combined_docs}\n",
    "\n",
    "Evaluate the relevance:\n",
    "- CORRECT: Documents contain relevant information\n",
    "- INCORRECT: Documents are not relevant\n",
    "- AMBIGUOUS: Documents partially relevant\n",
    "\n",
    "Respond ONLY:\n",
    "Evaluation: [CORRECT/INCORRECT/AMBIGUOUS]\n",
    "Confidence: [0.0-1.0]\n",
    "Reason: [brief]\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "        \n",
    "        eval_text, elapsed = self._llm_invoke(prompt, \"crag_evaluate\")\n",
    "        \n",
    "        evaluation = \"AMBIGUOUS\"\n",
    "        confidence = 0.5\n",
    "        \n",
    "        if \"CORRECT\" in eval_text and \"INCORRECT\" not in eval_text:\n",
    "            evaluation = \"CORRECT\"\n",
    "            confidence = 0.9\n",
    "        elif \"INCORRECT\" in eval_text:\n",
    "            evaluation = \"INCORRECT\"\n",
    "            confidence = 0.8\n",
    "        elif \"AMBIGUOUS\" in eval_text:\n",
    "            evaluation = \"AMBIGUOUS\"\n",
    "            confidence = 0.6\n",
    "        \n",
    "        return evaluation, confidence, elapsed\n",
    "    \n",
    "    def _refine_query(self, original_query: str, feedback: str) -> Tuple[str, int]:\n",
    "        \"\"\"Refine query based on feedback\"\"\"\n",
    "        prompt = f\"\"\"Refine this query to get better search results.\n",
    "\n",
    "Original: {original_query}\n",
    "Issue: {feedback}\n",
    "\n",
    "Return ONLY the refined query.\n",
    "\n",
    "Refined Query:\"\"\"\n",
    "        \n",
    "        refined, elapsed = self._llm_invoke(prompt, \"crag_refine_query\")\n",
    "        refined = refined.strip().replace('\"', '')\n",
    "        return refined, elapsed\n",
    "    \n",
    "    def _decompose_query(self, query: str) -> Tuple[List[str], int]:\n",
    "        \"\"\"Decompose ambiguous query\"\"\"\n",
    "        prompt = f\"\"\"Break this query into 2-3 specific sub-queries.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Return numbered list:\n",
    "1. [sub-query 1]\n",
    "2. [sub-query 2]\n",
    "3. [sub-query 3]\n",
    "\n",
    "Sub-queries:\"\"\"\n",
    "        \n",
    "        result, elapsed = self._llm_invoke(prompt, \"crag_decompose\")\n",
    "        \n",
    "        import re\n",
    "        lines = result.split('\\n')\n",
    "        sub_queries = []\n",
    "        for line in lines:\n",
    "            match = re.match(r'\\d+\\.\\s*(.+)', line.strip())\n",
    "            if match:\n",
    "                sub_queries.append(match.group(1))\n",
    "        \n",
    "        return sub_queries[:3], elapsed\n",
    "    \n",
    "    def _generate_answer(self, query: str, context: str, correction_history: str) -> Tuple[str, int]:\n",
    "        \"\"\"Generate final answer\"\"\"\n",
    "        prompt = f\"\"\"Answer using the context.\n",
    "\n",
    "{correction_history}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        answer, elapsed = self._llm_invoke(prompt, \"crag_generate\")\n",
    "        return answer, elapsed\n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"Execute CRAG pipeline\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîß CORRECTIVE RAG (CRAG)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚ùì {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        correction_count = 0\n",
    "        correction_history = []\n",
    "        final_context = \"\"\n",
    "        evaluation = \"AMBIGUOUS\"\n",
    "        \n",
    "        current_query = question\n",
    "        \n",
    "        while correction_count < self.max_corrections:\n",
    "            iteration = correction_count + 1\n",
    "            print(f\"\\n{'‚îÄ'*70}\")\n",
    "            print(f\"ITERATION {iteration}\")\n",
    "            print(f\"{'‚îÄ'*70}\")\n",
    "            \n",
    "            # Retrieve\n",
    "            print(f\"üìö Retrieving: '{current_query[:60]}...'\")\n",
    "            docs, ret_time = self._retrieve_documents(current_query)\n",
    "            print(f\"   ‚úì Retrieved {len(docs)} docs ({format_time_ns(ret_time)})\")\n",
    "            \n",
    "            if not docs:\n",
    "                print(\"   ‚ö†Ô∏è  No documents\")\n",
    "                correction_history.append(f\"Iter {iteration}: No results\")\n",
    "                correction_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Evaluate\n",
    "            print(f\"üîç Evaluating relevance...\")\n",
    "            evaluation, confidence, eval_time = self._evaluate_relevance(current_query, docs)\n",
    "            print(f\"   ‚úì {evaluation} (conf: {confidence:.2f}) ({format_time_ns(eval_time)})\")\n",
    "            \n",
    "            correction_history.append(f\"Iter {iteration}: {evaluation} ({confidence:.2f})\")\n",
    "            \n",
    "            # Corrective action\n",
    "            if evaluation == \"CORRECT\":\n",
    "                print(f\"   ‚úÖ Documents relevant\")\n",
    "                final_context = \"\\n\\n\".join([\n",
    "                    getattr(doc, \"page_content\", str(doc)) for doc in docs\n",
    "                ])\n",
    "                break\n",
    "            \n",
    "            elif evaluation == \"INCORRECT\":\n",
    "                print(f\"   ‚ùå Not relevant, refining...\")\n",
    "                refined_query, refine_time = self._refine_query(current_query, \"Not relevant\")\n",
    "                print(f\"   ‚úì Refined: '{refined_query[:60]}...' ({format_time_ns(refine_time)})\")\n",
    "                current_query = refined_query\n",
    "                correction_count += 1\n",
    "            \n",
    "            elif evaluation == \"AMBIGUOUS\":\n",
    "                print(f\"   ‚ö†Ô∏è  Ambiguous, decomposing...\")\n",
    "                sub_queries, decomp_time = self._decompose_query(current_query)\n",
    "                print(f\"   ‚úì {len(sub_queries)} sub-queries ({format_time_ns(decomp_time)})\")\n",
    "                \n",
    "                all_sub_docs = []\n",
    "                for i, sq in enumerate(sub_queries, 1):\n",
    "                    print(f\"      {i}. {sq[:50]}...\")\n",
    "                    sub_docs, _ = self._retrieve_documents(sq)\n",
    "                    all_sub_docs.extend(sub_docs)\n",
    "                \n",
    "                final_context = \"\\n\\n\".join([\n",
    "                    getattr(doc, \"page_content\", str(doc)) for doc in all_sub_docs[:6]\n",
    "                ])\n",
    "                print(f\"   ‚úì Retrieved {len(all_sub_docs)} docs total\")\n",
    "                break\n",
    "        \n",
    "        # Generate answer\n",
    "        print(f\"\\nüí≠ Generating answer...\")\n",
    "        history_text = \"\\nCorrections:\\n\" + \"\\n\".join(correction_history)\n",
    "        answer, gen_time = self._generate_answer(question, final_context, history_text)\n",
    "        \n",
    "        print(f\"\\nüí¨ ANSWER ({format_time_ns(gen_time)}):\")\n",
    "        print(f\"{answer}\\n\")\n",
    "        \n",
    "        total_time = time.time_ns() - overall_start\n",
    "        latency_report.add(\"crag_query_total\", total_time)\n",
    "        \n",
    "        print(f\"üìä Corrections: {correction_count}/{self.max_corrections}\")\n",
    "        print(f\"‚è±Ô∏è  Total: {format_time_ns(total_time)}\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'corrections': correction_count,\n",
    "            'final_evaluation': evaluation,\n",
    "            'correction_history': correction_history,\n",
    "            'total_time': total_time\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üîß CORRECTIVE RAG (CRAG) PIPELINE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Setup\n",
    "    text = load_pdf(PDF_PATH)\n",
    "    chunks = chunk_text(text)\n",
    "    embed = get_embeddings_model()\n",
    "    pc = init_pinecone(INDEX_NAME)\n",
    "    vs = create_vectorstore(embed, chunks, INDEX_NAME)\n",
    "    \n",
    "    print(f\"\\n‚úÖ LLM initializing...\")\n",
    "    llm = ChatGroq(model_name=MODEL_NAME, temperature=0, groq_api_key=GROQ_API_KEY)\n",
    "    \n",
    "    crag = CorrectiveRAG(vs, llm, max_corrections=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 1: CORRECTIVE RAG QUERIES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    queries = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Tell me about quantum physics\",\n",
    "        \"What happens in the story?\",\n",
    "        \"Describe the mother-daughter relationship\",\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        print(f\"\\n{'‚ïê'*70}\")\n",
    "        print(f\"QUERY {i}/{len(queries)}\")\n",
    "        print(f\"{'‚ïê'*70}\")\n",
    "        result = crag.query(q)\n",
    "        results.append(result)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # VADER\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 2: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    \n",
    "    for run in range(1, 4):\n",
    "        run_sentiment_benchmark(run, sa, examples)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Final report\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CRAG STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_corrections = sum(r['corrections'] for r in results)\n",
    "    print(f\"Total queries: {len(results)}\")\n",
    "    print(f\"Total corrections: {total_corrections}\")\n",
    "    print(f\"Avg: {total_corrections/len(results):.1f} corrections/query\")\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"{i}. {r['question'][:50]}...\")\n",
    "        print(f\"   Corrections: {r['corrections']}, Final: {r['final_evaluation']}\")\n",
    "    \n",
    "    latency_report.pretty_print()\n",
    "    print(\"‚úÖ CRAG COMPLETE\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
