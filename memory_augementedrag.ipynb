{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "memory_augmented_rag.py - FIXED VERSION WITH NEW INDEX\n",
    "Memory-Augmented RAG with conversation history and fact learning\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import traceback\n",
    "from collections import defaultdict, deque\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "import pdfplumber\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG - UPDATED WITH NEW INDEX\n",
    "# ---------------------------\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "INDEX_NAME = \"new2\"  # CHANGED: Use existing index\n",
    "DIM = 384\n",
    "PINECONE_API_KEY = \"pcsk_6nSvRh_ALcjY7qdJPA83yRRpUJSFzRnNp3Y5sRNZFwXRY2RDfynWRZJjGMDhc1eKFfArWL\"\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000\n",
    "SHORT_TERM_MEMORY_SIZE = 10\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "\n",
    "# ---------------------------\n",
    "# UTILITIES\n",
    "# ---------------------------\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} Âµs\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s\"\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    def add(self, component, ns):\n",
    "        self.store[component].append(ns)\n",
    "    def pretty_print(self):\n",
    "        s = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            s[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total\": format_time_ns(total),\n",
    "                \"avg\": format_time_ns(total // len(vals)),\n",
    "                \"min\": format_time_ns(min(vals)),\n",
    "                \"max\": format_time_ns(max(vals))\n",
    "            }\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items()):\n",
    "            print(f\"\\n{comp}:\")\n",
    "            for k, v in stats.items():\n",
    "                print(f\"  {k}: {v}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# ---------------------------\n",
    "# MEMORY STRUCTURES\n",
    "# ---------------------------\n",
    "@dataclass\n",
    "class ConversationTurn:\n",
    "    timestamp: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    turn_id: int\n",
    "    \n",
    "    def to_text(self):\n",
    "        return f\"[Turn {self.turn_id}]\\nQ: {self.question}\\nA: {self.answer[:200]}...\"\n",
    "\n",
    "@dataclass\n",
    "class LongTermFact:\n",
    "    fact: str\n",
    "    category: str\n",
    "    confidence: float\n",
    "    mentions: int\n",
    "    first_seen: str\n",
    "    last_updated: str\n",
    "\n",
    "class MemoryManager:\n",
    "    def __init__(self, max_short_term=SHORT_TERM_MEMORY_SIZE):\n",
    "        self.max_short_term = max_short_term\n",
    "        self.short_term_memory = deque(maxlen=max_short_term)\n",
    "        self.long_term_memory = {}\n",
    "        self.turn_counter = 0\n",
    "        \n",
    "        print(f\"ðŸ§  Memory Manager initialized (capacity: {max_short_term} turns)\")\n",
    "    \n",
    "    def add_turn(self, question: str, answer: str):\n",
    "        start = time.time_ns()\n",
    "        turn = ConversationTurn(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            turn_id=self.turn_counter\n",
    "        )\n",
    "        self.short_term_memory.append(turn)\n",
    "        self.turn_counter += 1\n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"memory_add_turn\", elapsed)\n",
    "        print(f\"ðŸ’¾ Stored in short-term memory (Turn #{turn.turn_id})\")\n",
    "    \n",
    "    def extract_facts(self, llm, question: str, answer: str):\n",
    "        start = time.time_ns()\n",
    "        prompt = f\"\"\"Extract key facts from this conversation. Return JSON only.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Extract facts, preferences, or entities mentioned.\n",
    "\n",
    "{{\n",
    "  \"facts\": [\n",
    "    {{\"text\": \"fact\", \"category\": \"preference|entity|summary\", \"confidence\": 0.8}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "JSON:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                data = json.loads(json_match.group())\n",
    "                facts = data.get('facts', [])\n",
    "                \n",
    "                for f in facts:\n",
    "                    fact_text = f['text']\n",
    "                    if fact_text in self.long_term_memory:\n",
    "                        self.long_term_memory[fact_text].mentions += 1\n",
    "                        self.long_term_memory[fact_text].last_updated = datetime.now().isoformat()\n",
    "                    else:\n",
    "                        self.long_term_memory[fact_text] = LongTermFact(\n",
    "                            fact=fact_text,\n",
    "                            category=f.get('category', 'summary'),\n",
    "                            confidence=f.get('confidence', 0.7),\n",
    "                            mentions=1,\n",
    "                            first_seen=datetime.now().isoformat(),\n",
    "                            last_updated=datetime.now().isoformat()\n",
    "                        )\n",
    "                \n",
    "                if facts:\n",
    "                    print(f\"ðŸ§  Extracted {len(facts)} facts â†’ Long-term memory\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Fact extraction error: {e}\")\n",
    "        \n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"memory_extract_facts\", elapsed)\n",
    "    \n",
    "    def get_short_term_context(self, k=3):\n",
    "        if not self.short_term_memory:\n",
    "            return \"\"\n",
    "        recent = list(self.short_term_memory)[-k:]\n",
    "        return \"\\n\\n\".join([turn.to_text() for turn in recent])\n",
    "    \n",
    "    def get_long_term_facts(self, k=5):\n",
    "        if not self.long_term_memory:\n",
    "            return \"\"\n",
    "        sorted_facts = sorted(\n",
    "            self.long_term_memory.values(),\n",
    "            key=lambda f: f.mentions * f.confidence,\n",
    "            reverse=True\n",
    "        )[:k]\n",
    "        \n",
    "        return \"Learned Facts:\\n\" + \"\\n\".join([\n",
    "            f\"â€¢ {f.fact} ({f.category}, {f.mentions}x)\"\n",
    "            for f in sorted_facts\n",
    "        ])\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            'short_term': len(self.short_term_memory),\n",
    "            'long_term': len(self.long_term_memory),\n",
    "            'total_turns': self.turn_counter\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# PDF/EMBEDDINGS/PINECONE - UPDATED\n",
    "# ---------------------------\n",
    "def load_pdf(path):\n",
    "    start = time.time_ns()\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        text = \"\\n\".join([p.extract_text() or \"\" for p in pdf.pages])\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"pdf_load\", elapsed)\n",
    "    print(f\"ðŸ“„ Loaded PDF: {len(text)} chars ({format_time_ns(elapsed)})\")\n",
    "    return text\n",
    "\n",
    "def chunk_text(text):\n",
    "    start = time.time_ns()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = splitter.split_text(text)\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"chunking\", elapsed)\n",
    "    print(f\"ðŸ“„ Created {len(chunks)} chunks ({format_time_ns(elapsed)})\")\n",
    "    return chunks\n",
    "\n",
    "def get_embeddings_model():\n",
    "    start = time.time_ns()\n",
    "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_init\", elapsed)\n",
    "    print(f\"ðŸ§  Embeddings loaded ({format_time_ns(elapsed)})\")\n",
    "    return emb\n",
    "\n",
    "def init_pinecone(index_name):\n",
    "    \"\"\"Connect to existing Pinecone index - NO CREATION\"\"\"\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing = [idx.name for idx in pc.list_indexes()]\n",
    "    \n",
    "    if index_name not in existing:\n",
    "        print(f\"âŒ ERROR: Index '{index_name}' does not exist!\")\n",
    "        print(f\"Available indexes: {existing}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"âœ… Connected to existing index '{index_name}'\")\n",
    "    return pc\n",
    "\n",
    "def create_vectorstore(embed, chunks, index_name):\n",
    "    \"\"\"Use existing index and add documents\"\"\"\n",
    "    start = time.time_ns()\n",
    "    \n",
    "    # Connect to existing index\n",
    "    vs = PineconeVectorStore.from_existing_index(\n",
    "        index_name=index_name,\n",
    "        embedding=embed\n",
    "    )\n",
    "    \n",
    "    # Add documents to existing index\n",
    "    print(f\"ðŸ“¤ Uploading {len(chunks)} chunks to '{index_name}'...\")\n",
    "    vs.add_texts(\n",
    "        texts=chunks,\n",
    "        metadatas=[{\"chunk_id\": i, \"source\": \"memory_rag\"} for i in range(len(chunks))]\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time_ns() - start\n",
    "    latency_report.add(\"vectorstore_create\", elapsed)\n",
    "    print(f\"âœ… Vector store ready: {len(chunks)} chunks ({format_time_ns(elapsed)})\")\n",
    "    return vs\n",
    "\n",
    "# ---------------------------\n",
    "# VADER SENTIMENT\n",
    "# ---------------------------\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    def analyze(self, text):\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        if compound >= 0.05:\n",
    "            return {'label': 'POSITIVE', 'compound': compound}\n",
    "        elif compound <= -0.05:\n",
    "            return {'label': 'NEGATIVE', 'compound': compound}\n",
    "        return {'label': 'NEUTRAL', 'compound': compound}\n",
    "\n",
    "def run_sentiment_benchmark(run_num, sa, examples):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ”¥ VADER RUN #{run_num}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed = time.time_ns() - start\n",
    "        times.append(elapsed)\n",
    "        latency_report.add(\"vader_inference\", elapsed)\n",
    "        status = \"âœ…\" if elapsed < TARGET_NS else \"âŒ\"\n",
    "        print(f\"[{i}] {format_time_ns(elapsed):15s} {status} {result['label']:8s} \\\"{text}\\\"\")\n",
    "    \n",
    "    avg = sum(times) // len(times)\n",
    "    print(f\"ðŸ“Š Average: {format_time_ns(avg)}\")\n",
    "    return avg\n",
    "\n",
    "# ---------------------------\n",
    "# MEMORY-AUGMENTED RAG\n",
    "# ---------------------------\n",
    "class MemoryAugmentedRAG:\n",
    "    def __init__(self, vectorstore, llm, memory):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "        self.retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    def query(self, question: str):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ðŸ§  MEMORY-AUGMENTED RAG\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"â“ {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        \n",
    "        # Get memory context\n",
    "        print(\"ðŸ“– Retrieving memories...\")\n",
    "        short_term = self.memory.get_short_term_context(k=3)\n",
    "        long_term = self.memory.get_long_term_facts(k=5)\n",
    "        \n",
    "        memory_context = \"\"\n",
    "        if short_term:\n",
    "            memory_context += f\"Recent Conversation:\\n{short_term}\\n\\n\"\n",
    "            print(f\"   âœ“ Short-term: {len(short_term)} chars\")\n",
    "        if long_term:\n",
    "            memory_context += f\"{long_term}\\n\\n\"\n",
    "            print(f\"   âœ“ Long-term: {len(self.memory.long_term_memory)} facts\")\n",
    "        \n",
    "        # Retrieve documents\n",
    "        print(\"ðŸ“š Retrieving documents...\")\n",
    "        start = time.time_ns()\n",
    "        docs = self.retriever.invoke(question)\n",
    "        ret_time = time.time_ns() - start\n",
    "        latency_report.add(\"retrieval\", ret_time)\n",
    "        \n",
    "        doc_context = \"\\n\\n\".join([\n",
    "            getattr(d, \"page_content\", str(d)) for d in docs\n",
    "        ])\n",
    "        print(f\"   âœ“ Retrieved: {len(docs)} docs ({format_time_ns(ret_time)})\")\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"ðŸ’­ Generating answer...\")\n",
    "        full_context = memory_context + f\"Documents:\\n{doc_context}\"\n",
    "        \n",
    "        prompt = f\"\"\"Use conversation history and documents to answer.\n",
    "\n",
    "{full_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        start = time.time_ns()\n",
    "        response = self.llm.invoke(prompt)\n",
    "        gen_time = time.time_ns() - start\n",
    "        latency_report.add(\"generation\", gen_time)\n",
    "        \n",
    "        answer = response.content if hasattr(response, 'content') else str(response)\n",
    "        \n",
    "        print(f\"\\nðŸ’¬ ANSWER ({format_time_ns(gen_time)}):\")\n",
    "        print(f\"{answer}\\n\")\n",
    "        \n",
    "        # Extract facts\n",
    "        print(\"ðŸ” Extracting facts...\")\n",
    "        self.memory.extract_facts(self.llm, question, answer)\n",
    "        \n",
    "        # Store turn\n",
    "        self.memory.add_turn(question, answer)\n",
    "        \n",
    "        total = time.time_ns() - overall_start\n",
    "        latency_report.add(\"total_query\", total)\n",
    "        \n",
    "        mem_sum = self.memory.get_summary()\n",
    "        print(f\"\\nðŸ“Š Memory: {mem_sum['short_term']} turns, {mem_sum['long_term']} facts\")\n",
    "        print(f\"â±ï¸  Total: {format_time_ns(total)}\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'memory_summary': mem_sum,\n",
    "            'total_time': total\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ§  MEMORY-AUGMENTED RAG PIPELINE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Setup\n",
    "    text = load_pdf(PDF_PATH)\n",
    "    chunks = chunk_text(text)\n",
    "    embed = get_embeddings_model()\n",
    "    pc = init_pinecone(INDEX_NAME)\n",
    "    vs = create_vectorstore(embed, chunks, INDEX_NAME)\n",
    "    \n",
    "    print(f\"\\nâœ… LLM initializing...\")\n",
    "    llm = ChatGroq(model_name=MODEL_NAME, temperature=0, groq_api_key=GROQ_API_KEY)\n",
    "    \n",
    "    memory = MemoryManager()\n",
    "    rag = MemoryAugmentedRAG(vs, llm, memory)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 1: MEMORY-AUGMENTED RAG CONVERSATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Multi-turn conversation\n",
    "    conversation = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Tell me more about the love theme you mentioned.\",\n",
    "        \"What other themes did you discuss earlier?\",\n",
    "        \"Summarize everything we talked about.\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for i, q in enumerate(conversation, 1):\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(f\"TURN {i}/{len(conversation)}\")\n",
    "        print(f\"{'â”€'*70}\")\n",
    "        result = rag.query(q)\n",
    "        results.append(result)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # VADER\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 2: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    \n",
    "    for run in range(1, 4):\n",
    "        run_sentiment_benchmark(run, sa, examples)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Final report\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL MEMORY STATE\")\n",
    "    print(\"=\"*70)\n",
    "    mem_sum = memory.get_summary()\n",
    "    print(f\"Short-term: {mem_sum['short_term']}/{SHORT_TERM_MEMORY_SIZE} turns\")\n",
    "    print(f\"Long-term: {mem_sum['long_term']} facts\")\n",
    "    print(f\"Total turns: {mem_sum['total_turns']}\")\n",
    "    \n",
    "    if memory.long_term_memory:\n",
    "        print(f\"\\nðŸ§  Learned Facts:\")\n",
    "        for i, (text, fact) in enumerate(list(memory.long_term_memory.items())[:5], 1):\n",
    "            print(f\"   {i}. {text[:60]}... ({fact.mentions}x)\")\n",
    "    \n",
    "    latency_report.pretty_print()\n",
    "    print(\"âœ… PIPELINE COMPLETE\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
