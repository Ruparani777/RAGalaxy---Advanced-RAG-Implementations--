{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "weaviate_memory_augmented_rag.py\n",
    "Memory-Augmented RAG with Weaviate and comprehensive latency instrumentation.\n",
    "\n",
    "Memory Features:\n",
    "- Short-term Memory: Recent conversation turns (last 10 interactions)\n",
    "- Long-term Memory: Persistent facts with mention tracking\n",
    "- Working Memory: Current context buffer\n",
    "- Conversation continuity across queries\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import traceback\n",
    "from collections import defaultdict, deque\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import pdfplumber\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "PDF_PATH = \"Data/ECHOES OF HER LOVE.pdf\"\n",
    "COLLECTION_NAME = \"MemoryRAG_Documents\"\n",
    "DIM = 384\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "TARGET_NS = 200_000\n",
    "SHORT_TERM_MEMORY_SIZE = 10\n",
    "\n",
    "# Weaviate credentials\n",
    "WEAVIATE_URL = \"21ookhjbswyl5urlawqmxw.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
    "WEAVIATE_API_KEY = \"NTVWQ1dZVDI1bkptcndrZF9JRTFySVg3TEFBc1R5V0luUEtHaU9MajB6am5VQkc3aG5yVkgwWkFQVDc0PV92MjAw\"\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"‚ùå ERROR: Set GROQ_API_KEY environment variable!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =========================================================\n",
    "# LATENCY UTILITIES\n",
    "# =========================================================\n",
    "def format_time_ns(ns: int) -> str:\n",
    "    if ns < 1_000:\n",
    "        return f\"{ns} ns\"\n",
    "    if ns < 1_000_000:\n",
    "        return f\"{ns/1_000:.3f} ¬µs ({ns} ns)\"\n",
    "    if ns < 1_000_000_000:\n",
    "        return f\"{ns/1_000_000:.3f} ms ({ns} ns)\"\n",
    "    return f\"{ns/1_000_000_000:.3f} s ({ns} ns)\"\n",
    "\n",
    "def timed_call(fn, *args, **kwargs):\n",
    "    start = time.time_ns()\n",
    "    result = fn(*args, **kwargs)\n",
    "    elapsed = time.time_ns() - start\n",
    "    return result, elapsed\n",
    "\n",
    "def timer_ns(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time_ns()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time_ns() - start\n",
    "        print(f\"‚è±Ô∏è  {func.__name__} time: {format_time_ns(elapsed)}\")\n",
    "        wrapper.last_elapsed_ns = elapsed\n",
    "        return result\n",
    "    wrapper.last_elapsed_ns = None\n",
    "    return wrapper\n",
    "\n",
    "class LatencyReport:\n",
    "    def __init__(self):\n",
    "        self.store = defaultdict(list)\n",
    "    \n",
    "    def add(self, component: str, ns: int):\n",
    "        self.store[component].append(ns)\n",
    "    \n",
    "    def pretty_print(self):\n",
    "        s = {}\n",
    "        for comp, vals in self.store.items():\n",
    "            total = sum(vals)\n",
    "            s[comp] = {\n",
    "                \"count\": len(vals),\n",
    "                \"total\": format_time_ns(total),\n",
    "                \"avg\": format_time_ns(total // len(vals) if vals else 0),\n",
    "                \"min\": format_time_ns(min(vals) if vals else 0),\n",
    "                \"max\": format_time_ns(max(vals) if vals else 0)\n",
    "            }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LATENCY SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for comp, stats in sorted(s.items()):\n",
    "            print(f\"\\nüìä Component: {comp}\")\n",
    "            for k, v in stats.items():\n",
    "                print(f\"   {k.capitalize():10s} {v}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "latency_report = LatencyReport()\n",
    "\n",
    "# =========================================================\n",
    "# MEMORY STRUCTURES\n",
    "# =========================================================\n",
    "@dataclass\n",
    "class ConversationTurn:\n",
    "    \"\"\"Single conversation turn\"\"\"\n",
    "    timestamp: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    context_used: str\n",
    "    turn_id: int\n",
    "    \n",
    "    def to_text(self):\n",
    "        return f\"[Turn {self.turn_id}]\\nQ: {self.question}\\nA: {self.answer[:200]}...\"\n",
    "\n",
    "@dataclass\n",
    "class LongTermFact:\n",
    "    \"\"\"Persistent fact or preference\"\"\"\n",
    "    fact: str\n",
    "    category: str  # preference, entity, relationship, summary\n",
    "    confidence: float\n",
    "    mentions: int\n",
    "    first_seen: str\n",
    "    last_updated: str\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"\n",
    "    Manages multiple memory types:\n",
    "    - Short-term: Recent conversation history\n",
    "    - Long-term: Persistent facts and preferences\n",
    "    - Working: Current context buffer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_short_term=SHORT_TERM_MEMORY_SIZE):\n",
    "        self.max_short_term = max_short_term\n",
    "        self.short_term_memory: deque = deque(maxlen=max_short_term)\n",
    "        self.long_term_memory: Dict[str, LongTermFact] = {}\n",
    "        self.working_memory: Dict = {\n",
    "            'current_topic': None,\n",
    "            'entities_mentioned': set(),\n",
    "            'temp_context': []\n",
    "        }\n",
    "        self.turn_counter = 0\n",
    "        \n",
    "        print(f\"üß† Memory Manager initialized (capacity: {max_short_term} turns)\")\n",
    "    \n",
    "    def add_conversation_turn(self, question: str, answer: str, context: str = \"\"):\n",
    "        \"\"\"Add a conversation turn to short-term memory\"\"\"\n",
    "        start = time.time_ns()\n",
    "        \n",
    "        turn = ConversationTurn(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            context_used=context[:500],\n",
    "            turn_id=self.turn_counter\n",
    "        )\n",
    "        \n",
    "        self.short_term_memory.append(turn)\n",
    "        self.turn_counter += 1\n",
    "        \n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"memory_add_turn\", elapsed)\n",
    "        \n",
    "        print(f\"üíæ Added turn to short-term memory (Turn #{turn.turn_id})\")\n",
    "    \n",
    "    def extract_and_store_facts(self, llm, question: str, answer: str):\n",
    "        \"\"\"Extract facts from conversation and store in long-term memory\"\"\"\n",
    "        start = time.time_ns()\n",
    "        \n",
    "        prompt = f\"\"\"Extract key facts from this conversation. Return JSON only.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Extract facts, preferences, or entities mentioned.\n",
    "\n",
    "{{\n",
    "  \"facts\": [\n",
    "    {{\"text\": \"fact\", \"category\": \"preference|entity|summary\", \"confidence\": 0.8}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "If no important facts, return {{\"facts\": []}}\n",
    "\n",
    "JSON:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                data = json.loads(json_match.group())\n",
    "                facts = data.get('facts', [])\n",
    "                \n",
    "                for f in facts:\n",
    "                    fact_text = f['text']\n",
    "                    if fact_text in self.long_term_memory:\n",
    "                        self.long_term_memory[fact_text].mentions += 1\n",
    "                        self.long_term_memory[fact_text].last_updated = datetime.now().isoformat()\n",
    "                    else:\n",
    "                        self.long_term_memory[fact_text] = LongTermFact(\n",
    "                            fact=fact_text,\n",
    "                            category=f.get('category', 'summary'),\n",
    "                            confidence=f.get('confidence', 0.7),\n",
    "                            mentions=1,\n",
    "                            first_seen=datetime.now().isoformat(),\n",
    "                            last_updated=datetime.now().isoformat()\n",
    "                        )\n",
    "                \n",
    "                if facts:\n",
    "                    print(f\"üß† Extracted {len(facts)} facts ‚Üí Long-term memory\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Fact extraction error: {e}\")\n",
    "        \n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"memory_extract_facts\", elapsed)\n",
    "    \n",
    "    def get_short_term_context(self, k=3):\n",
    "        if not self.short_term_memory:\n",
    "            return \"\"\n",
    "        recent = list(self.short_term_memory)[-k:]\n",
    "        return \"\\n\\n\".join([turn.to_text() for turn in recent])\n",
    "    \n",
    "    def get_long_term_facts(self, k=5):\n",
    "        if not self.long_term_memory:\n",
    "            return \"\"\n",
    "        sorted_facts = sorted(\n",
    "            self.long_term_memory.values(),\n",
    "            key=lambda f: f.mentions * f.confidence,\n",
    "            reverse=True\n",
    "        )[:k]\n",
    "        \n",
    "        return \"Learned Facts:\\n\" + \"\\n\".join([\n",
    "            f\"‚Ä¢ {f.fact} ({f.category}, {f.mentions}x)\"\n",
    "            for f in sorted_facts\n",
    "        ])\n",
    "    \n",
    "    def get_memory_summary(self):\n",
    "        return {\n",
    "            'short_term': len(self.short_term_memory),\n",
    "            'long_term': len(self.long_term_memory),\n",
    "            'total_turns': self.turn_counter\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# PDF/CHUNKING/EMBEDDINGS/WEAVIATE\n",
    "# =========================================================\n",
    "@timer_ns\n",
    "def load_pdf(path: str) -> str:\n",
    "    print(f\"üìÑ Loading PDF: {path}\")\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for p in pdf.pages:\n",
    "            start_ns = time.time_ns()\n",
    "            t = p.extract_text() or \"\"\n",
    "            elapsed = time.time_ns() - start_ns\n",
    "            latency_report.add(\"pdf_page_extract\", elapsed)\n",
    "            text += t + \"\\n\"\n",
    "    print(f\"‚úÖ Loaded PDF: {len(text)} characters\")\n",
    "    return text\n",
    "\n",
    "@timer_ns\n",
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:\n",
    "    print(f\"‚úÇÔ∏è  Chunking text...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "@timer_ns\n",
    "def load_embeddings(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    print(f\"üî¢ Loading embeddings: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    print(f\"‚úÖ Embeddings loaded\")\n",
    "    return embedder\n",
    "\n",
    "@timer_ns\n",
    "def init_weaviate(url: str, api_key: str, collection_name: str = COLLECTION_NAME) -> weaviate.WeaviateClient:\n",
    "    print(f\"üóÉÔ∏è  Initializing Weaviate\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=url,\n",
    "        auth_credentials=Auth.api_key(api_key)\n",
    "    )\n",
    "    connect_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_connect\", connect_time)\n",
    "    print(f\"‚úÖ Connected to Weaviate ({format_time_ns(connect_time)})\")\n",
    "    \n",
    "    try:\n",
    "        if client.collections.exists(collection_name):\n",
    "            client.collections.delete(collection_name)\n",
    "            print(f\"üóëÔ∏è  Deleted existing collection\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Collection check: {e}\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    try:\n",
    "        client.collections.create(\n",
    "            name=collection_name,\n",
    "            vectorizer_config=None,\n",
    "            properties=[\n",
    "                {\"name\": \"text\", \"dataType\": [\"text\"]},\n",
    "                {\"name\": \"chunk_id\", \"dataType\": [\"int\"]},\n",
    "                {\"name\": \"source\", \"dataType\": [\"text\"]}\n",
    "            ]\n",
    "        )\n",
    "        create_time = time.time_ns() - start\n",
    "        latency_report.add(\"weaviate_create_collection\", create_time)\n",
    "        print(f\"‚úÖ Collection created ({format_time_ns(create_time)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Collection creation: {e}\")\n",
    "    \n",
    "    return client\n",
    "\n",
    "@timer_ns\n",
    "def insert_chunks(client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                  chunks: List[str], collection_name: str = COLLECTION_NAME) -> None:\n",
    "    print(f\"‚¨ÜÔ∏è  Inserting {len(chunks)} chunks...\")\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    vectors = embedder.encode(chunks, show_progress_bar=False)\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"embedding_encode_batch\", encode_time)\n",
    "    print(f\"   ‚úÖ Encoded ({format_time_ns(encode_time)})\")\n",
    "    \n",
    "    collection = client.collections.get(collection_name)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    with collection.batch.dynamic() as batch:\n",
    "        for i, (chunk, vector) in enumerate(zip(chunks, vectors)):\n",
    "            batch.add_object(\n",
    "                properties={\"text\": chunk, \"chunk_id\": i, \"source\": f\"chunk_{i}\"},\n",
    "                vector=vector.tolist()\n",
    "            )\n",
    "    upsert_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_upsert\", upsert_time)\n",
    "    print(f\"   ‚úÖ Upserted ({format_time_ns(upsert_time)})\")\n",
    "\n",
    "def search_weaviate(client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                    query: str, limit: int = 4, collection_name: str = COLLECTION_NAME) -> Tuple[List[str], int]:\n",
    "    start = time.time_ns()\n",
    "    qvec = embedder.encode([query])[0]\n",
    "    encode_time = time.time_ns() - start\n",
    "    latency_report.add(\"query_embedding\", encode_time)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    collection = client.collections.get(collection_name)\n",
    "    response = collection.query.near_vector(\n",
    "        near_vector=qvec.tolist(),\n",
    "        limit=limit,\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "    search_time = time.time_ns() - start\n",
    "    latency_report.add(\"weaviate_search\", search_time)\n",
    "    \n",
    "    hits = [obj.properties.get(\"text\", \"\") for obj in response.objects]\n",
    "    return hits, encode_time + search_time\n",
    "\n",
    "# =========================================================\n",
    "# VADER SENTIMENT\n",
    "# =========================================================\n",
    "class VaderSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict[str, Any]:\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        compound = scores['compound']\n",
    "        \n",
    "        if compound >= 0.05:\n",
    "            label = \"POSITIVE\"\n",
    "        elif compound <= -0.05:\n",
    "            label = \"NEGATIVE\"\n",
    "        else:\n",
    "            label = \"NEUTRAL\"\n",
    "        \n",
    "        return {'label': label, 'compound': compound, 'scores': scores}\n",
    "\n",
    "def run_sentiment_benchmark(sa: VaderSentimentAnalyzer, examples: List[str], \n",
    "                            target_ns: int = TARGET_NS, run_number: int = 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî• SENTIMENT BENCHMARK RUN #{run_number}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üéØ TARGET: < {target_ns} ns\\n\")\n",
    "    \n",
    "    times = []\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        start = time.time_ns()\n",
    "        result = sa.analyze(text)\n",
    "        elapsed = time.time_ns() - start\n",
    "        latency_report.add(\"vader_per_example\", elapsed)\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        status = \"‚úÖ\" if elapsed < target_ns else \"‚ùå\"\n",
    "        print(f\"[{i:2d}] {format_time_ns(elapsed):20s} {status} | {result['label']:8s} | \\\"{text}\\\"\")\n",
    "    \n",
    "    avg = sum(times) // len(times)\n",
    "    print(f\"\\nüìä Average: {format_time_ns(avg)}\")\n",
    "\n",
    "# =========================================================\n",
    "# MEMORY-AUGMENTED RAG\n",
    "# =========================================================\n",
    "class MemoryAugmentedRAG:\n",
    "    \"\"\"RAG with integrated memory system\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, client: weaviate.WeaviateClient, embedder: SentenceTransformer,\n",
    "                 memory: MemoryManager, collection_name: str = COLLECTION_NAME):\n",
    "        self.llm = llm\n",
    "        self.client = client\n",
    "        self.embedder = embedder\n",
    "        self.memory = memory\n",
    "        self.collection_name = collection_name\n",
    "    \n",
    "    def query(self, question: str, use_memory: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Query with memory augmentation\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üß† MEMORY-AUGMENTED RAG\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚ùì {question}\\n\")\n",
    "        \n",
    "        overall_start = time.time_ns()\n",
    "        \n",
    "        # Step 1: Get memory context\n",
    "        memory_context = \"\"\n",
    "        if use_memory:\n",
    "            print(\"üìñ Retrieving from memory...\")\n",
    "            \n",
    "            short_term = self.memory.get_short_term_context(k=3)\n",
    "            long_term = self.memory.get_long_term_facts(k=5)\n",
    "            \n",
    "            if short_term or long_term:\n",
    "                memory_context = f\"CONVERSATION MEMORY:\\n\"\n",
    "                if short_term:\n",
    "                    memory_context += f\"\\nRecent Conversation:\\n{short_term}\\n\"\n",
    "                    print(f\"   ‚úì Short-term: {len(short_term)} chars\")\n",
    "                if long_term:\n",
    "                    memory_context += f\"\\n{long_term}\\n\"\n",
    "                    print(f\"   ‚úì Long-term: {len(self.memory.long_term_memory)} facts\")\n",
    "        \n",
    "        # Step 2: Retrieve from Weaviate\n",
    "        print(\"üìö Retrieving from documents...\")\n",
    "        hits, ret_time = search_weaviate(self.client, self.embedder, question, 4, self.collection_name)\n",
    "        doc_context = \"\\n\\n\".join(hits)\n",
    "        print(f\"   ‚úì Retrieved: {len(hits)} docs ({format_time_ns(ret_time)})\")\n",
    "        \n",
    "        # Step 3: Generate answer\n",
    "        print(\"üí≠ Generating answer...\")\n",
    "        \n",
    "        full_context = \"\"\n",
    "        if memory_context:\n",
    "            full_context += memory_context + \"\\n\\n\"\n",
    "        if doc_context:\n",
    "            full_context += f\"DOCUMENT CONTEXT:\\n{doc_context}\"\n",
    "        \n",
    "        prompt = f\"\"\"Use conversation history and documents to answer.\n",
    "\n",
    "{full_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        start = time.time_ns()\n",
    "        response = self.llm.invoke(prompt)\n",
    "        gen_time = time.time_ns() - start\n",
    "        latency_report.add(\"llm_generate_answer\", gen_time)\n",
    "        \n",
    "        answer = response.content if hasattr(response, 'content') else str(response)\n",
    "        \n",
    "        print(f\"\\nüí¨ ANSWER ({format_time_ns(gen_time)}):\")\n",
    "        print(answer[:500])\n",
    "        if len(answer) > 500:\n",
    "            print(\"...\")\n",
    "        \n",
    "        # Step 4: Extract and store facts\n",
    "        if use_memory:\n",
    "            print(\"\\nüß† Extracting facts...\")\n",
    "            self.memory.extract_and_store_facts(self.llm, question, answer)\n",
    "        \n",
    "        # Step 5: Add to short-term memory\n",
    "        if use_memory:\n",
    "            self.memory.add_conversation_turn(question, answer, doc_context[:500])\n",
    "        \n",
    "        mem_summary = self.memory.get_memory_summary()\n",
    "        \n",
    "        total = time.time_ns() - overall_start\n",
    "        latency_report.add(\"memory_rag_total\", total)\n",
    "        \n",
    "        print(f\"\\nüìä Memory: {mem_summary['short_term']} turns, {mem_summary['long_term']} facts\")\n",
    "        print(f\"‚è±Ô∏è  Total: {format_time_ns(total)}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'memory_summary': mem_summary,\n",
    "            'total_time': total\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üß† MEMORY-AUGMENTED RAG + WEAVIATE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    pipeline_start = time.time_ns()\n",
    "    \n",
    "    # Phase 1: Data preparation\n",
    "    print(\"üìö PHASE 1: DATA PREPARATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    text, load_time = timed_call(load_pdf, PDF_PATH)\n",
    "    latency_report.add(\"pipeline_pdf_load\", load_time)\n",
    "    \n",
    "    chunks, chunk_time = timed_call(chunk_text, text, 1000, 100)\n",
    "    latency_report.add(\"pipeline_chunking\", chunk_time)\n",
    "    \n",
    "    embedder, embed_time = timed_call(load_embeddings)\n",
    "    latency_report.add(\"pipeline_embeddings_load\", embed_time)\n",
    "    \n",
    "    weaviate_client, weaviate_time = timed_call(init_weaviate, WEAVIATE_URL, WEAVIATE_API_KEY, COLLECTION_NAME)\n",
    "    latency_report.add(\"pipeline_weaviate_init\", weaviate_time)\n",
    "    \n",
    "    insert_start = time.time_ns()\n",
    "    insert_chunks(weaviate_client, embedder, chunks, COLLECTION_NAME)\n",
    "    insert_time = time.time_ns() - insert_start\n",
    "    latency_report.add(\"pipeline_insert_chunks\", insert_time)\n",
    "    \n",
    "    # Phase 2: Initialize LLM\n",
    "    print(f\"\\nüìö PHASE 2: LLM INITIALIZATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    llm_start = time.time_ns()\n",
    "    llm = ChatGroq(model_name=MODEL_NAME, groq_api_key=GROQ_API_KEY, temperature=0)\n",
    "    llm_time = time.time_ns() - llm_start\n",
    "    latency_report.add(\"llm_init\", llm_time)\n",
    "    print(f\"‚úÖ LLM initialized ({format_time_ns(llm_time)})\")\n",
    "    \n",
    "    # Initialize Memory Manager\n",
    "    memory = MemoryManager()\n",
    "    \n",
    "    # Initialize Memory-Augmented RAG\n",
    "    mem_rag = MemoryAugmentedRAG(llm, weaviate_client, embedder, memory, COLLECTION_NAME)\n",
    "    print(f\"\\n‚úÖ Memory-Augmented RAG initialized!\")\n",
    "    \n",
    "    # Phase 3: Multi-turn conversation\n",
    "    print(f\"\\nüìö PHASE 3: MEMORY-AUGMENTED RAG QUERIES\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    conversation = [\n",
    "        \"What are the main themes in this story?\",\n",
    "        \"Tell me more about the love theme you mentioned.\",\n",
    "        \"What other themes did you discuss earlier?\",\n",
    "        \"Summarize everything we talked about.\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for i, q in enumerate(conversation, 1):\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(f\"TURN {i}/{len(conversation)}\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        result = mem_rag.query(q, use_memory=True)\n",
    "        results.append(result)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Phase 4: Sentiment benchmark\n",
    "    print(f\"\\nüìö PHASE 4: VADER SENTIMENT BENCHMARK\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    sa = VaderSentimentAnalyzer()\n",
    "    examples = [\n",
    "        \"I love this product!\",\n",
    "        \"This is very bad service.\",\n",
    "        \"It's okay, not too good, not too bad.\",\n",
    "        \"Not great, really disappointed\",\n",
    "        \"Amazing experience!\"\n",
    "    ]\n",
    "    \n",
    "    run_sentiment_benchmark(sa, examples, TARGET_NS, 1)\n",
    "    \n",
    "    # Final summary\n",
    "    pipeline_total = time.time_ns() - pipeline_start\n",
    "    latency_report.add(\"pipeline_total\", pipeline_total)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìà PIPELINE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    final_mem = memory.get_memory_summary()\n",
    "    print(f\"Total pipeline time: {format_time_ns(pipeline_total)}\")\n",
    "    print(f\"Queries executed: {len(conversation)}\")\n",
    "    print(f\"\\nüß† FINAL MEMORY STATE:\")\n",
    "    print(f\"   Short-term: {final_mem['short_term']}/{SHORT_TERM_MEMORY_SIZE} turns\")\n",
    "    print(f\"   Long-term: {final_mem['long_term']} facts\")\n",
    "    print(f\"   Total turns: {final_mem['total_turns']}\")\n",
    "    \n",
    "    if memory.long_term_memory:\n",
    "        print(f\"\\nüß† Learned Facts:\")\n",
    "        for i, (text, fact) in enumerate(list(memory.long_term_memory.items())[:5], 1):\n",
    "            print(f\"   {i}. {text[:70]}... ({fact.mentions}x)\")\n",
    "    \n",
    "    latency_report.pretty_print()\n",
    "    \n",
    "    # Cleanup\n",
    "    weaviate_client.close()\n",
    "    \n",
    "    print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Interrupted by user\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Fatal error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
